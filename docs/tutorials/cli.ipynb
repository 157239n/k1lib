{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb6e87b",
   "metadata": {},
   "source": [
    "# k1lib.bioinfo.cli module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d4457",
   "metadata": {},
   "source": [
    "This tutorial is for the basics of the `k1lib.bioinfo.cli` module (docs at https://k1lib.github.io/latest/bioinfo/cli.html). As a quick reminder, this module allows you to use common cli tools from the linux cli inside of Python. The idea for this module came across while I was reading over the [Biostar Handbook](https://www.biostarhandbook.com/). They used a lot of cli tools, but all of them are sort of weird, unintuitive, not powerful, and just painful to work with. That's why I made this module to move everything to regular Python.\n",
    "\n",
    "We're going to go over the multilanguage names dataset from a [PyTorch RNN tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html). The data folder is at [cli_name_languages](https://github.com/157239n/k1lib/blob/master/docs/tutorials/cli_name_languages) btw. My advice is to read this along with the docs page, and see the sources of functions that you're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950ab2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    div.jp-OutputArea-output pre {white-space: pre;}\n",
       "    div.output_area pre {white-space: pre;}\n",
       "    div.CodeMirror > div.highlight {overflow-y: auto;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from k1lib.imports import *\n",
    "import unicodedata, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0136c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['cli_name_languages/names/Korean.txt',\n",
       "  'cli_name_languages/names/Spanish.txt',\n",
       "  'cli_name_languages/names/Greek.txt'],\n",
       " 18)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namesFolder = \"cli_name_languages/names\"\n",
    "nameFiles = glob.glob(f\"{namesFolder}/*.txt\")\n",
    "withBareNames = insertColumn(*(nameFiles | op().split(\"/\")[-1].all() | op().split(\".\")[0].all())) | display(None)\n",
    "nameFiles[:3], len(nameFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcb547",
   "metadata": {},
   "source": [
    "So, we have 18 files in total. Let's look over a few of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba0819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahn\n",
      "Baik\n",
      "Bang\n"
     ]
    }
   ],
   "source": [
    "cat(nameFiles[0]) | headOut(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d34ff",
   "metadata": {},
   "source": [
    "You can also pipe the file name in btw, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c2db35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahn\n",
      "Baik\n",
      "Bang\n"
     ]
    }
   ],
   "source": [
    "nameFiles[0] | cat() | headOut(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6057c",
   "metadata": {},
   "source": [
    "Let's convert all unicode chars to regular ascii (taken from the PyTorch doc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b88daad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = string.ascii_letters + \".,;'\"\n",
    "def unicodeToAscii(s, notIn=False):\n",
    "    if notIn: # debug case\n",
    "        return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\" and c not in letters)\n",
    "    else: # \"right\" case\n",
    "        return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) if unicodedata.category(c) != \"Mn\" and c in letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d2993e",
   "metadata": {},
   "source": [
    "How many names in total across files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e46139e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20074"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameFiles | cat().all() | joinStreams() | shape(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f751ee7",
   "metadata": {},
   "source": [
    "How many names with weird unicode characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5520257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19962          99%   \n",
      "47             0%    \n",
      "3              0%    \n",
      "21      -      0%    \n",
      "2       --     0%    \n",
      "1              0%    \n",
      "23             0%    \n",
      "1       /      0%    \n",
      "3       1      0%    \n",
      "9       ß      0%    \n",
      "1       ł      0%    \n",
      "1       :      0%    \n"
     ]
    }
   ],
   "source": [
    "def unicodes(): return nameFiles | cat().all() | joinStreams() | apply(partial(unicodeToAscii, notIn=True))\n",
    "unicodes() | count() | display(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0dc9b",
   "metadata": {},
   "source": [
    "See over https://k1lib.github.io/latest/cli/streams for more info about how stuff like `cat()` and `joinStreams()` work. Also, `partial` is a pretty awesome function I might add, look over it at [Python functools docs](https://docs.python.org/3/library/functools.html#functools.partial). There're lots of empty names here, so let's get rid of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1587efb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21   -    55%   \n",
      "2    --   5%    \n",
      "1    /    3%    \n",
      "3    1    8%    \n",
      "9    ß    24%   \n",
      "1    ł    3%    \n",
      "1    :    3%    \n"
     ]
    }
   ],
   "source": [
    "unicodes() | op().strip().all() | ~isValue(\"\") | count() | display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bdf9b8",
   "metadata": {},
   "source": [
    "Here, we're just stripping white spaces at both ends of each name (`strip()`) and filters them out (`~isValue(\"\")`). The tilde `~` sign common in front of every filter functions effectively inverts the filter's condition. How many duplicate names are there in a file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a33d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean       94     \n",
      "Spanish      296    \n",
      "Greek        193    \n",
      "Irish        226    \n",
      "Scottish     100    \n",
      "Portuguese   74     \n",
      "Russian      9342   \n",
      "Czech        503    \n",
      "French       273    \n",
      "German       706    \n",
      "Japanese     990    \n",
      "Polish       138    \n",
      "Arabic       108    \n",
      "English      3668   \n",
      "Chinese      246    \n",
      "Dutch        286    \n",
      "Italian      701    \n",
      "Vietnamese   71     \n"
     ]
    }
   ],
   "source": [
    "nameFiles | cat().all() | (count() | ~isValue(\"1\", 0) | shape(0)).all() | unsqueeze(1) | withBareNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e03d9",
   "metadata": {},
   "source": [
    "Okay yeah there's a lot. Let's see how many unique names (of each file) that appear in other files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "225f13ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18015, 17458]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameFiles | cat().all() | toSet().all() | joinStreams() | (identity() & toSet()) | shape(0).all() | deref()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b701a",
   "metadata": {},
   "source": [
    "Let's see what are the actual Korean names that appear in other files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "787ef9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish                                                                                                                                 \n",
      "Greek                                                                                                                                   \n",
      "Irish                                                                                                                                   \n",
      "Scottish                                                                                                                                \n",
      "Portuguese                                                                                                                              \n",
      "Russian      Li      Han                                                                                                                \n",
      "Czech                                                                                                                                   \n",
      "French                                                                                                                                  \n",
      "German       Wang                                                                                                                       \n",
      "Japanese     Ko      Jo    Seo                                                                                                          \n",
      "Polish                                                                                                                                  \n",
      "Arabic                                                                                                                                  \n",
      "English      Moon    Lee   Wang   Yang   Chung   Chong                                                                                  \n",
      "Chinese      Chang   You   Chi    Song   Wang    Chu     Hong   Yang   Sun   Chou   Koo   Woo   Yun   Chin   Kang   Chong   Han   Yim   \n",
      "Dutch                                                                                                                                   \n",
      "Italian                                                                                                                                 \n",
      "Vietnamese   Ma      Ho    Kim    Chu    Chung   Ha      Han                                                                            \n"
     ]
    }
   ],
   "source": [
    "nameFiles | AA_(0) | ((cat() | toList() | repeat()) + cat().all()) | transpose() | intersection().all()\\\n",
    "| insertColumn(*list(nameFiles | op().split(\"/\")[-1].all() | op().split(\".\")[0].all())[1:]) | display(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7cf0d5",
   "metadata": {},
   "source": [
    "`cat() | toList() | repeat()`'s branch essentially creates `Iterator[File]`, and each `File` is actually just `Iterator[str]`. Result of `cat().all()` is also `Iterator[File]`. We want to place these 2 lists' elements on each row, so we can actually operate on them. `joinColumns()` will output `Iterator[(File, File)]`. First file is the Korean one, second file is every other file. `intersection()` will find the common names between the 2 files, and `insertColumn()` just to have some nice formatting.\n",
    "\n",
    "How about we do this for every file and record how many names in that that is in other files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf2ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean       37    \n",
      "Spanish      104   \n",
      "Greek        1     \n",
      "Irish        78    \n",
      "Scottish     115   \n",
      "Portuguese   57    \n",
      "Russian      74    \n",
      "Czech        41    \n",
      "French       102   \n",
      "German       148   \n",
      "Japanese     9     \n",
      "Polish       24    \n",
      "Arabic       5     \n",
      "English      381   \n",
      "Chinese      52    \n",
      "Dutch        58    \n",
      "Italian      54    \n",
      "Vietnamese   20    \n"
     ]
    }
   ],
   "source": [
    "analyze2Files = intersection() | shape(0) # takes 2 files, and squish them into 1 value\n",
    "analyze1Combo = ((cat() | toList() | repeat()) + cat().all()) | transpose() | analyze2Files.all() | toSum() # summing all common values\n",
    "nameFiles | AA_() | analyze1Combo.all() | unsqueeze(1) | withBareNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee79a7",
   "metadata": {},
   "source": [
    "Nice. Anyway, hope you are as thrilled as I am about this. Really complicated loops and whatnot can be explored quite quickly without actually writing any loops, and that helps with bringing down iteration time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50ff56",
   "metadata": {},
   "source": [
    "## Speed analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f30536",
   "metadata": {},
   "source": [
    "While developing this module, I thought I'd have to drop down to C level for it to be fast enough to process anything at all. However, time and time again, it seems like Python is good enough for most things. Any Python operation is around 1.5 orders of magnitude slower than 1ns, so 30ns. Also means that flops rate should be around 7-7.5 orders of magnitude, while we should expect 8-8.5 out of C code. Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62942c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181500\n",
      "221500\n",
      "181500\n",
      "221500\n",
      "181500\n",
      "221500\n",
      "181500\n",
      "221500\n",
      "181500\n",
      "221500\n",
      "CPU times: user 1.25 s, sys: 0 ns, total: 1.25 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "range(400) | repeatFrom() | apply(lambda x: x+2) | batched(1000) | toSum().all() | ~head(10000) | headOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c444d",
   "metadata": {},
   "source": [
    "This is just taking an infinite list of numbers, add 2 to it, batches every 1000 numbers, summing over each row, and do that 10000 times. So 10M ops in around 1 second. Right around the 6.8-6.9 (nice haha) orders of magnitude flop rate. Is this good enough though?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318ac56",
   "metadata": {},
   "source": [
    "[Lots of ppl](https://stackoverflow.com/questions/57489806/speed-of-csv-reading-slow-python) reported the builtin module `csv` can parse around 50MB/s. Let's say there're 10 columns, and each column has 10 characters, which equals to 100 bytes/row. So, the throughput should be 500k rows/s, well below what we have here. Even if you assume it's costly to operate on a table, so the figure 5M table elements/s, then that's still lower than what cli tools can achieve. So no need to worry about this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89786425",
   "metadata": {},
   "source": [
    "### For loops with yields vs .all()\n",
    "\n",
    "A lot of time, I was worried about the performance of `.all()` operation. But turns out, `applyS(f).all()` has roughly the same performance as `apply(f)`, so don't worry about it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "899b2b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.9 ms, sys: 0 ns, total: 72.9 ms\n",
      "Wall time: 71.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "range(int(1e6)) | applyS(lambda x: x / 2).all() | ignore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44485087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 72.3 ms, sys: 0 ns, total: 72.3 ms\n",
      "Wall time: 71.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "range(int(1e6)) | apply(lambda x: x / 2) | ignore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d891e471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 73.4 ms, sys: 0 ns, total: 73.4 ms\n",
      "Wall time: 72.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "range(int(1e6)) | apply(applyS(lambda x: x / 2)) | ignore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f716a3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
