{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d06aac2-407e-4073-916b-7adbd2d8819a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    div.jp-OutputArea-output pre {white-space: pre;}\n",
       "    div.output_area pre {white-space: pre;}\n",
       "    div.CodeMirror > div.highlight {overflow-y: auto;}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 19:14:56,648\tINFO worker.py:1458 -- Connecting to existing Ray cluster at address: 192.168.1.17:6379...\n",
      "2024-04-02 19:14:56,656\tINFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from k1lib.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b5cdc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\"\"\"\n",
    "This is for quick modifiers, think of them as changing formats, or modifying an\n",
    "array in place\n",
    "\"\"\"\n",
    "__all__ = [\"applyS\", \"aS\", \"iterDelay\", \"apply\", \"applyMp\", \"parallel\", \"applyCl\",\n",
    "           \"applyTh\", \"applySerial\",\n",
    "           \"sort\", \"sortF\", \"consume\", \"randomize\", \"stagger\", \"op\",\n",
    "           \"integrate\", \"roll\", \"clamp\"]\n",
    "from typing import Callable, Iterator, Any, Union, List, Tuple\n",
    "from k1lib.cli.init import patchDefaultDelim, BaseCli, fastF; import k1lib.cli.init as init\n",
    "import k1lib.cli as cli, numpy as np, threading, gc, random, k1lib\n",
    "from collections import deque, defaultdict; from functools import partial, update_wrapper, lru_cache\n",
    "from k1lib.cli.typehint import *; requests = k1lib.dep.requests\n",
    "import dill, pickle, json, k1lib, warnings, atexit, signal, time, os, random, sys\n",
    "try: import torch; import torch.multiprocessing as mp; hasTorch = True\n",
    "except: import multiprocessing as mp; hasTorch = False\n",
    "try: import ray; hasRay = True\n",
    "except: hasRay = False\n",
    "try: import pandas as pd; pd.core; hasPandas = True\n",
    "except: hasPandas = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e425bb19-fc89-446a-96de-a0464c760f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "settings = k1lib.settings.cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7646c3f-63b8-49f4-8192-5c550dd07703",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cli.init.patchNumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1b8e0dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class applyS(BaseCli):\n",
    "    blurb=\"Applies a function to the input in pipe style\"\n",
    "    def __init__(self, f:Callable[[Any], Any], *args, **kwargs):\n",
    "        \"\"\"Like :class:`apply`, but much simpler, just operating on the entire input\n",
    "object, essentially. The \"S\" stands for \"single\". There's\n",
    "also an alias shorthand for this called :class:`aS`. Example::\n",
    "\n",
    "    # returns 5\n",
    "    3 | aS(lambda x: x+2)\n",
    "\n",
    "Like :class:`apply`, you can also use this as a decorator like this::\n",
    "\n",
    "    @aS\n",
    "    def f(x):\n",
    "        return x+2\n",
    "    # returns 5\n",
    "    3 | f\n",
    "\n",
    "This also decorates the returned object so that it has same qualname, docstring\n",
    "and whatnot.\n",
    "\n",
    ".. admonition:: Shorthands\n",
    "\n",
    "    Writing out \"lambda x:\" all the time is annoying, and there are ways\n",
    "    to quickly say ``lambda x: x+2`` like so::\n",
    "\n",
    "        3 | op()+2 # returns 5\n",
    "        3 | aS(\"x+2\") # returns 5. Behind the scenes, it compiles and execute `lambda x: x+2`\n",
    "\n",
    "    The first way is to use :class:`op`, that will absorb all operations done on it,\n",
    "    like \"+\", and returns a function that essentially replays all the operations.\n",
    "\n",
    "    In the second way, you only have to pass in the string containing code that you want\n",
    "    done on the variable \"x\". Then internally, it will compile to regular Python code.\n",
    "\n",
    "    In fact, you can pass in ``op()`` or just a string to any cli that accepts any kind\n",
    "    of function, like :class:`~k1lib.cli.filt.filt` or :class:`apply`::\n",
    "\n",
    "        range(4) | apply(\"x-2\")  | deref()\n",
    "        range(4) | apply(op()-2) | deref()\n",
    "        range(4) | filt(\"x%2\")   | deref()\n",
    "        range(4) | filt(op()%2)  | deref()\n",
    "\n",
    ":param f: the function to be executed\n",
    ":param kwargs: other keyword arguments to pass to the function, together with ``args``\"\"\"\n",
    "        super().__init__(fs=[f]); self.args = args; self.kwargs = kwargs\n",
    "        self.f = f; self._fC = fastF(f); update_wrapper(self, f, updated=())\n",
    "        self.inverted = False; self.preInvAS = None\n",
    "    def _typehint(self, inp):\n",
    "        if self.hasHint: return self._hint\n",
    "        try: return self.f._typehint(inp)\n",
    "        except: return tAny()\n",
    "    def __ror__(self, it:Any) -> Any: return self._fC(it, *self.args, **self.kwargs)\n",
    "    def _all_array_opt(self, it, level):\n",
    "        res = None; ogShape = tuple(it.shape[:level])\n",
    "        if isinstance(it, np.ndarray):\n",
    "            n = len(it.shape); out = self._fC(np.transpose(it, (*range(level, n), *range(level))), *self.args, **self.kwargs); outN = len(out.shape)\n",
    "            res = np.transpose(out, (*range(outN - level, outN), *range(outN - level)))\n",
    "        if hasTorch and isinstance(it, torch.Tensor):\n",
    "            n = len(it.shape); out = self._fC(torch.transpose_axes(it, (*range(level, n), *range(level))), *self.args, **self.kwargs); outN = len(out.shape)\n",
    "            res = torch.transpose_axes(out, (*range(outN - level, outN), *range(outN - level)))\n",
    "        if res is None: return NotImplemented\n",
    "        elif ogShape != tuple(res.shape[:level]): return NotImplemented\n",
    "        else: return res\n",
    "    def __invert__(self):\n",
    "        \"\"\"Configures it so that it expand the arguments out.\n",
    "Example::\n",
    "\n",
    "    # returns 5\n",
    "    [2, 3] | ~aS(lambda x, y: x + y)\n",
    "\n",
    "    def f(x, y, a=4):\n",
    "        return x*y + a\n",
    "    # returns 10\n",
    "    [2, 3] | ~aS(f)\n",
    "    # returns 11\n",
    "    [2, 3] | ~aS(f, a=5)\"\"\"\n",
    "        if self.inverted: raise Exception(\"Doesn't support __invert__()ing multiple times\")\n",
    "        f = self._fC; a = self.args; kw = self.kwargs; res = aS(lambda x: f(*init.dfGuard(x), *a, **kw));\n",
    "        res.inverted = True; res.preInvAS = self; return res\n",
    "    def _jsF(self, meta):\n",
    "        # if len(self.kwargs) > 0: raise Exception(\"JS does not have the concept of keyword arguments\")\n",
    "        # if len(self.args) > 0: raise Exception(\"aS._jsF() doesn't support *args yet\")\n",
    "        fIdx = init._jsFAuto(); dataIdx = init._jsDAuto(); argIdx = init._jsDAuto(); inverted = False\n",
    "        if self.inverted: self = self.preInvAS; inverted = True\n",
    "        # lookup for custom _jsF() functions\n",
    "        header, _fIdx, _async = k1lib.kast.asyncGuard(k1lib.kast.prepareFunc3(self.f, (\"aS\", meta), self.kwargs, self.args))\n",
    "        # TODO: might want to inject args right here, on the JS side, instead of on the Python side\n",
    "        if inverted: return f\"{header}\\n{fIdx} = {'async ' if _async else ''}({dataIdx}) => {{ return {'await ' if _async else ''}{dataIdx}.aSInv{'_async' if _async else ''}({_fIdx}); }}\", fIdx\n",
    "        else: return header, _fIdx\n",
    "        # for x,y in self.pattern | grep(\"\\ue157\", sep=True).till(\"\\ue239\") | cli.apply(cli.join(\"\")) | cli.filt(\"x\") | cli.apply(lambda x: [x, x.replace(\"\\ue157\", \"${\").replace(\"\\ue239\", \"}\")]): p = p.replace(x, y)\n",
    "        # return f\"const {fIdx} = ({dataIdx}) => {dataIdx}.grep(`{p}`)\", fIdx\n",
    "aS = applyS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa6590fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert 3 | aS(lambda x: x+2) == 5\n",
    "@aS\n",
    "def f(x): return x+2\n",
    "assert 3 | f == 5\n",
    "assert [2, 3] | ~aS(lambda x, y: x + y) == 5\n",
    "def f(x, y, a=4): return x*y + a\n",
    "assert [2, 3] | ~aS(f) == 10\n",
    "assert [2, 3] | ~aS(f, a=5) == 11\n",
    "assert aS(cli.iden())._typehint(tList(int)) == tList(int)\n",
    "assert aS(lambda x: x.split(\"3\"))._typehint(tList(int)) == tAny()\n",
    "assert aS(lambda x: x.split(\"3\")).hint(str)._typehint(tList(int)) == str\n",
    "assert range(4) | cli.apply(\"x-2\") | cli.deref() == [-2, -1, 0, 1]\n",
    "assert range(4) | cli.apply(cli.op()-2) | cli.deref() == [-2, -1, 0, 1]\n",
    "assert range(4) | cli.filt(\"x%2\") | cli.deref() == [1, 3]\n",
    "assert range(4) | cli.filt(cli.op()%2) | cli.deref() == [1, 3]\n",
    "b = np.random.randn(3, 4, 2); tb = torch.tensor(b)\n",
    "a = b | ~aS(lambda a,b,c: a+b+c); assert isinstance(a, np.ndarray); assert a | cli.shape() == (4, 2)\n",
    "a = b | (~aS(lambda a,b,c,d: a+d)).all(); assert isinstance(a, np.ndarray); assert a | cli.shape() == (3, 2)\n",
    "a = b | (~aS(lambda a,b: 2*a+b)).all(2); assert isinstance(a, np.ndarray); assert a | cli.shape() == (3, 4)\n",
    "a = tb | ~aS(lambda a,b,c: a+b+c); assert isinstance(a, torch.Tensor); assert a | cli.shape() == (4, 2)\n",
    "a = tb | (~aS(lambda a,b,c,d: a+d)).all(); assert isinstance(a, torch.Tensor); assert a | cli.shape() == (3, 2)\n",
    "a = tb | (~aS(lambda a,b: 2*a+b)).all(2); assert isinstance(a, torch.Tensor); assert a | cli.shape() == (3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7273ee9d-6308-4e90-89b3-9d49200f5246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class iterDelay(BaseCli):\n",
    "    def __init__(self, n=5):\n",
    "        \"\"\"Iterates through the array, but make sure that there's a bit of a delay.\n",
    "Example::\n",
    "\n",
    "    # returns [0, 1, 2, 3, 4, 5, 6, 7, 8. 9]\n",
    "    range(10) | iterDelay(5) | deref()\n",
    "\n",
    "At first glance, it's not obvious what's going on. Internally, :class:`iterDelay`\n",
    "will fetch 5 items from the input iterator right away and stores them in an\n",
    "internal deque (double-ended queue). Then on the 6th fetch, it will start yielding\n",
    "elements from the deque, hence introducing a delay while iterating through the input.\n",
    "\n",
    "I'd admit that this is pretty niche. I normally would not need such a cli when doing\n",
    "normal analysis, but some clis internally need this, like :class:`apply` when it detects\n",
    "that there's an _all_opt() optimization. Another use case is that prefetch feature\n",
    "of :class:`applyTh`, :class:`applyMp`, :class:`applyCl`\"\"\"\n",
    "        self.n = n\n",
    "    def __ror__(self, it):\n",
    "        if self.n <= 0: return it\n",
    "        sentinel = object(); q = deque(); it = iter(init.dfGuard(it))\n",
    "        for i in range(self.n):\n",
    "            e = next(it, sentinel)\n",
    "            if e is sentinel: break\n",
    "            q.append(e)\n",
    "        def g():\n",
    "            while True:\n",
    "                e = next(it, sentinel)\n",
    "                if e is sentinel: break\n",
    "                q.append(e); yield q.popleft()\n",
    "            while len(q) > 0: yield q.popleft()\n",
    "        return g()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7202e89e-01eb-45e3-8f27-52bc6551e6bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert range(10) | iterDelay() | cli.deref() == list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf823145-956f-4e60-915f-40b7c94aee8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# this section of code is a bit magical. The internal dynamics are so damn chaotic, but somehow, they all\n",
    "# work together beautifully\n",
    "def _allOpt_gen(a, q, n:int): # a is a complex, not deref-ed structure. Returns flattened a\n",
    "    if n == 0: yield a; return\n",
    "    c = 0; idx = len(q); q.append(None)\n",
    "    for e in a: c += 1; yield from _allOpt_gen(e, q, n-1)\n",
    "    q[idx] = c\n",
    "def _allOpt_genIr(a, n) -> \"(flattened structure, ir, depth)\": # a is a complex, not deref-ed structure\n",
    "    q = deque(); return _allOpt_gen(a, q, n), q, n\n",
    "def _allOpt_recover(b:Iterator[\"data_structure\"], ir:\"Deque[int]\", irIdx:Iterator[int], n): # assumes b and ir are iterators\n",
    "    l = next(irIdx)\n",
    "    for i in range(ir[l]): yield list(_allOpt_recover(b, ir, irIdx, n-1)) if n-1 > 0 else next(b)\n",
    "def _allOpt_recover(b:Iterator[\"data_structure\"], ir:\"Deque[int]\", irIdx:Iterator[int], n): # assumes b and ir are iterators\n",
    "    l = next(irIdx); c = 0\n",
    "    while True:\n",
    "        if ir[l] != None and ir[l] <= c: break\n",
    "        c += 1; yield list(_allOpt_recover(b, ir, irIdx, n-1)) if n-1 > 0 else next(b) # that list() right there is quite crucial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "758a9d31-59e5-481e-bc24-5302834bdbb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ag = lambda: iter([[\"a\", \"abc\", \"ab\"], [], [\"1234\", \"\", \"12\", \"12345\"]]) # (2, 3, 1)\n",
    "assert _allOpt_genIr(ag(), 3) | cli.deref() | cli.op()[1] == [3, 3, 1, 3, 2, 0, 4, 4, 0, 2, 5]\n",
    "# if derefed everything\n",
    "af, ir, n = _allOpt_genIr(ag(), 3) | cli.deref() # af = a flat\n",
    "assert _allOpt_recover(af | cli.apply('x+\"|\"'), ir, iter(range(int(1e9))), n) | cli.join(\"\").all(2) | cli.deref() == [['a|', 'a|b|c|', 'a|b|'], [], ['1|2|3|4|', '', '1|2|', '1|2|3|4|5|']]\n",
    "# incremental deref-ing\n",
    "af, ir, n = _allOpt_genIr(ag(), 3); af = af | iterDelay(1)\n",
    "assert _allOpt_recover(af | cli.apply('x+\"|\"'), ir, iter(range(int(1e9))), n) | cli.join(\"\").all(2) | cli.deref() == [['a|', 'a|b|c|', 'a|b|'], [], ['1|2|3|4|', '', '1|2|', '1|2|3|4|5|']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "473a6379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def infIter():\n",
    "    i = 0;\n",
    "    while True: yield i; i += 1\n",
    "class apply(BaseCli):\n",
    "    blurb=\"Applies a function to all input elements\"\n",
    "    def __init__(self, f:Callable[[Any], Any], column:Union[int, List[int]]=None, cache:int=0, **kwargs):\n",
    "        \"\"\"Applies a function f to every element in the incoming list/iterator.\n",
    "Example::\n",
    "\n",
    "    # returns [0, 1, 4, 9, 16]\n",
    "    range(5) | apply(lambda x: x**2) | deref()\n",
    "    # returns [[3.0, 1.0, 1.0], [3.0, 1.0, 1.0]], running the function on the 0th column\n",
    "    torch.ones(2, 3) | apply(lambda x: x+2, 0) | deref()\n",
    "    # returns [[0, -1, 2, 3, -4], [2, -3, 4, 5, -6], [0, -1, 4, 9, -16]], running the function on the 1st (0-index btw) and 4th columns\n",
    "    [[0, 1, 2, 3, 4], [2, 3, 4, 5, 6], [0, 1, 4, 9, 16]] | apply(lambda x: -x, [1, 4]) | deref()\n",
    "\n",
    "You can also use this as a decorator, like this::\n",
    "\n",
    "    @apply\n",
    "    def f(x):\n",
    "        return x**2\n",
    "    # returns [0, 1, 4, 9, 16]\n",
    "    range(5) | f | deref()\n",
    "\n",
    "You can also add a cache, like this::\n",
    "\n",
    "    def calc(i): time.sleep(0.5); return i**2\n",
    "    # takes 2.5s\n",
    "    range(5) | repeatFrom(2) | apply(calc, cache=10) | deref()\n",
    "    # takes 5s\n",
    "    range(5) | repeatFrom(2) | apply(calc) | deref()\n",
    "\n",
    "You can add custom keyword arguments into the function::\n",
    "\n",
    "    def f(x, y, z=3):\n",
    "        return x + y + z\n",
    "    # returns [15, 17, 19, 21, 23]\n",
    "    [range(5), range(10, 15)] | transpose() | ~apply(f, z=5) | deref()\n",
    "\n",
    "Slight reminder that you can't pass in extra positional args like in :class:`aS`, just extra keyword arguments.\n",
    "\n",
    "See also: :class:`aS`, :class:`~k1lib.cli.filt.filt`\n",
    "\n",
    ".. admonition:: JS transpiler notes\n",
    "\n",
    "    So, because JS don't have the concept of keyword arguments, ``kwargs`` will have its values\n",
    "    extracted, then injected as positional arguments in the transpiled JS function.\n",
    "\n",
    ":param column: if not None, then applies the function to that column or columns only\n",
    ":param cache: if specified, then caches this much number of values\n",
    ":param kwargs: extra keyword arguments to pass in the function\"\"\"\n",
    "        super().__init__(fs=[f]); self.f = f; self.kwargs = kwargs # f is the original operator, _fC is \n",
    "        if column: # quick type checks\n",
    "            ex = Exception(f\"Applying a function on a negative-indexed column ({column}) is not supported\")\n",
    "            if isinstance(column, int):\n",
    "                if column < 0: raise ex\n",
    "            else:\n",
    "                column = list(column)\n",
    "                if len([c for c in column if c < 0]): raise ex\n",
    "        self.column = column; self.cache = cache; self._fC = fastF(f)\n",
    "        if cache > 0: self._fC = lru_cache(cache)(self._fC)\n",
    "        self.normal = self.column is None and self.cache == 0 and len(kwargs) == 0 # cached value to say that this apply is just being used as a wrapper, nothing out of the ordinary, like custom columns, cache or custom kwargs\n",
    "        if self.normal: # just propagating information upward, to save runtime graph analysis time\n",
    "            try: self._propagatedF = f._propagatedF; self._applyDepth = f._applyDepth + 1 # assuming f is another apply()\n",
    "            except: self._propagatedF = f; self._applyDepth = 1\n",
    "        else: self._propagatedF = None; self._applyDepth = 1 # might have to rethink if this depth should be 1 or not\n",
    "        # optimization 1: BaseCli._all_array_opt(), aimed at accelerating array types\n",
    "        self.__arrayTypeF = None # None for not formulated yet, 0 for cannot formulate a faster operation, else the cached, accelerated function (that might not work)\n",
    "        # optimization 2: BaseCli._all_opt(), aimed at accelerating language models\n",
    "        self.__allOptF = None # None for not formulated yet, 0 for cannot formulate a faster operation, else the cached, accelerated function (that will guaranteed to work)\n",
    "        self.inverted = False; self.preInvApply = None # for ._jsF(), to contain information about the apply() pre __invert__(), so that .jsF() can extract out information\n",
    "    @property\n",
    "    def _arrayTypeF(self): # optimization 1: returns None or the function (that might not work)\n",
    "        if self.__arrayTypeF == 0: return None\n",
    "        if self.__arrayTypeF is None:\n",
    "            arrs = []; last = self # figure out the depth\n",
    "            while isinstance(last, apply) and last.normal: arrs.append(last); last = last.f\n",
    "            depth = len(arrs)\n",
    "            if depth == 0: self.__arrayTypeF = 0; return None\n",
    "            if isinstance(last, cli.serial): # breaks up the serial: (A | B.all(2)).all(3) -> A.all(3) | B.all(5)\n",
    "                self.__arrayTypeF = cli.serial(*[(e if isinstance(e, BaseCli) else aS(e)).all(depth) for e in last.clis]); return self.__arrayTypeF\n",
    "            else: # it | A.all(3) -> A._all_array_opt(it, 3). This function might return NotImplemented, which means it can't figure out how to utilize the speed up\n",
    "                if not hasattr(last, \"_all_array_opt\"): last = aS(last)\n",
    "                self.__arrayTypeF = lambda it: last._all_array_opt(it, depth); return self.__arrayTypeF\n",
    "        return self.__arrayTypeF\n",
    "    @property\n",
    "    def _allOptF(self): # optimization 2: returns None or the function (that has to work all the time!)\n",
    "        if self.__allOptF == 0: return None\n",
    "        if self._propagatedF is None or not hasattr(self._propagatedF, \"_all_opt\"): self.__allOptF = 0; return None\n",
    "        f = self._propagatedF._all_opt\n",
    "        def inner(it): # has to regenerate the IR on each pass through. Slow (O(30*n) or so), but the function this is supposed to run (LLMs), are even slower, so this is fine for now            \n",
    "            af, ir, n = _allOpt_genIr(it, self._applyDepth) # af = a flat\n",
    "            af = af | iterDelay()\n",
    "            return _allOpt_recover(iter(f(af)), ir, infIter(), n)\n",
    "        return inner\n",
    "    def _typehint(self, inp):\n",
    "        if self.column is None:\n",
    "            if isinstance(inp, tListIterSet):\n",
    "                try: return tIter(self.f._typehint(inp.child))\n",
    "                except: return tIter(tAny())\n",
    "        return super()._typehint(inp)\n",
    "    def _copy(self): return apply(self.f, self.column, self.cache, **self.kwargs) # ~apply() case handled automatically\n",
    "    def __ror__(self, it:Iterator[str]):\n",
    "        c = self.column; f = self._fC; ogF = self.f; farr = (getattr(ogF, \"_all_opt2\", None) or f); kwargs = self.kwargs\n",
    "        if c is None:\n",
    "            if self.normal:\n",
    "                if isinstance(it, settings.arrayTypes): # optimization 1\n",
    "                    af = self._arrayTypeF\n",
    "                    if af is not None: # there're lots of code here, but it doesn't impact perf cause it's done once for each array object\n",
    "                        try:\n",
    "                            ans = af(it)\n",
    "                            if ans is not NotImplemented: return ans\n",
    "                        except init.ArrayOptException as e: raise e # this is a special exception. If the function throws this then the error relates to incorrect constraints, and not accidental, so shouldn't be ignored\n",
    "                        except: pass\n",
    "                        self.__arrayTypeF = 0 # tried to use the accelerated version, but failed, so won't ever try the accelerated version again\n",
    "                elif self._allOptF is not None: return self._allOptF(it) # optimization 2, for LLMs\n",
    "                elif hasPandas and isinstance(it, pd.core.arraylike.OpsMixin):\n",
    "                    it = init.dfGuard(it)\n",
    "                    try:\n",
    "                        res = farr(it)\n",
    "                        if res is not NotImplemented: return res\n",
    "                    except: return np.array([f(x) for x in it])\n",
    "            return (f(line, **kwargs) for line in init.dfGuard(it))\n",
    "        elif isinstance(c, int):\n",
    "            if hasPandas and isinstance(it, pd.DataFrame):\n",
    "                it = it.copy(); colN = list(it)[c]\n",
    "                try: it[colN] = f(it[colN])\n",
    "                except: it[colN] = [f(x) for x in it[colN]]\n",
    "                return it\n",
    "            def gen(it):\n",
    "                for row in it: row = list(row); row[c] = f(row[c], **kwargs); yield row\n",
    "            return gen(it) # return ([(e if i != c else f(e, **kwargs)) for i, e in enumerate(row)] for row in it) # old version\n",
    "        else: # List[int]\n",
    "            if hasPandas and isinstance(it, pd.DataFrame):\n",
    "                it = it.copy()\n",
    "                for c_ in c:\n",
    "                    colN = list(it)[c_]\n",
    "                    try: it[colN] = f(it[colN])\n",
    "                    except: it[colN] = [f(x) for x in it[colN]]\n",
    "                return it\n",
    "            def gen(it):\n",
    "                for row in it:\n",
    "                    row = list(row)\n",
    "                    for c_ in c: row[c_] = f(row[c_], **kwargs)\n",
    "                    yield row\n",
    "            return gen(it)\n",
    "    def __invert__(self):\n",
    "        \"\"\"Same mechanism as in :class:`applyS`, it expands the\n",
    "arguments out. Just for convenience really. Example::\n",
    "\n",
    "    # returns [10, 12, 14, 16, 18]\n",
    "    [range(5), range(10, 15)] | transpose() | ~apply(lambda x, y: x+y) | deref()\"\"\"\n",
    "        if self.inverted: raise Exception(\"Doesn't support _invert__()ing multiple times\")\n",
    "        res = apply(lambda x: self._fC(*x, **self.kwargs), self.column, self.cache)\n",
    "        res.preInvApply = self; res.inverted = True; return res\n",
    "    def _jsF(self, meta):\n",
    "        if self.cache != 0: raise Exception(\"apply._jsF() doesn't support caching values yet\")\n",
    "        fIdx = init._jsFAuto(); dataIdx = init._jsDAuto(); kwIdx = init._jsDAuto(); argIdx = init._jsDAuto(); inverted = False\n",
    "        if self.inverted: self = self.preInvApply; inverted = True\n",
    "        header, _fIdx, _async = k1lib.kast.asyncGuard(k1lib.kast.prepareFunc3(self.f, (\"apply\", meta), self.kwargs))\n",
    "        return f\"{header}\\n{kwIdx} = {json.dumps(self.kwargs)};\\n{fIdx} = {'async ' if _async else ''}({dataIdx}) => {dataIdx}.apply{'_async' if _async else ''}({'async ' if _async else ''}({argIdx}) => {'await ' if _async else ''}{_fIdx}({'...' if inverted else ''}{argIdx}), {cli.kjs.v(self.column)}, {kwIdx}, false)\", fIdx\n",
    "\n",
    "        # old code below, with args (self, kast, jsFnVars:\"list[str]\", **kwargs)\n",
    "        argVars = kast.kast_lambda(self.f); var = \",\".join(argVars)\n",
    "        fn, header = kast.kast_prepareFunc(self.f, [*argVars, *jsFnVars])\n",
    "        col = self.column\n",
    "        if col is None: return f\".apply(({var}) => {fn})\", header\n",
    "        else:\n",
    "            cols = [col] if isinstance(col, int) else col\n",
    "            return \"\".join([f\".apply(({var}) => {fn}, {c})\" for c in cols]), header\n",
    "    def _pyF(self, expr, **kw):\n",
    "        if self.column is not None: return None, None, NotImplemented, None\n",
    "        vD = {}; kwS = \"\"; varAuto = init._pyFAuto; rowN = varAuto(); inverted = \"\"\n",
    "        if self.inverted: inverted = \"*\"; self = self.preInvApply\n",
    "        for k,v in self.kwargs: varN = varAuto(); vD[varN] = v; kwS += f\", {k}={varN}\"\n",
    "        funcN = varAuto(); vD[funcN] = self._fC\n",
    "        return \"\", \"\", f\"({funcN}({inverted}{rowN}{kwS}) for {rowN} in {expr})\", vD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bf1c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert range(5) | apply(lambda x: x**2) | cli.deref() == [0, 1, 4, 9, 16]\n",
    "assert torch.ones(2, 3) | apply(lambda x: x+2, 0) | cli.deref() == [[3.0, 1.0, 1.0], [3.0, 1.0, 1.0]]\n",
    "@apply\n",
    "def f(x): return x**2\n",
    "assert range(5) | f | cli.deref() == [0, 1, 4, 9, 16]\n",
    "def calc(i): time.sleep(0.1); return i**2\n",
    "with k1lib.timer() as t1:\n",
    "    range(5) | cli.repeatFrom(2) | apply(calc, cache=10) | cli.deref()\n",
    "with k1lib.timer() as t2:\n",
    "    range(5) | cli.repeatFrom(2) | apply(calc) | cli.deref()\n",
    "assert t1() < t2()*0.7\n",
    "assert [range(5), range(10, 15)] | cli.transpose() | ~apply(lambda x, y: x+y) | cli.deref() == [10, 12, 14, 16, 18]\n",
    "assert apply(aS(lambda x: x + 3).hint(int))._typehint(tIter(float)) == tIter(int)\n",
    "assert apply(aS(lambda x: x + 3))._typehint(tIter(float)) == tIter(tAny())\n",
    "def f(x, y, z=3): return x + y + z\n",
    "assert [range(5), range(10, 15)] | cli.transpose() | ~apply(f, z=5) | cli.deref() == [15, 17, 19, 21, 23]\n",
    "assert [[0, 1, 2, 3, 4], [2, 3, 4, 5, 6], [0, 1, 4, 9, 16]] | apply(lambda x: -x, [1, 4]) | cli.deref() == [[0, -1, 2, 3, -4], [2, -3, 4, 5, -6], [0, -1, 4, 9, -16]]\n",
    "assert np.random.randn(4, 2) | ~apply(lambda x,y: x-y) | cli.shape() == (4,)\n",
    "assert np.random.randn(4, 2) | (~aS(lambda x,y: x-y)).all() | cli.shape() == (4,)\n",
    "a = np.random.randn(3, 4, 2) | (~cli.apply(lambda x,y: x-y)).all(); assert isinstance(a, np.ndarray); assert a | cli.shape() == (3, 4)\n",
    "df1 = pd.DataFrame({\"A\": [1.0,2,3,4], \"B\": pd.Timestamp(\"20130102\"), \"C\": pd.Series(1, index=list(range(4)), dtype=\"float32\"), \"D\": np.array([3] * 4, dtype=\"int32\"), \"E\": pd.Categorical([\"test\", \"train\", \"test\", \"train\"]), \"F\": \"foo\",})\n",
    "assert isinstance(df1 | apply(\"x+2\", 3) | deref(), pd.core.frame.DataFrame)\n",
    "assert isinstance(df1 | apply(\"x+2\", [0,3]) | deref(), pd.core.frame.DataFrame)\n",
    "assert isinstance(df1 | apply(lambda x: x.startswith(\"te\"), 4) | deref(), pd.core.frame.DataFrame)\n",
    "assert isinstance(df1[\"A\"] | apply(lambda x: x*3), pd.core.series.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4bb187b-5a02-44dc-bef0-f6d55a770808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#longtest\n",
    "a = [[\"abc\", \"def\", \"ghi\"], [\"123\", \"456\"]] # verify that _all_opt() works\n",
    "res = a | apply(apply(cli.embed())) | cli.deref()\n",
    "assert isinstance(res, list); assert len(res | cli.shape()) == 3\n",
    "assert res | cli.shape() | cli.head(2) == (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4500b057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "cloudpickle = k1lib.dep(\"cloudpickle\", url=\"https://github.com/cloudpipe/cloudpickle\")\n",
    "def executeFunc(common, line, usingDill):\n",
    "    import time\n",
    "    if usingDill:\n",
    "        import dill; f, kwargs = dill.loads(common)\n",
    "        res = f(dill.loads(line), **kwargs)\n",
    "    else:\n",
    "        import cloudpickle; f, kwargs = cloudpickle.loads(common)\n",
    "        res = f(cloudpickle.loads(line), **kwargs)\n",
    "    time.sleep(0.1); return res # suggestion by https://stackoverflow.com/questions/36359528/broken-pipe-error-with-multiprocessing-queue\n",
    "def terminateGraceful(): signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "_k1_applyMp_global_ctx = {}; _k1_applyMp_global_ctx_autoInc = k1lib.AutoIncrement(prefix=\"_k1_applyMp\")\n",
    "class applyMp(BaseCli):\n",
    "    blurb=\"Applies a function to all input elements across multiple processes\"\n",
    "    _pools = set()\n",
    "    _torchNumThreads = None\n",
    "    def __init__(self, f:Callable[[Any], Any], prefetch:int=None, timeout:float=8, utilization:float=0.8, bs:int=1, newPoolEvery:int=0, **kwargs):\n",
    "        \"\"\"Like :class:`apply`, but execute a function over the input iterator\n",
    "in multiple processes. Example::\n",
    "\n",
    "    # returns [3, 2]\n",
    "    [\"abc\", \"de\"] | applyMp(len) | deref()\n",
    "    # returns [5, 6, 9]\n",
    "    range(3) | applyMp(lambda x, bias: x**2+bias, bias=5) | deref()\n",
    "    \n",
    "    # returns [[1, 2, 3], [1, 2, 3]], demonstrating outside vars work\n",
    "    someList = [1, 2, 3]\n",
    "    [\"abc\", \"de\"] | applyMp(lambda s: someList) | deref()\n",
    "\n",
    "Internally, this will continuously spawn new jobs up until 80% of all CPU\n",
    "cores are utilized. On posix systems, the default multiprocessing start method is\n",
    "``fork()``. This sort of means that all the variables in memory will be copied\n",
    "over. On windows and macos, the default start method is ``spawn``, meaning each\n",
    "child process is a completely new interpreter, so you have to pass in all required\n",
    "variables and reimport every dependencies. Read more at https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n",
    "\n",
    "If you don't wish to schedule all jobs at once, you can specify a ``prefetch``\n",
    "amount, and it will only schedule that much jobs ahead of time. Example::\n",
    "\n",
    "    range(10000) | applyMp(lambda x: x**2)    | head() | deref() # 700ms\n",
    "    range(10000) | applyMp(lambda x: x**2, 5) | head() | deref() # 300ms\n",
    "\n",
    "    # demonstrating there're no huge penalties even if we want all results at the same time\n",
    "    range(10000) | applyMp(lambda x: x**2)    | deref() # 900ms\n",
    "    range(10000) | applyMp(lambda x: x**2, 5) | deref() # 1000ms\n",
    "\n",
    "The first line will schedule all jobs at once, and thus will require more RAM and\n",
    "compute power, even though we discard most of the results anyway (the\n",
    ":class:`~k1lib.cli.filt.head` cli). The second line only schedules 5 jobs ahead of\n",
    "time, and thus will be extremely more efficient if you don't need all results right\n",
    "away.\n",
    "\n",
    ".. note::\n",
    "\n",
    "    Remember that every :class:`~k1lib.cli.init.BaseCli` is also a\n",
    "    function, meaning that you can do stuff like::\n",
    "\n",
    "        # returns [['ab', 'ac']]\n",
    "        [[\"ab\", \"cd\", \"ac\"]] | applyMp(filt(op().startswith(\"a\")) | deref()) | deref()\n",
    "\n",
    "    Also remember that the return result of ``f`` should be serializable, meaning it\n",
    "    should not be a generator. That's why in the example above, there's a ``deref()``\n",
    "    inside f. You should also convert PyTorch tensors into Numpy arrays\n",
    "\n",
    "Most of the time, you would probably want to specify ``bs`` to something bigger than 1\n",
    "(may be 32 or sth like that). This will executes ``f`` multiple times in a single job,\n",
    "instead of executing ``f`` only once per job. Should reduce overhead of process\n",
    "creation dramatically.\n",
    "\n",
    "If you encounter strange errors not seen on :class:`apply`, you can try to clear all\n",
    "pools (using :meth:`clearPools`), to terminate all child processes and thus free\n",
    "resources. On earlier versions, you have to do this manually before exiting, but now\n",
    ":class:`applyMp` is much more robust.\n",
    "\n",
    "Also, you should not immediately assume that :class:`applyMp` will always be faster\n",
    "than :class:`apply`. Remember that :class:`applyMp` will create new processes,\n",
    "serialize and transfer data to them, execute it, then transfer data back. If your code\n",
    "transfers a lot of data back and forth (compared to the amount of computation done), or\n",
    "the child processes don't have a lot of stuff to do before returning, it may very well\n",
    "be a lot slower than :class:`apply`.\n",
    "\n",
    "There's a potential loophole here that can make your code faster. Because the main\n",
    "process is forked (at least on linux), every variable is still there, even the big\n",
    "ones. So, you can potentially do something like this::\n",
    "\n",
    "    bigData = [] # 1B items in the list\n",
    "    # summing up all items together. No input data transfers (because it's forked instead)\n",
    "    range(1_000_000_000) | batched(100) | applyMp(lambda r: r | apply(lambda i: bigData[i]) | toSum()) | toSum()\n",
    "\n",
    "In fact, I use this loophole all the time, and thus has made the function :meth:`shared`,\n",
    "so check it out.\n",
    "\n",
    ":param prefetch: if not specified, schedules all jobs at the same time. If\n",
    "    specified, schedules jobs so that there'll only be a specified amount of\n",
    "    jobs, and will only schedule more if results are actually being used.\n",
    ":param timeout: seconds to wait for job before raising an error\n",
    ":param utilization: how many percent cores are we running? 0 for no cores, 1 for\n",
    "    all the cores. Defaulted to 0.8\n",
    ":param bs: if specified, groups ``bs`` number of transforms into 1 job to be more\n",
    "    efficient.\n",
    ":param kwargs: extra arguments to be passed to the function. ``args`` not\n",
    "    included as there're a couple of options you can pass for this cli.\n",
    ":param newPoolEvery: creates a new processing pool for every specific amount of input\n",
    "    fed. 0 for not refreshing any pools at all. Turn this on in case your process consumes\n",
    "    lots of memory and you have to kill them eventually to free up some memory\"\"\"\n",
    "        super().__init__(fs=[f]); self.f = fastF(f)\n",
    "        self.prefetch = prefetch or int(1e9)\n",
    "        self.timeout = timeout; self.utilization = utilization\n",
    "        self.bs = bs; self.kwargs = kwargs; self.p = None\n",
    "        self.newPoolEvery = newPoolEvery; self.ps = []; self._serializeF = True\n",
    "    def __ror__(self, it:Iterator[Any]) -> Iterator[Any]:\n",
    "        timeout = self.timeout; f = self.f # really make sure it's an iterator, for prefetch\n",
    "        if self.bs > 1: return it | cli.batched(self.bs, True) | applyMp(apply(f) | cli.toList(), self.prefetch, timeout, **self.kwargs) | cli.joinStreams()\n",
    "        def newPool():\n",
    "            if hasTorch:\n",
    "                try: applyMp._torchNumThreads = applyMp._torchNumThreads or torch.get_num_threads(); torch.set_num_threads(1)\n",
    "                except: pass # why do all of this? Because some strange interaction between PyTorch and multiprocessing, outlined here: https://github.com/pytorch/pytorch/issues/82843\n",
    "            os.environ[\"py_k1lib_in_applyMp\"] = \"True\"\n",
    "            self.p = mp.Pool(int(mp.cpu_count()*self.utilization), terminateGraceful); self.ps.append(self.p)\n",
    "            if hasTorch and applyMp._torchNumThreads is not None: torch.set_num_threads(applyMp._torchNumThreads)\n",
    "        def intercept(it, n):\n",
    "            for i, e in enumerate(it):\n",
    "                if i % n == 0:\n",
    "                    if self.p is not None: self.p.close(); self.ps.remove(self.p)\n",
    "                    gc.collect(); newPool()\n",
    "                yield e\n",
    "        try: common = dill.dumps([f, self.kwargs]); usingDill = True\n",
    "        except: common = cloudpickle.dumps([f, self.kwargs]); usingDill = False\n",
    "        def gen(it):\n",
    "            with k1lib.captureStdout(False, True) as out:\n",
    "                try:\n",
    "                    if self.newPoolEvery > 0: it = intercept(it, self.newPoolEvery)\n",
    "                    else: newPool()\n",
    "                    yield from it | apply(lambda line: self.p.apply_async(executeFunc, [common, dill.dumps(line), usingDill])) | iterDelay(self.prefetch) | apply(lambda x: x.get(timeout))\n",
    "                except KeyboardInterrupt as e:\n",
    "                    print(\"applyMp interrupted. Terminating pool now\")\n",
    "                    for p in self.ps: p.close(); p.terminate();\n",
    "                    raise e\n",
    "                except Exception as e:\n",
    "                    print(\"applyMp encounter errors. Terminating pool now\")\n",
    "                    for p in self.ps: p.close(); p.terminate();\n",
    "                    raise e\n",
    "                else:\n",
    "                    for p in self.ps: p.close(); p.terminate();\n",
    "        return gen(it)\n",
    "    @staticmethod\n",
    "    def cat(fileName: str, f:Callable, n:int=None, rS=None, **kwargs):\n",
    "        \"\"\"Like :meth:`applyCl.cat`, this will split a file up into multiple\n",
    "sections, execute ``f`` over all sections and return the results.\n",
    "Example::\n",
    "\n",
    "    fn = \"~/repos/labs/k1lib/k1lib/cli/test/applyMp.cat\"\n",
    "    \"0123456789\\\\n\"*100 | file(fn)\n",
    "    # returns [6, 6, 6, 7, 6, 6, 6, 7, 6, 6, 6, 7, 6, 6, 6, 8]\n",
    "    applyMp.cat(fn, shape(0), 16) | deref()\n",
    "\n",
    ":param f: function to execute on an iterator of lines\n",
    ":param n: how many chunks should it split the file into. Defaulted to the number of cpu cores available\n",
    ":param rS: :class:`~k1lib.cli.inp.refineSeek` instance, if you need more fine-grained\n",
    "    control over section boundaries so as to not make everything corrupted\n",
    ":param kwargs: extra keyword arguments for :class:`applyMp`\"\"\"\n",
    "        return fileName | cli.splitSeek(n or os.cpu_count()) | (rS or cli.iden()) | cli.window(2) | ~applyMp(lambda x,y: cli.cat(fileName, sB=x, eB=y) | f, **kwargs)\n",
    "    @staticmethod\n",
    "    def shared(f, **kwargs):\n",
    "        \"\"\"Execution model where the input iterator is dereferenced and shared across\n",
    "all processes, bypassing serialization. Example::\n",
    "\n",
    "    a = range(1_000_000_000) | apply(lambda x: x*1.5 - 2000) | aS(list) # giant data structure\n",
    "    a | batched(50_000_000, True) | applyMp(toSum()) | toSum() # has to serialize and deserialize lists of numbers, which wastes lots of cpu cycles and memory\n",
    "    a | applyMp.shared(toSum()) | toSum() # giant data structure is forked, no serialization happens, no memory even gets copied, much faster\n",
    "\n",
    "In the 2nd line, most of the time is spent on serializing the data and transferring\n",
    "it to other processes, while in the 3rd line, most of the time is spent on calculating\n",
    "the sum instead, as the giant data structure is forked, and Linux doesn't copy it internally.\"\"\"\n",
    "        def inner(it):\n",
    "            try: n = len(it)\n",
    "            except: it = list(it); n = len(it)\n",
    "            # this is pretty unintuitive right? Why do it this way? Turns out, if you were to reference `it` directly, it will store it in f's co_freevars,\n",
    "            # which will be serialized, defeating the purpose. Moving it to a global variable forces it to move to co_names instead, avoiding serialization. This took forever to understand\n",
    "            idx = _k1_applyMp_global_ctx_autoInc(); _k1_applyMp_global_ctx[idx] = it\n",
    "            res = range(n) | cli.batched(round(n/os.cpu_count()+1), True) | applyMp(lambda r: f(_k1_applyMp_global_ctx[idx][r.start:r.stop]), **kwargs) | aS(list)\n",
    "            _k1_applyMp_global_ctx[idx] = None; return res\n",
    "        return aS(inner)\n",
    "    def _copy(self): return applyMp(self.f, self.prefetch, self.timeout, self.utilization, self.bs, self.newPoolEvery, **self.kwargs)\n",
    "    def __invert__(self):\n",
    "        \"\"\"Expands the arguments out, just like :class:`apply`.\n",
    "Example::\n",
    "\n",
    "    # returns [20, 20, 18, 14, 8, 0, -10, -22, -36, -52]\n",
    "    [range(10), range(20, 30)] | transpose() | ~applyMp(lambda x, y: y-x**2) | deref()\"\"\"\n",
    "        res = self._copy(); f = res.f; res.f = lambda x: f(*x); return res\n",
    "    @staticmethod\n",
    "    def clearPools():\n",
    "        \"\"\"Terminate all existing pools. Do this before restarting/quitting the\n",
    "script/notebook to make sure all resources (like GPU) are freed. **Update**:\n",
    "you probably won't have to call this manually anymore since version 0.9, but\n",
    "if you run into problems, try doing this.\"\"\"\n",
    "        for p in applyMp._pools:\n",
    "            try: p.terminate()\n",
    "            except: pass\n",
    "        applyMp._pools = set()\n",
    "    @staticmethod\n",
    "    def pools():\n",
    "        \"\"\"Get set of all pools. Meant for debugging purposes only.\"\"\"\n",
    "        return applyMp._pools\n",
    "    def __del__(self):\n",
    "        return\n",
    "        if hasattr(self, \"p\"):\n",
    "            self.p.terminate();\n",
    "            if self.p in applyMp._pools: applyMp._pools.remove(self.p)\n",
    "# apparently, this doesn't do anything, at least in jupyter environment\n",
    "atexit.register(lambda: applyMp.clearPools())\n",
    "parallel = applyMp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af69270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#longtest\n",
    "assert [\"abc\", \"de\"] | applyMp(len) | cli.deref() == [3, 2]\n",
    "assert range(3) | applyMp(lambda x, bias: x**2+bias, bias=5) | cli.deref() == [5, 6, 9]\n",
    "someList = [1, 2, 3]\n",
    "assert [\"abc\", \"de\"] | applyMp(lambda s: someList) | cli.deref() == [[1, 2, 3], [1, 2, 3]]\n",
    "assert [[\"ab\", \"cd\", \"ac\"]] | applyMp(cli.filt(cli.op().startswith(\"a\")) | cli.deref()) | cli.deref() == [['ab', 'ac']]\n",
    "beginTime = time.time(); range(100000) | applyMp(lambda x: x**2) | cli.head() | cli.deref(); time1 = time.time() - beginTime\n",
    "beginTime = time.time(); range(100000) | applyMp(lambda x: x**2, 5) | cli.head() | cli.deref(); time2 = time.time() - beginTime\n",
    "assert time1 > time2\n",
    "# 1 and 3 are essentially identical, to make sure no dubious copy-on-write is happening\n",
    "with k1lib.timer() as t1: res1 = range(100) | applyMp(lambda x: x**2) | cli.deref() | cli.head() | cli.deref()\n",
    "with k1lib.timer() as t2: res2 = range(100) | applyMp(lambda x: x**2, prefetch=10, bs=32) | cli.deref() | cli.head() | cli.deref()\n",
    "with k1lib.timer() as t3: res3 = range(100) | applyMp(lambda x: x**2) | cli.deref() | cli.head() | cli.deref()\n",
    "assert t1() > t2(); assert t3() > t2(); assert res1 == res2 == res3\n",
    "assert range(100) | applyMp(lambda x: x**2, newPoolEvery=10) | cli.deref() == range(100) | apply(lambda x: x**2) | cli.deref()\n",
    "assert [range(10), range(20, 30)] | cli.transpose() | ~applyMp(lambda x, y: y-x**2) | cli.deref() == [20, 20, 18, 14, 8, 0, -10, -22, -36, -52]\n",
    "fn = \"~/repos/labs/k1lib/k1lib/cli/test/applyMp.cat\"\n",
    "\"0123456789\\n\"*100 | cli.file(fn)\n",
    "assert applyMp.cat(fn, cli.shape(0), 16) | cli.deref() == [7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 8]\n",
    "data = range(1_000_000) | apply(lambda x: x*1.5 - 2000) | aS(list) # giant data structure\n",
    "with k1lib.timer() as t1: a = data | cli.batched(50_000, True) | applyMp(cli.toSum()) | cli.toSum() # has to serialize and deserialize lists of numbers, which wastes lots of cpu cycles and memory\n",
    "with k1lib.timer() as t2: b = data | applyMp.shared(cli.toSum()) | cli.toSum() # giant data structure is forked, no serialization happens, no memory even gets copied, much faster\n",
    "assert t1() > t2(); assert a == b; del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ee17aa-38fe-4b0e-9107-fba75d5b7e7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "s = k1lib.Settings(); settings.add(\"applyCl\", s, \"modifier.applyCl() settings\")\n",
    "s.add(\"sudoTimeout\", 300, \"seconds before deleting the stored password for sudo commands\")\n",
    "s.add(\"cpuLimit\", None, \"if specified (int), will not schedule more jobs if the current number of assigned cpus exceeds this\")\n",
    "_password = k1lib.Wrapper(None); _cpuUsed = k1lib.Wrapper(0)\n",
    "def removePw():\n",
    "    \"\"\"Thread that deletes stored password for applyCl sudo commands\"\"\"\n",
    "    while True: time.sleep(settings.applyCl.sudoTimeout); _password.value = None\n",
    "_removePwStarted = [False]\n",
    "def _start_removePwThread():\n",
    "    if _removePwStarted[0]: return\n",
    "    _removePwStarted[0] = True; t = threading.Thread(target=removePw, daemon=True).start()\n",
    "_nodeIdsCache = k1lib.Wrapper([])\n",
    "def specificNode(f, nodeId:str, num_gpus=0): # modify a function so that it will only run on a specific node only\n",
    "    if num_gpus > 0:\n",
    "        #return f.options(num_gpus=num_gpus, scheduling_strategy=\"SPREAD\")\n",
    "        return f.options(num_gpus=num_gpus, scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(node_id=nodeId, soft=False))\n",
    "    else: return f.options(scheduling_strategy=ray.util.scheduling_strategies.NodeAffinitySchedulingStrategy(node_id=nodeId, soft=False))\n",
    "def exportSe(se):\n",
    "    if not isinstance(se, k1lib.Settings): return se\n",
    "    return {k:exportSe(v) for k,v in se.__dict__.items() if not k.startswith(\"_\") and k != \"kjs\"} # excluding dicts of kjs transpilers cause that causes problems for applyCl() as it's using cloudpickle\n",
    "def movePropsSe(obj, se):\n",
    "    d = se.__dict__; keys = [e for e in d.keys() if not e.startswith(\"_\")]\n",
    "    for key in keys:\n",
    "        if key not in obj or key == \"kjs\": continue\n",
    "        if isinstance(d[key], k1lib.Settings): movePropsSe(obj[key], d[key])\n",
    "        else: d[key] = obj[key]\n",
    "_applyCl_soCache = set() # dynamic library (.so) that has been installed across all nodes, so don't have to reimport\n",
    "class applyCl(BaseCli):\n",
    "    blurb=\"Applies a function to all input elements across a Ray cluster\"\n",
    "    def __init__(self, f, prefetch=None, timeout=60, bs=1, rss:Union[dict, str]={}, pre:bool=False, num_cpus=1, num_gpus=0, memory=None, resolve=True, **kwargs):\n",
    "        \"\"\"Like :class:`apply`, but execute a function over the input iterator\n",
    "in multiple processes on multiple nodes inside of a cluster (hence \"cl\"). So, just a more\n",
    "powerful version of :class:`applyMp`, assuming you have a cluster to run it on.\n",
    "Example::\n",
    "\n",
    "    # returns [3, 2]\n",
    "    [\"abc\", \"de\"] | applyCl(len) | deref()\n",
    "    # returns [5, 6, 9]\n",
    "    range(3) | applyCl(lambda x, bias: x**2+bias, bias=5) | deref()\n",
    "    \n",
    "    # returns [[1, 2, 3], [1, 2, 3]], demonstrating outside vars work\n",
    "    someList = [1, 2, 3]\n",
    "    [\"abc\", \"de\"] | applyCl(lambda s: someList) | deref()\n",
    "    \n",
    "    nIds = applyCl.nodeIds()\n",
    "    # returns [[<nodeId1>, 0], [<nodeId2>, 1], ...], demonstrating preserve mode\n",
    "    [nIds, range(10)] | transpose() | applyCl(lambda x: x**2, pre=True) | deref()\n",
    "\n",
    "    # executes the function, but stores the result on remote nodes, instead of copying result to this node\n",
    "    a = range(5) | applyCl(lambda x: x**2, resolve=False) | deref()\n",
    "    # returns [0, 1, 4, 9, 16]\n",
    "    a | applyCl(lambda x: x) | deref()\n",
    "\n",
    "Summary of all mode of operations::\n",
    "\n",
    "    # Data types:\n",
    "    # - 1:   literal value, just a normal Python object\n",
    "    # - or1: ray.ObjectRef object - Ray's reference to a remote object living somewhere\n",
    "    # - h1:  Handle        object - k1lib's reference to a remote object, obtained if `resolve` is set to False. Use `h.get()` to \n",
    "    # - n1:  node id, string\n",
    "\n",
    "    [1/or1/h1, 2/or2/h2] | applyCl(...)                                              # returns [1, 2, 3]. \"1/or1/h1\" means that the input can be a list of literals, ObjectRef, or Handle\n",
    "    [1/or1/h1, 2/or2/h2] | applyCl(..., resolve=False)                               # returns [h1, h2, h3]\n",
    "    [[n1/h1,   1/or1/h3], [n2/h2, 2/or2/h4]] | applyCl(..., pre=True)                # returns [[n1/h1, 1],  [n2/h2, 2]], executed on n1/h1, h3 is copied over\n",
    "    [[n1/h1,   1/or1/h3], [n2/h2, 2/or2/h4]] | applyCl(..., pre=True, resolve=False) # returns [[n1/h1, h3], [n2/h2, h4]]\n",
    "\n",
    "    [n1, n2] | applyCl.aS(lambda: ...)                # returns [[n1, 1],  [n2, 2]]\n",
    "    None     | applyCl.aS(lambda: ...)                # returns [[n1, 1],  [n2, 2], ...], executes once on all nodes\n",
    "    [n1, n2] | applyCl.aS(lambda: ..., resolve=False) # returns [[n1, h1], [n2, h2]]\n",
    "\n",
    "Internally, this uses the library Ray (https://www.ray.io) to do the heavy\n",
    "lifting. So, :class:`applyCl` can be thought of as a thin wrapper around that\n",
    "library, but still has the same consistent interface as :class:`apply` and\n",
    ":class:`applyMp`. From all of my tests so far, it seems that :class:`applyCl`\n",
    "works quite well and is quite robust, so if you have access to a cluster, use\n",
    "it over :class:`applyMp`.\n",
    "\n",
    "The library will connect to a Ray cluster automatically when you import\n",
    "everything using ``from k1lib.imports import *``. It will execute\n",
    "``import ray; ray.init()``, which is quite simple. If you have ray installed,\n",
    "but does not want this default behavior, you can do this::\n",
    "\n",
    "    import k1lib\n",
    "    k1lib.settings.startup.init_ray = False\n",
    "    from k1lib.imports import *\n",
    "\n",
    "As with :class:`applyMp`, there are pitfalls and weird quirks to multiprocessing,\n",
    "on 1 or multiple nodes, so check out the docs over there to be aware of them,\n",
    "as those translates well to here.\n",
    "\n",
    "There're more extensive documentation on these notebooks: `27-multi-node <https://mlexps.com/other/27-multi-node/>`_,\n",
    "`30-applyCl-benchmarks <https://mlexps.com/other/30-applyCl-benchmarks/>`_, if you want to kinda get the feel of this\n",
    "tool more.\n",
    "\n",
    ".. admonition:: Time sharing the cluster\n",
    "\n",
    "    Let's say that the cluster is located in a company, and that multiple users want to\n",
    "    access it, then you might have to think about it a little more. Say the cluster has\n",
    "    60 cores, and someone has launched a long-running job: 2160 tasks, 10 minutes/task,\n",
    "    1 core/task, totalling 6 hours. If you want to launch another job that has 20 tasks,\n",
    "    requiring 10 cores, 1 second/task, totalling 2 seconds on an idle cluster.\n",
    "    \n",
    "    All modern schedulers (Ray, Slurm, Spark, etc) can't schedule your 20 tasks immediately.\n",
    "    It has to wait for some running tasks to finish to schedule your task. This means you\n",
    "    have to wait on average for 5-10 minutes before all of your tasks finish. This might be\n",
    "    fine if you've used Slurm a lot, but extremely not okay for me and my patience. The whole\n",
    "    point of a cluster is to get results immediately, within a few seconds. So here's a\n",
    "    workaround::\n",
    "    \n",
    "        # long running task, on notebook 1\n",
    "        from k1lib.imports import *\n",
    "        settings.cli.applyCl.cpuLimit = 40\n",
    "        range(2160) | applyCl(lambda x: time.sleep(10*60)) | ignore() # long running task\n",
    "        \n",
    "        # short running task, on notebook 2\n",
    "        from k1lib.imports import *\n",
    "        range(20) | applyCl(lambda x: time.sleep(1)) | ignore() # short running task, should finishes almost immediately\n",
    "\n",
    "    Essentially, there's that setting that you can adjust. Like with Ray's ``num_cpus``,\n",
    "    this is merely a suggestion to my library to not schedule jobs past that cpu limit,\n",
    "    but you can circumvent it in some strange edge cases that I'm too lazy to implement.\n",
    "    Likewise, when you schedule a Ray task, you can specify that it will only take 1 cpu,\n",
    "    but you can end up forking it into 5 different processes, which can cause congestion\n",
    "    and memory thrashing. If Ray doesn't do it right (possibly impossible to do so anyway)\n",
    "    then do I really have to?\n",
    "\n",
    ".. admonition:: Advanced use case\n",
    "\n",
    "    Not really advanced, but just a bit difficult to understand/follow. Let's say\n",
    "    that you want to scan through the home directory of all nodes, grab all files,\n",
    "    read them, and get the number of bytes they have. You can do something like this::\n",
    "    \n",
    "        a = None | applyCl.aS(lambda: None | cmd(\"ls ~\") | filt(os.path.isfile) | deref()) | deref()\n",
    "        b = a | ungroup() | deref()\n",
    "        c = b | applyCl(cat(text=False) | shape(0), pre=True) | deref()\n",
    "        d = c | groupBy(0, True) | apply(item().all() | toSum(), 1) | deref()\n",
    "    \n",
    "    Noted, this is relatively complex. Let's see what A, B, C and D looks like::\n",
    "    \n",
    "        # A\n",
    "        [['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', ['Miniconda3-latest-Linux-x86_64.sh', 'mintupgrade-2023-04-01T232950.log']],\n",
    "         ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', ['5a', 'abc.jpg', 'a.txt']]]\n",
    "        # B\n",
    "        [['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', 'Miniconda3-latest-Linux-x86_64.sh'],\n",
    "         ['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', 'mintupgrade-2023-04-01T232950.log'],\n",
    "         ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', '5a'],\n",
    "         ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 'abc.jpg'],\n",
    "         ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 'a.txt']]\n",
    "        # C\n",
    "        [['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', 74403966],\n",
    "         ['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', 1065252],\n",
    "         ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 2601],\n",
    "         ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 16341],\n",
    "         ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 10177]]\n",
    "        # D\n",
    "        [['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 92185432],\n",
    "         ['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', 75469218]]\n",
    "\n",
    "    The steps we're concerned with is A and C. In step A, we're running 2 processes, 1 for each\n",
    "    node, to get all the file names in the home directory. In step C, we're running 5 processes\n",
    "    total, 2 on the first node and 3 on the second node. For each process, it's going to read as\n",
    "    bytes and count up those bytes. Finally in step D, the results are grouped together and the\n",
    "    sizes summed.\n",
    "\n",
    "    So yeah, it's pretty nice that we did all of that in a relatively short amount of code.\n",
    "    The data is distributed too (reading multiple files from multiple nodes), so we're truly\n",
    "    not bottlenecked by anything.\n",
    "\n",
    ".. admonition:: Context object handle\n",
    "\n",
    "    Let's say you have these unresolved handles::\n",
    "    \n",
    "        # creates a bunch of infinite random number generators, one on each node\n",
    "        its = None | applyCl.aS(lambda: repeatF(lambda: random.randint(2, 15)), resolve=False) | deref()\n",
    "        # gets the next value of all generators, can return [4, 4, 9, 2] for a 4-node cluster\n",
    "        its | cut(1) | applyCl(lambda x: next(x)) | deref()\n",
    "        # gets the next value of all generators, add 0 to the 1st generator, 1 to the 2nd generator, etc, then return the resulting output that might look like [3, 16, 10, 7]\n",
    "        [its | cut(1), range(4)] | transpose() | applyCl(lambda x: next(ctxHandle) + x, pre=True) | cut(1) | deref()\n",
    "\n",
    "    So, the special thing about this is that variable `ctxHandle` on the last line. That is a\n",
    "    special variable that is injected on the way. Why all this complexity?\n",
    "    \n",
    "    The whole idea with unresolved object handles is that you have a distributed complex\n",
    "    data structure that can't be serialized and juggle around easily. That's the `its` handles\n",
    "    in the example. Then, you might want to feed in some (simple, serializable) input X,\n",
    "    change the complex data structure in its own process, then return some (simple, serializable)\n",
    "    output Y. In the example, X is range(4), while Y is the resulting number array.\n",
    "\n",
    ".. admonition:: Cython\n",
    "\n",
    "    Even with running everything distributedly like this, you might run into speed issues.\n",
    "    Then, you'll essentially have 2 options. First is to write (pleasant) Cython code, or\n",
    "    second is to write (unpleasant) C/C++ Python extensions. If you were to choose the C/C++\n",
    "    option, then here's the flow:\n",
    "    \n",
    "    - Develop Python C extension, export everything as a shared library (a single .so file)\n",
    "    - Execute ``applyCl.installSo(\"library.so\")`` to install the library to all nodes\n",
    "    - Use functions provided by your library normally, like ``import yourlibrary; range(10) | applyCl(yourlibrary.afunction) | deref()``\n",
    "    \n",
    "    But applyCl can deal with cython functions directly in your notebook. Here's the flow:\n",
    "    \n",
    "    - Annotate a code cell with the magic \"%%cython\", write Cython code as usual\n",
    "    - Just use that function normally\n",
    "    \n",
    "    Let's see an example::\n",
    "    \n",
    "        # ---------- code cell 1 ----------\n",
    "        from k1lib.imports import *      # cython ipython extension is automatically loaded\n",
    "        # ---------- code cell 2 ----------\n",
    "        %%cython\n",
    "        from k1lib.cli import ls         # demonstrating that you can use all of the existing tools and libraries as usual\n",
    "        cdef g(a:int): return f\"{a} 123\" # demonstrating that you can refactor out to other functions\n",
    "        def f (a:int): return [g(a), ls(\".\")]\n",
    "        # ---------- code cell 3 ----------\n",
    "        range(10) | applyCl(f) | deref()\n",
    "    \n",
    "    You only have to install Cython on the current node and not the other nodes. Also note\n",
    "    that currently, this only supports you passing in Cython-compiled functions directly into\n",
    "    ``applyCl()`` or ``applyCl.aS()``. You can't pass a normal Python function that uses a\n",
    "    Cython function like this::\n",
    "    \n",
    "        # ---------- code cell 1 ----------\n",
    "        from k1lib.imports import *\n",
    "        # ---------- code cell 2 ----------\n",
    "        %%cython\n",
    "        from k1lib.cli import ls          # note: have to reimport here because all the symbols inside this code block is independent from the rest of the notebook\n",
    "        cpdef g(a:int): return f\"{a} 123\"\n",
    "        # ---------- code cell 3 ----------\n",
    "        def f (a:int): return [g(a), ls(\".\")]\n",
    "        range(10) | applyCl(f) | deref() # this throws an import error, as the compiled code won't be installed on the remote nodes\n",
    "    \n",
    "    This behavior can potentially be fixed in the future, but I'm lazy and it's not a hard\n",
    "    thing to follow the rules. The dynamic library will be installed in the working directory.\n",
    "    You can delete them after a coding session to free up some space, but they're likely to be\n",
    "    tiny, so you don't really have to worry about it.\n",
    "    \n",
    "    Also, like everything else in parallel programming, please benchmark absolutely everything\n",
    "    because it might even be slower using Cython if internally you're allocating space for\n",
    "    large data structures constantly, compared to cli tool's lazy execution model. For operations\n",
    "    that work on giant files, I actually find it very difficult to gain any appreciable speedups\n",
    "    using Cython, as cli tools are already pretty optimized, so best task for this is probably\n",
    "    long-running, complex mathematical modelling, and not generic text manipulation.\n",
    "\n",
    ".. warning::\n",
    "\n",
    "    Just like with any other parallel processing model, there are some quirks that\n",
    "    can happen behind the scenes that aren't quite what you expected, as this is\n",
    "    incredibly tricky. Dig into Ray's serialization page (https://docs.ray.io/en/latest/ray-core/objects/serialization.html)\n",
    "    or their whitepapers (https://docs.ray.io/en/latest/ray-contribute/whitepaper.html)\n",
    "    to get a feel for how it all works underneath. The notable quirks that you might need to think about is:\n",
    "    \n",
    "    - A lot of the internal code assumes that you're on a Unix system, preferably Linux,\n",
    "      so it might not work on other platforms like Windows. But honestly, screw Windows.\n",
    "\n",
    ":param prefetch: if not specified, schedules all jobs at the same time. If\n",
    "    specified, schedules jobs so that there'll only be a specified amount of\n",
    "    jobs, and will only schedule more if results are actually being used.\n",
    ":param timeout: seconds to wait for job before raising an error\n",
    ":param bs: if specified, groups ``bs`` number of transforms into 1 job to be more\n",
    "    efficient.\n",
    ":param rss: resources required for the task. Can be {\"custom_resource1\": 2} or \"custom_resource1\" as a shortcut\n",
    ":param pre: \"preserve\", same convention as :meth:`applyCl.aS`. If True, then allow passing\n",
    "    through node ids as the first column to shedule jobs on those specific nodes only\n",
    ":param num_cpus: how many cpu does each task take?\n",
    ":param memory: how much memory to give to the task in bytes?\n",
    ":param resolve: whether to resolve the outputs or not. Set this to False to not move\n",
    "    memory to the requesting node and cache the big data structure on the remote node\n",
    ":param kwargs: extra arguments to be passed to the function. ``args`` not\n",
    "    included as there're a couple of options you can pass for this cli.\"\"\"\n",
    "        super().__init__(fs=[f]); _fC = fastF(f); self.ogF = f; self.pre = pre; self.num_cpus = num_cpus\n",
    "        try: # f might be a string, so can't do f.__module__\n",
    "            isCythonFunc = \"cython\" in f.__module__\n",
    "            if isCythonFunc: applyCl.installSo(sys.modules[f.__module__].__file__)\n",
    "        except: isCythonFunc = False\n",
    "        self.rss = rss = {rss: 1} if isinstance(rss, str) else rss\n",
    "        cwd = os.getcwd(); se = exportSe(k1lib.settings)\n",
    "        def remoteF(s, e, idxCtxHandle=None): # function that will be executed on remote node. Have to setup environment a little bit before executing\n",
    "            # e: the real element. s is just e's storage context, in case e is a Handle. Else s is not used and can be None. Why? Because Actors can't be\n",
    "            # serialized directly with cloudpickle, but it can be passed as function parameters, which Ray will do some special sauce serialization\n",
    "            import k1lib; movePropsSe(se, k1lib.settings) # do this to sync current settings with the remote worker nodes\n",
    "            if k1lib.settings.startup.or_patch.numpy: k1lib.cli.init.patchNumpy()\n",
    "            if k1lib.settings.startup.or_patch.dict: k1lib.cli.init.patchDict()\n",
    "            import os; os.makedirs(cwd, exist_ok = True); os.chdir(cwd)\n",
    "            if isinstance(e, Handle): e.setStorage(s); e = e.get()\n",
    "            if idxCtxHandle: _fC.__globals__[\"ctxHandle\"] = s.d[idxCtxHandle]\n",
    "            return _fC(e, **kwargs)\n",
    "        # self.remoteF = remoteF; f = ray.remote(resources=rss, num_cpus=num_cpus, **({\"memory\": memory} if memory else {}))(remoteF)\n",
    "        rssKw = {\"resources\": rss, \"num_cpus\": num_cpus, \"num_gpus\": num_gpus, **({\"memory\": memory} if memory else {})}#; rssKw = None\n",
    "        self.prefetch = prefetch or int(1e8); self.timeout = timeout; self.bs = bs\n",
    "        self._copyCtx = lambda: [f, [prefetch, timeout, bs, rss, pre, num_cpus, num_gpus, memory, resolve], kwargs]\n",
    "        nodeId = applyCl.nodeId(); rssF = ray.remote(**rssKw)(remoteF) # f that has constraints injected into it\n",
    "        def preprocessF(e): # return Handle (if pre=False), or [nodeId, Handle] (if pre=True). f is remoteF, core element can be a Handle, real object, or ObjectRef\n",
    "            if resolve: # storage location managed by ray, returns or2/h2 or [nId/h1, or2/h2]\n",
    "                if pre:\n",
    "                    a, b = e; s = extractStorage(b)\n",
    "                    if isinstance(a, Handle): h = a.deposit(b); return [a, h.executeAsync(remoteF, rssKw, a.idx)], [a,h]\n",
    "                    else: return [a, specificNode(rssF, a, num_gpus=num_gpus).remote(s, b)], []\n",
    "                else:\n",
    "                    if isinstance(e, Handle): return e.executeAsync(remoteF, rssKw), [e]\n",
    "                    else: return rssF.remote(None, e), []\n",
    "            else: # storage location explicitly managed by me, returns h2 or [nId/h1, h2]\n",
    "                storageWarmup()\n",
    "                if pre:\n",
    "                    a, b = e; s = extractStorage(b)\n",
    "                    if isinstance(a, Handle): # [h1, 2/or2/h2]\n",
    "                        h = a.deposit(b) # first deposits b into a's storage context to get handle. h and a have the same storage context\n",
    "                        a.report(\"report7\"); b.report(\"report7\")\n",
    "                        return [a, h.executeAsync(remoteF, rssKw, a.idx)], [a,b,h] # then executes it in h's storage context\n",
    "                    else: # [nId, 2/or2/h2]\n",
    "                        if isinstance(b, Handle) and b.nodeId == nodeId: return [a, b.executeAsync(remoteF, rssKw)], [b] # [nId, h2], if h2 is on nId, then use h2's storage context\n",
    "                        h = Handle.create(b, a, num_gpus=num_gpus); return [a, h.executeAsync(remoteF, rssKw)], [h] # create storage context on `a`, deposits b on it, then execute\n",
    "                else: # 1/or1/h1\n",
    "                    s = extractStorage(e)\n",
    "                    if isinstance(e, Handle): return e.executeAsync(remoteF, rssKw), [e] # has storage context already, execute on it directly\n",
    "                    else: h = Handle.create(e, num_gpus=num_gpus); return h.executeAsync(remoteF, rssKw), [h] # create storage context, deposit e on it, then execute\n",
    "        @ray.remote\n",
    "        def resolveFRemote(o): return 1\n",
    "        def resolveF(e):\n",
    "            e = e[0] # why do this? Because of a pretty obscure bug related to the reference counting mechanism I have here\n",
    "            # in preprocessF(), it returns [meats, Python references to (potential) Handles]\n",
    "            # I have to do this in order to keep the Handles alive. After resolveF(), everything is settled, so\n",
    "            # old Handles can safely be deleted\n",
    "            if resolve:\n",
    "                if pre: a, b = e; return [a, b.block().get() if isinstance(b, Handle) else ray.get(b)]\n",
    "                else: return e.block().get() if isinstance(e, Handle) else ray.get(e)\n",
    "            else:\n",
    "                return [e[0], e[1].block()] if pre else e.block() # don't resolve to this node, but still block execution until that object is resolvable\n",
    "        self.preprocessF = preprocessF; self.resolveF = resolveF; applyCl.preprocessF = preprocessF; applyCl.resolveF = resolveF # references for lprun so that I can benchmark these 2 functions\n",
    "    @staticmethod\n",
    "    def installSo(fn:str, force:bool=False):\n",
    "        \"\"\"Installs dynamic library (.so file) to all nodes.\n",
    "\n",
    ":param fn: file name of the shared library\n",
    ":param force: force reinstall even if the library is already on the remote node\"\"\"\n",
    "        basename = os.path.basename(fn)\n",
    "        if not force and basename in _applyCl_soCache: return\n",
    "        print(\"Installing dynamic library to all nodes... \", end=\"\"); _applyCl_soCache.add(basename); contents = cli.cat(fn, False)\n",
    "        None | applyCl.aS(lambda: contents | cli.file(basename)) | cli.ignore(); print(\"Done\")\n",
    "    def __ror__(self, it):\n",
    "        timeout = self.timeout; bs = self.bs; ogF = self.ogF; preprocessF = self.preprocessF; resolveF = self.resolveF\n",
    "        if bs > 1: return it | cli.batched(bs, True) | applyCl(lambda x: x | apply(ogF) | cli.aS(list), self.prefetch, timeout) | cli.joinStreams()\n",
    "        def gen(it):\n",
    "            futures = deque(); it = iter(it); n = self.num_cpus; limit = settings.applyCl.cpuLimit or int(1e9)\n",
    "            for i, e in zip(range(min(self.prefetch, (limit-_cpuUsed.value)//n)), it): # try to anticipate how much resources can be consumed ahead of time and only schedule that much, to prevent deadlocks when multiple applyCl() is called, but their parent process has not consumed the yield statement, so the cpu count doesn't get decremented\n",
    "                while _cpuUsed.value + n > limit: time.sleep(0.1) # this is a very rudimentary lock. Doesn't have to be accurate though, and Python's GIL ensure atomic-ness\n",
    "                futures.append(preprocessF(e)); _cpuUsed.value += n\n",
    "            for e in it: yield resolveF(futures.popleft()); futures.append(preprocessF(e)) # free and allocate cpu slot immediately, so no while loop necessary\n",
    "            for e in futures: res = resolveF(e); _cpuUsed.value -= n; yield res\n",
    "        applyCl.rorGen = gen\n",
    "        return gen(it)\n",
    "    def __invert__(self):\n",
    "        \"\"\"Expands the arguments out, just like :class:`apply`.\n",
    "Example::\n",
    "\n",
    "    # returns [20, 20, 18, 14, 8, 0, -10, -22, -36, -52]\n",
    "    [range(10), range(20, 30)] | transpose() | ~applyCl(lambda x, y: y-x**2) | deref()\"\"\"\n",
    "        f, rest, kwargs = self._copyCtx(); return applyCl(lambda x: f(*x), *rest, **kwargs)\n",
    "    @staticmethod\n",
    "    def nodeIds(includeSelf=True) -> List[str]:\n",
    "        \"\"\"Returns a list of all node ids in the current cluster.\n",
    "Example::\n",
    "\n",
    "    applyCl.nodeIds() # returns something like ['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', '1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068']\n",
    "\n",
    "If you want to get nodes' metadata, then just use ray's builtin function ``ray.nodes()``\n",
    "\n",
    ":param includeSelf: whether to include node id of the current process or not\"\"\"\n",
    "        res = ray.nodes() | cli.filt(lambda x: x[\"Alive\"]) | apply(lambda x: x[\"NodeID\"]) | aS(list)\n",
    "        if includeSelf: return res\n",
    "        res.remove(applyCl.nodeId()); return res\n",
    "    @staticmethod\n",
    "    @lru_cache\n",
    "    def nodeId() -> str:\n",
    "        \"\"\"Returns current node id\"\"\"\n",
    "        return ray.runtime_context.get_runtime_context().get_node_id()\n",
    "    @staticmethod\n",
    "    def meta() -> object:\n",
    "        \"\"\"Grabs the metadata object for the current node\"\"\"\n",
    "        return ray.nodes() | cli.filt(lambda x: x[\"NodeID\"] == applyCl.nodeId()) | cli.item()\n",
    "    @staticmethod\n",
    "    def cpu() -> int:\n",
    "        \"\"\"Grabs the number of cpus available on this node\"\"\"\n",
    "        return int(applyCl.meta()[\"Resources\"][\"CPU\"])\n",
    "    @staticmethod\n",
    "    def aS(f, **kwargs):\n",
    "        \"\"\"Executes function f once for all node ids that are piped in.\n",
    "Example::\n",
    "\n",
    "    # returns [['1051da...', ['Desktop', 'Downloads']], ['7bb387...', ['Pictures', 'Music']]]\n",
    "    applyCl.nodeIds() | applyCl.aS(lambda: None | cmd(\"ls ~\") | deref()) | deref()\n",
    "    # same as above, demonstrating passing in a list of nodeIds\n",
    "    [\"1051da...\", \"7bb387...\"] | applyCl.aS(lambda: None | cmd(\"ls ~\") | deref()) | deref()\n",
    "    # same as above, demonstrating passing in \"None\" for all nodeIds in the cluster\n",
    "    None | applyCl.aS(lambda: None | cmd(\"ls ~\") | deref()) | deref()\n",
    "\n",
    "If you want to execute f for all nodes, you can pass in None instead.\n",
    "\n",
    "As a reminder, this kinda follows the same logic as the popular cli :class:`aS`, where\n",
    "f is executed once, hence the name \"apply Single\". Here, the meaning of \"single\" is\n",
    "different. It just means execute once for each node ids. If you want to quickly execute\n",
    "a function on a single node, without all the fuss, there's this short form that you can follow::\n",
    "\n",
    "    # returns ['Desktop', 'Downloads'], demonstrating that you can also pass in a single node id\n",
    "    \"1051da...\" | applyCl.aS(lambda: None | cmd(\"ls ~\") | deref()) | deref()\n",
    "\n",
    ":param f: main function to execute in each node. Not supposed to accept any arguments\n",
    ":param kwargs: keyword arguments for the main :class:`applyCl` function\"\"\"\n",
    "        f = fastF(f); final = apply(lambda x: [x, 0]) | applyCl(lambda _: f(), pre=True, **kwargs)\n",
    "        def inner(it):\n",
    "            shortform = False\n",
    "            if it is None: it = applyCl.nodeIds()\n",
    "            if isinstance(it, str): it = [it]; shortform = True\n",
    "            if it | ~cli.inSet(_nodeIdsCache()) | cli.shape(0) > 0: # caching nodeIds(), because that takes a surprising amount of time\n",
    "                _nodeIdsCache.value = applyCl.nodeIds(); outliers = it | ~cli.inSet(_nodeIdsCache()) | cli.deref()\n",
    "                if len(outliers) > 0: raise Exception(f\"These nodes cannot be found: {outliers}\")\n",
    "            return it | final | ((cli.cut(1) | cli.item()) if shortform else cli.iden())\n",
    "        return aS(inner)\n",
    "    @staticmethod\n",
    "    def cmd(s:str, sudo=False, nodeIds=None, **kwargs):\n",
    "        \"\"\"Convenience function to execute shell command on all nodes.\n",
    "Example::\n",
    "\n",
    "    applyCl.cmd(\"mkdir -p /some/folder\")\n",
    "\n",
    "It returns [[nodeid1, output1], [nodeid2, output2]]. If you need more flexibility,\n",
    "fall back to :meth:`applyCl.aS`\n",
    "\n",
    ":param s: shell command to execute\n",
    ":param sudo: if True, will execute the command with sudo privileges. Will ask for password\n",
    "    and then cache it internally for 5 minutes\n",
    ":param kwargs: keyword arguments to pass to :class:`applyCl`\"\"\"\n",
    "        global _password; import getpass\n",
    "        if sudo:\n",
    "            _start_RemovePwThread()\n",
    "            if _password() is None: print(\"Enter password:\"); _password.value = getpass.getpass(prompt=\"\")\n",
    "            return   nodeIds | applyCl.aS(lambda: _password() | cli.cmd(f\"sudo -S {s}\") | cli.deref(), **kwargs) | cli.deref()\n",
    "        else: return nodeIds | applyCl.aS(lambda: None        | cli.cmd(s)              | cli.deref(), **kwargs) | cli.deref()\n",
    "    @staticmethod\n",
    "    def lookup():\n",
    "        \"\"\"Tries to lookup a particular file to see on which node it's at.\n",
    "Example::\n",
    "\n",
    "    # returns [[nodeId, \"something.txt\"], [nodeId, \"abc.jpg\"]]\n",
    "    [\"something.txt\", \"abc.jpg\"] | applyCl.lookup()\n",
    "    # returns [nodeId, \"something.txt\"]\n",
    "    \"something.txt\" | applyCl.lookup()\n",
    "\n",
    "Files that don't exist won't be included in the result, and files that\n",
    "exist on multiple nodes will be returned multiple times. The output format\n",
    "is such that I can pipe it into applyCl(..., pre=True) and have it execute\n",
    "some function that I want. This is pretty much just a convenience function.\"\"\"\n",
    "        def inner(fns):\n",
    "            fns = fns | cli.deref(); single = isinstance(fns, str)\n",
    "            if single: fns = [fns]\n",
    "            ans = None | applyCl.aS(lambda: fns | cli.iden() & (apply(os.path.expanduser) | apply(os.path.exists)) | cli.transpose() | cli.filt(\"x\", 1) | cli.cut(0) | cli.deref()) | cli.ungroup() | cli.deref()\n",
    "            return ans[0] if single else ans\n",
    "        return cli.aS(inner)\n",
    "    @staticmethod\n",
    "    def replicateFile(fn:str, nodeIds=None):\n",
    "        \"\"\"Replicates a specific file in the current node to all the other nodes.\n",
    "Example::\n",
    "\n",
    "    applyCl.replicateFile(\"~/cron.log\")\n",
    "\n",
    "Internally, this will read chunks of 100kB of the specified file and dump it\n",
    "incrementally to all other nodes, which has implications on performance. To\n",
    "increase or decrease it, check out :class:`~k1lib.cli.inp.cat`. This also means\n",
    "you can replicate arbitrarily large files around as long as you have the disk\n",
    "space for it, while ram size doesn't really matter\n",
    "\n",
    "Please note that this operation is not symmetric. Unlike :meth:`balanceFile` and\n",
    ":meth:`balanceFolder`, in which they can be invoke on any node and it'll roughly\n",
    "do the same thing (rebalances everything out), this operation can do totally\n",
    "different things depending on which node you run it on. Let's say the file exists\n",
    "on nodes A and B, but not on nodes C and D. If you run this function on either\n",
    "node A or B, it will replicate the file to C and D. However, if you run this\n",
    "function on node C or D, it will instead throw an error since the file doesn't\n",
    "exist.\n",
    "\n",
    ":param fn: file name\"\"\"\n",
    "        fn = os.path.expanduser(fn); dirname = os.path.dirname(fn)\n",
    "        # checking if there's an existing file already. If there is, then don't try to copy data to that node\n",
    "        if nodeIds is None: canSize = os.path.getsize(fn); nodeIds = None | applyCl.aS(lambda: os.path.getsize(fn) if os.path.exists(fn) else 0) | cli.filt(cli.op() != canSize, 1) | cli.cut(0) | cli.deref()\n",
    "        nodeIds = nodeIds | cli.wrapList().all() | cli.deref()\n",
    "        nodeIds | cli.insert(None, False).all() | applyCl(lambda _: None | cli.cmd(f\"mkdir -p {dirname}; rm -rf {fn}\") | cli.deref(), pre=True) | cli.deref()\n",
    "        for chunk in cli.cat(fn, text=False, chunks=True): nodeIds | cli.insert(chunk, False).all() | applyCl(lambda chunk: chunk >> cli.file(fn) | cli.deref(), pre=True) | cli.deref()\n",
    "    @staticmethod\n",
    "    def balanceFile(fn:str, nAs:List[str]=None, nBs:List[str]=None, rS=None, chunkSize:int=100_000_000):\n",
    "        \"\"\"Splits a specified file in node nAs and dumps other parts\n",
    "to nodes nBs. Example::\n",
    "\n",
    "    applyCl.balanceFile(\"~/cron.log\")\n",
    "\n",
    "This will split the big files up into multiple segments (1 for each node). Then\n",
    "for each segment, it will read through it chunk by chunk into memory, and then\n",
    "deposits it into the respective nodes. Finally, it truncates the original files\n",
    "down to its segment boundary.\n",
    "\n",
    "The main goal of this is so that you can analyze a single big (say 200GB) file\n",
    "quickly. If that file is on a single node, then it will take forever, even with\n",
    ":class:`applyMp`. So splitting things up on multiple nodes will make analyzing\n",
    "it a lot faster.\n",
    "\n",
    "There's also the function :meth:`balanceFolder`, which has the opposite problem of\n",
    "having lots of small (say 100MB) files. So it will try to move files around (keeping\n",
    "them intact in the meantime) to different nodes so that the folder size ratio is\n",
    "roughly proportional to the cpu count.\n",
    "\n",
    "The exact split rule depends on the number of CPUs of each node. Best to see an\n",
    "example::\n",
    "\n",
    "    Command:         applyCl.balanceFile(\"~/cron.log\")\n",
    "    Verbose command: applyCl.balanceFile(\"~/cron.log\", [\"1\"], [\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "    ----------- Before -----------\n",
    "    Node:      1  2  3  4 5\n",
    "    Cpu:       8  12 16 8 8\n",
    "    Size (GB): 52 0  0  0 0\n",
    "    ----------- After  -----------\n",
    "    Node:      1  2  3  4 5\n",
    "    Cpu:       8  12 16 8 8\n",
    "    Size (GB): 8  12 16 8 8\n",
    "\n",
    "This also works if you have files on existing nodes already, and are upgrading the\n",
    "cluster::\n",
    "\n",
    "    Command:         applyCl.balanceFile(\"~/cron.log\")\n",
    "    Verbose command: applyCl.balanceFile(\"~/cron.log\", [\"1\", \"5\"], [\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "    ----------- Before -----------\n",
    "    Node:      1  2  3  4  5\n",
    "    Cpu:       8  12 16 8  8\n",
    "    Size (GB): 26 0  0  26 0\n",
    "    ----------- After  -----------\n",
    "    Node:      1  2  3  4  5\n",
    "    Cpu:       8  12 16 8  8\n",
    "    Size (GB): 8  12 16 8  8\n",
    "\n",
    "If you want to move files out of a node when decommissioning them, you can do\n",
    "something like this::\n",
    "\n",
    "    Command:         applyCl.decommission(\"~/cron.log\", [\"3\", \"4\"])\n",
    "    Verbose command: applyCl.balanceFile(\"~/cron.log\", [\"1\", \"2\", \"3\", \"4\", \"5\"], [\"1\", \"2\", \"5\"])\n",
    "    ----------- Before -----------\n",
    "    Node:      1  2  3  4 5\n",
    "    Cpu:       8  12 16 8 8\n",
    "    Size (GB): 8  12 16 8 8\n",
    "    ----------- After  -----------\n",
    "    Node:      1  2  3  4 5\n",
    "    Cpu:       8  12 16 8 8\n",
    "    Size (GB): 15 22 0  0 15\n",
    "\n",
    "Remember that the node ids \"1\", etc. is for illustrative purposes only. You should get\n",
    "real node ids from :meth:`nodeIds`.\n",
    "\n",
    "Why is the file size proportional to the number of cores on each node? Well, if\n",
    "you have more cores, you should be able to process more, so as to make everything\n",
    "balanced, right?\n",
    "\n",
    "Again, this means that you can split arbitrarily large files as long as you have\n",
    "the disk space for it, ram size is not a concern. How does this perform? Not\n",
    "the best in the world if you don't have a lot of nodes. With sata 3 ssds, 750MB/s\n",
    "ethernet, I got transfer speeds of roughly 100MB/s. This should increase as you\n",
    "have more nodes based on the code structure, but I haven't tested it yet. Can\n",
    "it be faster? Definitely. Am I willing to spend time optimizing it? No.\n",
    "\n",
    ":param fn: file name\n",
    ":param nAs: node ids that currently stores the file. If not specified, try to detect\n",
    "    what nodes the file exists in\n",
    ":param nBs: node ids that will store the file after balancing everything out. If not\n",
    "    specified, will take all available nodes\n",
    ":param rS: :class:`~k1lib.cli.inp.refineSeek` instance, if you need more fine-grained\n",
    "    control over section boundaries so as to not make everything corrupted\n",
    ":param chunkSize: see :meth:`balanceFolder`\n",
    "\"\"\"\n",
    "        from k1lib.cli._applyCl import balanceFile\n",
    "        with settings.cat.context(chunkSize=chunkSize): balanceFile(fn, nAs, nBs, rS)\n",
    "    @staticmethod\n",
    "    def decommissionFile(fn, nAs:List[str], rS=None, chunkSize:int=100_000_000):\n",
    "        \"\"\"Convenience function for :meth:`balanceFile`. See docs over there.\"\"\"\n",
    "        from k1lib.cli._applyCl import balanceFile\n",
    "        with settings.cat.context(chunkSize=chunkSize): balanceFile(fn, None, applyCl.nodeIds() | ~cli.inSet(nAs) | cli.deref(), rS)\n",
    "    @staticmethod\n",
    "    def cat(fn:str=None, f:Callable=None, nodeIds=None, timeout:float=60, pre:bool=False, multiplier:int=1, includeId:bool=False, resolve:bool=True):\n",
    "        \"\"\"Reads a file distributedly, does some operation on them, collects and\n",
    "returns all of the data together. Example::\n",
    "\n",
    "    fn = \"~/repos/labs/k1lib/k1lib/cli/test/applyCl.cat.data\"\n",
    "    (\"0123456789\"*5 + \"\\\\n\") * 1000 | file(fn)\n",
    "    applyCl.splitFile(fn)\n",
    "    applyCl.cat(fn, shape(0), keepNodeIds=True) | deref()\n",
    "\n",
    "That returns something like this (for a 2-node cluster, with 2 (node A) and 4 (node B) cpus respectively)::\n",
    "\n",
    "    [['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', 167],\n",
    "     ['7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d', 167],\n",
    "     ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 166],\n",
    "     ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 167],\n",
    "     ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 166],\n",
    "     ['1051dafd2b0dac13561c46fe052f561400592f0723df2cd746a41068', 167]]\n",
    "\n",
    "Here, we're creating an initial file with 1000 lines. Then we'll split it up into\n",
    "2 fragments: 334 lines and 667 lines and store them on the respective nodes. Then,\n",
    "on node A, we'll split the file up into 2 parts, each with 167 lines. On node B,\n",
    "we'll split the file up into 4 parts, each with around 166 lines. Then we'll\n",
    "schedule 6 processes total, each dealing with 166 lines. After all of that, results\n",
    "are collected together and returned.\n",
    "\n",
    "If you want to distinguish between different processes inside f, for example you\n",
    "want to write results into different files, you can do something like this::\n",
    "\n",
    "    dir_ = \"~/repos/labs/k1lib/k1lib/cli/test\"\n",
    "    fn = f\"{dir_}/applyCl.cat.data\"\n",
    "    applyCl.cmd(f\"rm -r {dir_}/applyCl\")    # clear out old folders\n",
    "    applyCl.cmd(f\"mkdir -p {dir_}/applyCl\") # creating folders\n",
    "    # do processing on fn distributedly, then dump results into multiple files\n",
    "    applyCl.cat(fn, ~aS(lambda idx, lines: lines | shape(0) | aS(dill.dumps) | file(f\"{dir_}/applyCl/{idx}.pth\")), includeId=True) | deref()\n",
    "    # reading all files and summing them together\n",
    "    None | applyCl.aS(lambda: ls(f\"{dir_}/applyCl\")) | ungroup() | applyCl(cat(text=False) | aS(dill.loads), pre=True) | cut(1) | toSum()\n",
    "\n",
    ".. admonition:: Simple mode\n",
    "\n",
    "    There's also another mode that's activated whenever f is not specified that feels\n",
    "    more like vanilla :class:`~k1lib.cli.inp.cat`. Say you have a file on a specific node::\n",
    "    \n",
    "        nodeId = \"7bb387b2920694abe9f7d2a2ed939b6d31843faf91d174d0221e871d\"\n",
    "        fn = \"~/ssd2/randomFile.txt\"\n",
    "        \n",
    "        # -------------- file is on current node --------------\n",
    "        cat(fn) # returns iterator of lines inside the file\n",
    "        fn | cat() # same thing as above\n",
    "        # -------------- file is on remote node --------------\n",
    "        [nodeId, fn] | applyCl.cat() # returns iterator of lines of the file\n",
    "        applyCl.cat([nodeId, fn]) # same thing\n",
    "        nodeId | applyCl.cat(fn) # also same thing\n",
    "    \n",
    "    So yeah, there're lots of ways to just simply read a file on a remote node. Is\n",
    "    it too much? Probably, but good thing is that you can pick any that's intuitive\n",
    "    for you. Note that this mode is just for convenience only, for when you want to do\n",
    "    exploratory analysis on a single remote file. To be efficient at bulk processing,\n",
    "    use the normal mode instead.\n",
    "\n",
    ":param fn: file name\n",
    ":param f: function to execute in every process\n",
    ":param nodeIds: only read file from these nodes\n",
    ":param timeout: kills the processes if it takes longer than this amount of seconds\n",
    ":param pre: \"preserve\" mode, just like in :class:`applyCl`. Whether to keep the node id column or not\n",
    ":param multiplier: by default, each node will spawn as many process as there\n",
    "    are cpus. Sometimes you want to spawn more process, change this to a higher number\n",
    ":param includeId: includes a unique id for this process (just normal integers from 0 to n)\n",
    ":param resolve: whether to resolve the remote objects or not\n",
    "\"\"\"\n",
    "        fn = os.path.expanduser(fn) if fn is not None else None\n",
    "        if f is None: # simple case\n",
    "            def inner(nodeId_fn:Tuple[str, str]):\n",
    "                nodeId, fn = nodeId_fn; seeks = [nodeId] | applyCl.aS(lambda: fn | cli.splitSeek(round(os.path.getsize(fn)/settings.cat.chunkSize+1))) | cli.cut(1) | cli.item() | cli.deref()\n",
    "                inter = seeks | cli.window(2) | apply(cli.wrapList() | cli.insert(nodeId)) | cli.deref()\n",
    "                return inter | ~applyCl(lambda sB, eB: cli.cat(fn,sB=sB,eB=eB) | cli.deref(), pre=True) | cli.cut(1) | cli.joinStreams()\n",
    "                # return [nodeId_fn] | applyCl(cat() | deref(), pre=True) | cut(1) | item() # direct, no chunking method\n",
    "            if fn is None: return aS(inner) # [nodeId, fn] | applyCl.cat()\n",
    "            if isinstance(fn, str): return aS(lambda nodeId: inner([nodeId, fn])) # nodeId | applyCl.cat()\n",
    "            else: return inner(fn) # applyCl.cat([nodeId, fn])\n",
    "        nodeIds = nodeIds or (applyCl.nodeIds() | applyCl.aS(lambda: os.path.exists(fn)) | cli.filt(cli.op(), 1) | cli.cut(0) | cli.deref())\n",
    "        checkpoints = nodeIds | applyCl.aS(lambda: fn | cli.splitSeek(int(applyCl.meta()[\"Resources\"][\"CPU\"]*multiplier)) | cli.window(2) | cli.deref()) | cli.ungroup() | cli.insertIdColumn(True, False) | ~apply(lambda x,y,z: [x,[*y,z]]) | cli.deref()\n",
    "        return checkpoints | applyCl(~aS(lambda x,y,idx: cli.cat(fn, sB=x, eB=y) | ((cli.wrapList() | cli.insert(idx)) if includeId else cli.iden()) | f), pre=True, timeout=timeout, num_cpus=1, resolve=resolve) | (cli.iden() if pre else cli.cut(1))\n",
    "    @staticmethod\n",
    "    def replicateFolder(folder:str, nodeIds=None):\n",
    "        \"\"\"Replicates a specific folder in the current node to all the other nodes.\n",
    "Example::\n",
    "\n",
    "    applyCl.replicateFolder(\"~/ssd2/data/owl\")\n",
    "\n",
    "This just list out all files recursively in the specified folder, then replicate each file using :meth:`replicateFile`\"\"\"\n",
    "        applyCl.getFilesInFolder(folder) | applyCl(lambda fn: applyCl.replicateFile(fn, nodeIds), num_cpus=0.1) | cli.deref()\n",
    "    @staticmethod\n",
    "    def balanceFolder(folder:str, maxSteps:int=None, audit:bool=False, bs:int=5, chunkSize:int=100_000_000):\n",
    "        \"\"\"Balances all files within a folder across all nodes.\n",
    "Example::\n",
    "\n",
    "    # make the chunk size huge so that transfers become faster\n",
    "    settings.cli.cat.chunkSize = 100_000_000\n",
    "    base = \"~/repos/labs/k1lib/k1lib/cli/test/applyCl.balance\"\n",
    "    # deletes old structures and making test folder    \n",
    "    applyCl.cmd(f\"rm -r {base}\"); applyCl.cmd(f\"mkdir -p {base}\")\n",
    "    # creates 20 files of different sizes and dump it in the base folder of the current node\n",
    "    torch.linspace(1e4, 1e5, 20).int() | apply(lambda x: \"x\"*x) | insertIdColumn() | ~apply(lambda idx, contents: contents | file(f\"{base}/{idx}.txt\")) | deref();\n",
    "    # transfers files between nodes such that the total folder size is proportional to the number of cpus across nodes\n",
    "    applyCl.balanceFolder(base)\n",
    "    # get folder size of all nodes\n",
    "    None | applyCl.aS(lambda: ls(base) | apply(os.path.getsize) | toSum()) | deref()\n",
    "\n",
    "    # creates 20 additional files and dump it to the current node\n",
    "    torch.linspace(1e4, 1e5, 20).int() | apply(lambda x: \"x\"*x) | insertIdColumn() | ~apply(lambda idx, contents: contents | file(f\"{base}/{idx+20}.txt\")) | deref();\n",
    "    # balances the tree out again\n",
    "    applyCl.balance(base)\n",
    "    # get folder size of all nodes\n",
    "    None | applyCl.aS(lambda: ls(base) | apply(os.path.getsize) | toSum()) | deref()\n",
    "\n",
    "So imagine that you just downloaded 1000 files to a single node on a specific folder,\n",
    "but you need to analyze all of them in a distributed manner. What you can do is to\n",
    "move some files to other nodes and then do your analysis. If you want to download\n",
    "more files, just dump it to any node (or download distributed across all nodes),\n",
    "then rebalance the folders and do your analysis.\n",
    "\n",
    "Also, internally, it splits files into multiple chunks, transfer the chunks to other\n",
    "nodes and append to the correct files. It uses :meth:`~k1lib.cli.inp.cat` to split up\n",
    "the file, which has settings under ``settings.cli.cat``. By default, the chunk size is\n",
    "100k bytes, which I think is the sweet spot because :meth:`~k1lib.cli.inp.cat` also\n",
    "supports remote file accessed from the internet and sometimes the library is used for\n",
    "systems with very few memory. But for this use case where you already have the insane\n",
    "hardware for this, 100kB is extremely small and will slow transfer rates to a crawl,\n",
    "so in this function, it will be temporarily be set to the parameter ``ChunkSize``, which\n",
    "is 100MB by default.\n",
    "\n",
    ":param folder: folder to rebalance all of the files\n",
    ":param maxSteps: what's the maximum number of file transfers? By default has no limit, so that files are transferred until \n",
    ":param audit: if True, don't actually move files around and just return what files are going to be moved where\n",
    ":param bs: batch size for transporting this many files at once. Increase to make it faster, but with the\n",
    "    penalty of the progress bar not updating as frequently\n",
    ":param chunkSize: file chunk size to split up and send to other nodes\n",
    "\"\"\"\n",
    "        from k1lib.cli._applyCl import balanceFolder\n",
    "        with settings.cat.context(chunkSize=chunkSize): return balanceFolder(folder, audit, maxSteps, bs=bs)\n",
    "    def decommissionFolder(folder:str, nAs:List[str], maxSteps:int=10000, audit:bool=False, timeout:float=3600, bs:int=5, chunkSize:int=100_000_000):\n",
    "        \"\"\"Like :meth:`decommissionFile`, but works for distributed folders instead.\n",
    "\n",
    ":param nAs: list of node ids to migrate files away from\n",
    ":param maxSteps: limits the total number of optimization steps. Normally don't have to specify,\n",
    "    but just here in case it runs for too long trying to optimize the folder structure\n",
    ":param audit: if True, just returns the file movements it's planning to do\n",
    ":param bs: batch size for transporting this many files at once. Increase to make it faster, but with the\n",
    "    penalty of the progress bar not updating as frequently\n",
    ":param chunkSize: see :meth:`balanceFolder`\n",
    "\"\"\"\n",
    "        from k1lib.cli._applyCl import decommissionFolder\n",
    "        with settings.cat.context(chunkSize=chunkSize): return decommissionFolder(folder, nAs, audit=audit, maxSteps=maxSteps, timeout=timeout, bs=bs)\n",
    "    @staticmethod\n",
    "    def pruneFolder(folder):\n",
    "        \"\"\"Removes empty directories recursively from a root folder.\"\"\"\n",
    "        def inner(folder):\n",
    "            folder = os.path.expanduser(folder)\n",
    "            dirs, files = folder | ls() | filt(os.path.isdir).split() | deref()\n",
    "            if len(files) > 0: return\n",
    "            dirs | apply(pruneFolder) | ignore()\n",
    "            if folder | ls() | shape(0) == 0: None | cmd(f\"rm -rf {folder}\") | ignore()\n",
    "        None | applyCl.aS(lambda: inner(folder)) | deref()\n",
    "    @staticmethod\n",
    "    def diskScan(folder:str, raw=False, accurate=True, f=None):\n",
    "        \"\"\"Scans for files and folders in the specified folder for potential\n",
    "distributed files and folders. A distributed file is a file that exists on more\n",
    "than 1 node. A distributed folder is a folder that that exists on more than 1\n",
    "node and does not have any shared children. Example::\n",
    "\n",
    "    applyCl.diskScan(\"~/ssd2\")\n",
    "    applyCl.diskScan(\"~/ssd2\", True)\n",
    "\n",
    "The first line does not return anything, but will print out something like this:\n",
    "\n",
    ".. include:: ../literals/diskScan.rst\n",
    "\n",
    "While the second line will return a parseable data structure instead::\n",
    "\n",
    "    [[['/home/kelvin/ssd2/data/genome/RegulationFeatureActivity', [4113489746, 7912834090, 4164314316]],\n",
    "      ['/home/kelvin/ssd2/data/genome/go/release_geneontology_org', [2071645117, 4172737915, 2107005131]],\n",
    "      ['/home/kelvin/ssd2/data/genome/RegulationFeatureActivity.backup', [568878496, 552888466, 600610083]],\n",
    "      ['/home/kelvin/ssd2/data/genome/00-common_all.idx', [341738564, 671136833, 0]],\n",
    "      ['/home/kelvin/ssd2/data/genome/genbank/ch1.dat.gz', [25356744, 0, 25356764]],\n",
    "      ['/home/kelvin/ssd2/test', [136152, 273530, 136351]],\n",
    "      ['/home/kelvin/ssd2/data/genome/genbank/ch1', [0, 0, 0]]],\n",
    "     [['/home/kelvin/ssd2/data/genome/dummy.txt', [1101, 1101, 1101]]],\n",
    "     [['/home/kelvin/ssd2/data/genome/00-All.vcf', [32737509360, 65475018903, 32737509588]],\n",
    "      ['/home/kelvin/ssd2/data/genome/MotifFeatures/homo_sapiens.GRCh38.motif_features.gff', [13963854962, 27927709895, 13963854962]],\n",
    "      ['/home/kelvin/ssd2/data/genome/00-common_all.vcf', [2353901811, 4707803470, 2353901831]]]]\n",
    "\n",
    "Remember that since an operating system usually have lots of shared files\n",
    "(like \"~/.bashrc\", for example), these might be mistaken as a distributed file.\n",
    "Make sure to only scan folders that you store data in, or else it'll take a long time to return.\n",
    "\n",
    ":param folder: the folder to scan through\n",
    ":param raw: whether to return raw data or display it out nicely\n",
    ":param accurate: if True, returns size when you read all files into RAM. If False\n",
    "    returns size occupied by the entire file/folder (will be larger because files\n",
    "    are arranged into different blocks in the underlying disk)\n",
    ":param f: optional post process function applied after getting the raw results, if ``raw=False``\"\"\"\n",
    "        from k1lib.cli._applyCl import diskScan4, diskScan5\n",
    "        if raw: return diskScan4(folder, accurate=accurate)\n",
    "        else: return diskScan5(folder, accurate=accurate, f=(f or cli.iden()))\n",
    "    @staticmethod\n",
    "    def balancedNodeIds():\n",
    "        \"\"\"Returns a stream of node ids that's balanced based on cpu count/performance.\n",
    "Example::\n",
    "\n",
    "    # returns list of 10 node ids: [\"abc...\", \"def...\", \"abc...\", ...]\n",
    "    applyCl.balancedNodeIds() | head() | deref()\n",
    "\"\"\"\n",
    "        from k1lib.cli._applyCl import balancedNodeIds\n",
    "        return balancedNodeIds()\n",
    "    @staticmethod\n",
    "    def balancedCpus():\n",
    "        \"\"\"Returns Dict[nodeId (str) -> #cpu (int))]. Could be useful to know\n",
    "how much to split up files and folders according to your custom rules. Example::\n",
    "\n",
    "    # returns {\"abc...\": 8, \"def...\": 19, \"ghi...\": 88} for 7700 (4c8t), 10700k (8c16t) and 13900k (24c32t)\n",
    "    applyCl.balancedCpus()\n",
    "\"\"\"\n",
    "        from k1lib.cli._applyCl import loadTestGuard\n",
    "        return loadTestGuard()\n",
    "    @staticmethod\n",
    "    def loadTest():\n",
    "        \"\"\"Performs a load test on the cluster.\n",
    "Example::\n",
    "\n",
    "    applyCl.loadTest()\n",
    "\n",
    "What is a load test? It basically tries to perform some intensive and\n",
    "long-running calculations on all processes on all nodes in the cluster\n",
    "to know how good are each individual nodes. This is useful information\n",
    "because whenever you try to split a file up to form a distributed file,\n",
    "or move files in a folder around to form a distributed folder, the amount\n",
    "of data each node gets is going to be proportional to this performance\n",
    "information. More powerful nodes will have more data to process, so that\n",
    "the total running time across all nodes is going to roughly be the same.\n",
    "\n",
    "But isn't cpu count good enough for this? No, not actually. The i7 7700\n",
    "has 4 cores, 8 threads, and the i9 13900k has 8 performance cores and 16\n",
    "efficiency cores, totalling to 32 threads. You would suspect that the\n",
    "13900k to be 4x (32/8=4) or 6x (24/4=6) more powerful than the 7700, but\n",
    "it's actually 10x more powerful.\n",
    "\n",
    "The test itself takes around 1-2 minutes, and the test results are going\n",
    "to be saved locally in the folder \"~/.k1lib/\", so that it can use that\n",
    "info directly in future runs.\"\"\"\n",
    "        from k1lib.cli._applyCl import loadTest\n",
    "        return loadTest()\n",
    "    @staticmethod\n",
    "    def getFolderSize(folder:str=None) -> int:\n",
    "        \"\"\"Shortcut function to get size of a folder on the current node.\"\"\"\n",
    "        from k1lib.cli._applyCl import getFolderSize\n",
    "        if folder is None: return getFolderSize\n",
    "        return folder | getFolderSize\n",
    "    @staticmethod\n",
    "    def getFilesInFolder(folder:str=None):\n",
    "        from k1lib.cli._applyCl import getFilesInFolder\n",
    "        if folder is None: return getFilesInFolder\n",
    "        return folder | getFilesInFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa5f6bd-f27d-4479-8fc0-6dda1a99d123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "if hasRay:\n",
    "    @ray.remote(num_cpus=0)\n",
    "    class Storage: # a wrapper for specific objects, kinda like ObjectRef, but it's an ObjectRef in my control. Should not be serialized to every other place\n",
    "        def __init__(self, v=None):\n",
    "            self.lockExecute = threading.Lock(); self.lockIncref = threading.Lock(); self.lockDecref = threading.Lock()\n",
    "            self.nodeId = applyCl.nodeId(); self.d = {}; self.refs = defaultdict(lambda: 0)\n",
    "            self.autoInc = k1lib.AutoIncrement(prefix=\"_d\")\n",
    "            self.idx = f\"{self.nodeId}_\" + f\"{random.random()}\"[:10] + f\"_{time.time()}\"\n",
    "            self.idx2 = f\"{random.randint(100, 900)}\"\n",
    "            self.deposit(v)\n",
    "        def getIdx(self): return self.idx\n",
    "        def getMeta(self): return [self.nodeId, self.idx, self.idx2]\n",
    "        def lookup(self, idx:str): return self.d[idx]\n",
    "        def remove(self, idx:str): del self.d[idx]\n",
    "        def keys(self): return list(self.d.keys())\n",
    "        def incref(self, idx:str):\n",
    "            # with self.lockIncref:\n",
    "            if idx is not None:\n",
    "                # requests.get(f\"http://192.168.1.133:8892/increfd/{self.idx2}/{idx}\")\n",
    "                if self.refs[idx] <= 0: raise Exception(f\"incref-ing object {self.idx2}/{idx} that has already been deleted\")\n",
    "                self.refs[idx] += 1#; return self\n",
    "        def decref(self, idx:str):\n",
    "            # with self.lockDecref:\n",
    "            if idx is not None:\n",
    "                self.refs[idx] -= 1; v = self.d[idx]\n",
    "                if self.refs[idx] == 0: self.remove(idx)\n",
    "                if self.refs[idx] < 0: raise Exception(f\"decref-ing object {self.idx2}/{idx} that no longer exists\")\n",
    "                # requests.get(f\"http://192.168.1.133:8892/decrefd/{self.idx2}/{idx}\")#; return self\n",
    "        def deposit(self, v:\"1/or1/h1\", s:\"Storage\"=None) -> \"idx:str\": # injecting s into v if v is a Handle\n",
    "            if isinstance(v, Handle): v.setStorage(s); v = v.get()\n",
    "            if isinstance(v, ray.ObjectRef): v = ray.get(v)\n",
    "            idx = self.autoInc()\n",
    "            # requests.get(f\"http://192.168.1.133:8892/deposit/{self.idx2}/{idx}/{v}\"[:100])\n",
    "            self.d[idx] = v; self.refs[idx] += 1; return idx\n",
    "        def execute(self, f, idx:str, idxCtxHandle:str=None) -> \"idx:str\":\n",
    "            # requests.get(f\"http://192.168.1.133:8892/execute/{self.idx2}/{idx}\")\n",
    "            # if idxCtxHandle: f.__globals__[\"ctxHandle\"] = self.d[idxCtxHandle] # injecting in global variable\n",
    "            with self.lockExecute: return self.deposit(f(self, self.d[idx], idxCtxHandle)) # executing \"pure\" f with some argument taken from the storage\n",
    "        def __getstate__(self): raise Exception(\"Can't be serialized!\")\n",
    "else:\n",
    "    class Storage: pass\n",
    "_storages = {} # nodeId -> {idx: int, ss: [Storage]}, idx for current index to yield the storage\n",
    "_cpuCounts = {} # nodeId -> int\n",
    "_nodeIdsGen = None\n",
    "def getStorage(nodeId:str=None, num_gpus=0) -> Storage:\n",
    "    \"\"\"Handles creating storage contexts. This is created mainly because it's\n",
    "costly to actually instantiate a new actor (1-2 second/actor!), so this will\n",
    "spawn new storage contexts till the cpu count of each node is reached. From\n",
    "then on, it will keep reusing old storage contexts\"\"\"\n",
    "    global _nodeIdsGen; _nodeIdsGen = _nodeIdsGen or applyCl.balancedNodeIds()\n",
    "    nodeId = nodeId or next(_nodeIdsGen)\n",
    "    if num_gpus == 0:\n",
    "        if nodeId not in _storages or nodeId not in _cpuCounts:\n",
    "            _storages[nodeId] = {\"idx\": 0, \"ss\": []}\n",
    "            _cpuCounts[nodeId] = nodeId | applyCl.aS(lambda: os.cpu_count())\n",
    "        if len(_storages[nodeId][\"ss\"]) < _cpuCounts[nodeId]: # add new storage context\n",
    "            _storages[nodeId][\"ss\"].append(specificNode(Storage, nodeId).remote())\n",
    "        idx = _storages[nodeId][\"idx\"]; res = _storages[nodeId][\"ss\"][idx]\n",
    "        _storages[nodeId][\"idx\"] = (idx+1)%_cpuCounts[nodeId]; return res\n",
    "    else: return specificNode(Storage, nodeId, num_gpus).remote()\n",
    "def extractStorage(x): return x.storage if isinstance(x, Handle) else None\n",
    "class Handle: # specific object in storage, pretty much ObjectRef that I'm in control of. Can be serialized to every other place\n",
    "    def __init__(self, storage, idx:str=None, _or=None):\n",
    "        self.storage = storage; self.idx = idx; self._or = _or\n",
    "        self.nodeId, self.storageId, self.idx2 = ray.get(storage.getMeta.remote()); self.weakref = False # whether this Handle should decrement reference count of storage element or not\n",
    "    @staticmethod\n",
    "    def create(v, nodeId:str=None, num_gpus=0) -> \"Handle\": # creates new storage and put value into it\n",
    "        s = getStorage(nodeId, num_gpus); return Handle(s, ray.get(s.deposit.remote(v, extractStorage(v))))\n",
    "    def deposit(self, v) -> \"Handle\": # put new value into this handle's Storage\n",
    "        return Handle(self.storage, ray.get(self.storage.deposit.remote(v, extractStorage(v))))\n",
    "    def execute(self, f, s:\"Storage\"=None, kwargs=None) -> \"Handle\": # f is a normal function. Blocks until finished executing\n",
    "        if kwargs:\n",
    "            kwargs.pop(\"num_gpus\", None)\n",
    "            @ray.remote(**kwargs)\n",
    "            def inner(sto, s): return sto.execute.remote(f, self.idx)\n",
    "            return Handle(self.storage, ray.get(ray.get(inner.remote(self.storage, s))))\n",
    "        else: return Handle(self.storage, ray.get(self.storage.execute.remote(f, self.idx)))\n",
    "    def block(self) -> \"Handle\": # block execution and finalize Handle's state\n",
    "        if self._or: self.idx = ray.get(self._or)\n",
    "        return self\n",
    "    def executeAsync(self, f, kwargs=None, idxCtxHandle:str=None) -> \"Handle\": # f is a normal function. Returns immediately, finalize it by calling .block()\n",
    "        # requests.get(f\"https://logs.mlexps.com/async12/{self.idx}/N/{self.idx2}\")\n",
    "        if kwargs: # idxCtxHandle is the (optional) string of the background handle object (left arg in pre=True mode), to be dynamically injected into the \"ctxHandle\" global variable\n",
    "            # i1 = self.idx; i2 = self.idx2\n",
    "            kwargs.pop(\"num_gpus\", None)\n",
    "            @ray.remote(**kwargs)\n",
    "            def inner(sto):\n",
    "                # requests.get(f\"https://logs.mlexps.com/async56/{i1}/N/{i2}\")\n",
    "                return ray.get(sto.execute.remote(f, self.idx, idxCtxHandle))\n",
    "            # requests.get(f\"https://logs.mlexps.com/async34/{self.idx}/N/{self.idx2}\")\n",
    "            return Handle(self.storage, _or=inner.remote(self.storage))\n",
    "        else: return Handle(self.storage, _or=self.storage.execute.remote(f, self.idx, idxCtxHandle))\n",
    "    def get(self): return ray.get(self.storage.lookup.remote(self.idx))\n",
    "    def setStorage(self, s): # inject storage dependency into this handle, increment storage's refcount\n",
    "        if s: self.storage = s; self.storage.incref.remote(self.idx); self.weakref = False\n",
    "    def __repr__(self): return f\"<Handle idx={self.idx} storage={self.storageId}>\"\n",
    "    def __getstate__(self): d = dict(self.__dict__); d[\"storage\"] = None; return d\n",
    "    def __setstate__(self, d): self.__dict__.update(d); self.weakref = True # reconstructed Handles don't decrement reference count of variable, because Actors can't be serialized\n",
    "    # def report(self, s): requests.get(f\"http://192.168.1.133:8892/{s}/{self.idx2}/{self.idx}\")\n",
    "    def __del__(self):\n",
    "        # requests.get(f\"http://192.168.1.133:8892/handdel/{self.idx2}/{self.idx}\")\n",
    "        # print(f\"storage: {self.storage}, nodeId: {self.nodeId}\")\n",
    "        if not self.weakref: self.storage.decref.remote(self.idx)\n",
    "@lru_cache\n",
    "def storageWarmup(): print(\"Warming up distributed storage...\"); None | applyCl.aS(lambda: os.cpu_count()) | ~apply(lambda x,y: [x]*y) | cli.joinSt() | apply(getStorage) | cli.ignore(); print(\"Finished warming up\")\n",
    "def storageSize(): return _storages.values() | cli.op()[\"ss\"].all() | cli.joinSt() | apply(lambda s: ray.get(s.keys.remote())) | cli.joinSt() | cli.shape(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60cc37-d65c-4133-a025-e643c6f8d134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#longtest\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340991fa-88ae-4d5e-afe8-6bd314f9de12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#longtest\n",
    "assert range(100) | applyCl(lambda x: x + 3, bs=10) | cli.toSum() == 5250\n",
    "assert [range(10), range(20, 30)] | cli.transpose() | ~applyCl(lambda x, y: y-x**2) | cli.deref() == [20, 20, 18, 14, 8, 0, -10, -22, -36, -52]\n",
    "assert [range(10), range(10, 20)] | cli.transpose() | ~applyCl(lambda x, y: x+y) | cli.deref() | cli.toSum() == 190\n",
    "None | applyCl.aS(lambda: None | cli.cmd(\"ls ~\") | cli.filt(os.path.isfile) | cli.deref())\\\n",
    "| cli.ungroup() | applyCl(cli.cat(text=False) | cli.shape(0), pre=True)\\\n",
    "| cli.groupBy(0, True) | apply(cli.item().all() | cli.toSum(), 1) | cli.deref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff1cc5-f06f-436a-ad24-3ea8c5b9d155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#longtest\n",
    "assert [\"abc\", \"de\"] | applyCl(len) | cli.deref() == [3, 2]\n",
    "assert range(3) | applyCl(lambda x, bias: x**2+bias, bias=5) | cli.deref() == [5, 6, 9]\n",
    "someList = [1, 2, 3]\n",
    "assert [\"abc\", \"de\"] | applyCl(lambda s: someList) | cli.deref() == [[1, 2, 3], [1, 2, 3]]\n",
    "fn = \"/home/kelvin/repos/labs/k1lib/k1lib/cli/test/applyCl.data\"\n",
    "range(10) | cli.repeatFrom(5) | cli.join(\"\") | cli.repeat(2) | cli.join(\"\\n\") | cli.file(fn)\n",
    "applyCl.balanceFile(fn)\n",
    "assert None | applyCl.aS(lambda: cli.cat(fn, False) | cli.shape(0)) | cli.cut(1) | cli.toProd() & cli.toSum() | cli.deref() | ~aS(lambda x,y: x/y) > 1000\n",
    "fn = \"~/repos/labs/k1lib/k1lib/cli/test/applyCl.cat.data\"\n",
    "(\"0123456789\"*5 + \"\\n\") * 1000 | cli.file(fn)\n",
    "applyCl.balanceFile(fn); assert applyCl.cat(fn, cli.shape(0)) | cli.shape(0) > 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f1b09-32b2-4dcf-ac08-f148890f6023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#longtest\n",
    "dir_ = \"~/repos/labs/k1lib/k1lib/cli/test\"\n",
    "fn = f\"{dir_}/applyCl.cat.data\"\n",
    "# clear out old folders\n",
    "None | applyCl.aS(lambda: None | cli.cmd(f\"rm -r {dir_}/applyCl\") | cli.deref()) | cli.deref()\n",
    "# creating folders\n",
    "None | applyCl.aS(lambda: None | cli.cmd(f\"mkdir -p {dir_}/applyCl\") | cli.deref()) | cli.deref()\n",
    "# do processing on fn distributedly, then dump results into multiple files\n",
    "applyCl.cat(fn, ~aS(lambda idx, lines: lines | cli.shape(0) | cli.aS(dill.dumps) | cli.file(f\"{dir_}/applyCl/{idx}.pth\")), includeId=True) | cli.deref()\n",
    "assert None | applyCl.aS(lambda: cli.ls(f\"{dir_}/applyCl\")) | cli.ungroup() | applyCl(cli.cat(text=False) | aS(dill.loads), pre=True) | cli.cut(1) | cli.toSum() > 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655f232-17a3-4a9d-8505-4a6d666756a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#longtest\n",
    "base = \"~/repos/labs/k1lib/k1lib/cli/test/applyCl.balance\"; #base = \"~/ssd2/test\"\n",
    "applyCl.cmd(f\"rm -r {base}\"); applyCl.cmd(f\"mkdir -p {base}\")\n",
    "torch.linspace(1e4, 1e5, 20).int() | apply(lambda x: \"x\"*x) | cli.insertIdColumn() | ~apply(lambda idx, contents: contents | cli.file(f\"{base}/{idx}.txt\")) | cli.deref();\n",
    "applyCl.balanceFolder(base); assert 20e15 < (None | applyCl.aS(lambda: cli.ls(base) | apply(os.path.getsize) | cli.toSum()) | cli.cut(1) | cli.toProd()) < 60e15\n",
    "torch.linspace(1e4, 1e5, 20).int() | apply(lambda x: \"x\"*x) | cli.insertIdColumn() | ~apply(lambda idx, contents: contents | cli.file(f\"{base}/{idx+20}.txt\")) | cli.deref();\n",
    "applyCl.balanceFolder(base); assert 280e15 < (None | applyCl.aS(lambda: cli.ls(base) | apply(os.path.getsize) | cli.toSum()) | cli.cut(1) | cli.toProd()) < 400e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d9697db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "thEmptySentinel = object()\n",
    "class applyTh(BaseCli):\n",
    "    blurb=\"Applies a function to all input elements across multiple threads\"\n",
    "    def __init__(self, f, prefetch:int=None, timeout:float=5, bs:int=1, sync=True, **kwargs):\n",
    "        \"\"\"Kinda like the same as :class:`applyMp`, but executes ``f`` on multiple\n",
    "threads, instead of on multiple processes. Advantages:\n",
    "\n",
    "- Relatively low overhead for thread creation\n",
    "- Fast, if ``f`` is io-bound\n",
    "- Does not have to serialize and deserialize the result, meaning iterators can be\n",
    "  exchanged\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- Still has thread creation overhead, so it's still recommended to specify ``bs``\n",
    "- Is slow if ``f`` has to obtain the GIL to be able to do anything\n",
    "\n",
    "All examples from :class:`applyMp` should work perfectly here.\n",
    "\n",
    ":param prefetch: how many results to execute ahead of time\n",
    ":param timeout: kills the thread if it takes longer than this amount\n",
    ":param bs: how much to bunch function calls together\n",
    ":param sync: if True, execute the functions, hang and wait for the result of each\n",
    "    operation, then return. Else schedules the thread but does not wait for the\n",
    "    result and yields None right away\n",
    ":param kwargs: keyword arguments to be fed to the function\"\"\"\n",
    "        fs = [f]; super().__init__(fs=fs); self.f = fs[0]; self.kwargs = kwargs\n",
    "        self.prefetch = prefetch or int(1e9); self.timeout = timeout; self.bs = bs; self.sync = sync\n",
    "    def __ror__(self, it):\n",
    "        if self.bs > 1:\n",
    "            yield from (it | cli.batched(self.bs, True) | applyTh(apply(self.f) | aS(list), self.prefetch, self.timeout, sync=self.sync) | cli.joinStreams()); return\n",
    "        datas = deque(); it = iter(it); kwargs = self.kwargs; sync = self.sync\n",
    "        innerF = fastF(self.f); timeout = self.timeout\n",
    "        def f(line, wrapper): wrapper.value = innerF(line, **kwargs)\n",
    "        for _, line in zip(range(self.prefetch), it):\n",
    "            w = k1lib.Wrapper(thEmptySentinel)\n",
    "            t = threading.Thread(target=f, args=(line,w))\n",
    "            t.start(); datas.append((t, w))\n",
    "        for line in it:\n",
    "            data = datas.popleft() # pop before checking sync to clean up memory\n",
    "            if sync:\n",
    "                data[0].join(timeout)\n",
    "                if data[1].value is thEmptySentinel:\n",
    "                    for data in datas: data[0].join(0.01)\n",
    "                    raise RuntimeError(\"Thread timed out!\")\n",
    "                yield data[1].value\n",
    "            else: yield None\n",
    "            w = k1lib.Wrapper(thEmptySentinel)\n",
    "            t = threading.Thread(target=f, args=(line,w))\n",
    "            t.start(); datas.append((t, w))\n",
    "        for i in range(len(datas)): # do it this way so that python can remove threads early, due to ref counting\n",
    "            data = datas.popleft()\n",
    "            if sync:\n",
    "                data[0].join(timeout)\n",
    "                if data[1].value is thEmptySentinel:\n",
    "                    for data in datas: data[0].join(0.01)\n",
    "                    raise RuntimeError(\"Thread timed out!\")\n",
    "                yield data[1].value\n",
    "            else: yield None\n",
    "    def _copy(self): return applyTh(self.f, self.prefetch, self.timeout, self.bs, **self.kwargs)\n",
    "    def __invert__(self):\n",
    "        res = self._copy(); f = fastF(res.f)\n",
    "        kw = res.kwargs\n",
    "        res.f = lambda x: f(*x, **kw)\n",
    "        res.kwargs = {}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd890f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with k1lib.timer() as t1: range(10000) | applyTh(lambda x: x**2) | cli.toList()\n",
    "with k1lib.timer() as t2: range(10000) | applyTh(lambda x: x**2, bs=32) | cli.toList()\n",
    "assert t1() > t2()*10\n",
    "assert [range(10), range(10, 20)] | cli.transpose() | ~applyTh(lambda x, y, z: x+y-z, z=4) | cli.deref() == [6, 8, 10, 12, 14, 16, 18, 20, 22, 24]\n",
    "def f(x): time.sleep(0.01); return x**2\n",
    "with k1lib.timer() as t1: range(100) | applyTh(f) | cli.toList()\n",
    "with k1lib.timer() as t2: range(100) | applyTh(f, sync=False) | cli.toList()\n",
    "assert t1() > t2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d485b77-eb0e-4b1b-aabc-7db185e082fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class applySerial(BaseCli):\n",
    "    blurb=\"Applies a function to an element repeatedly\"\n",
    "    def __init__(self, f, *args, **kwargs):\n",
    "        \"\"\"Applies a function repeatedly. First yields input iterator ``x``. Then\n",
    "yields ``f(x)``, then ``f(f(x))``, then ``f(f(f(x)))`` and so on. Example::\n",
    "\n",
    "    # returns [2, 4, 8, 16, 32]\n",
    "    2 | applySerial(op()*2) | head(5) | deref()\n",
    "\n",
    "If the result of your operation is an iterator, you might want to\n",
    ":class:`~k1lib.cli.utils.deref` it, like this::\n",
    "\n",
    "    rs = iter(range(8)) | applySerial(rows()[::2])\n",
    "    # returns [0, 2, 4, 6]\n",
    "    rs | rows(1) | item() | deref()\n",
    "    # returns []. This is because all the elements are taken by the previous deref()\n",
    "    rs | item() | deref()\n",
    "    # returns [[2, 8], [10, -6], [4, 16], [20, -12]]\n",
    "    [2, 8] | ~applySerial(lambda a, b: (a + b, a - b)) | head(4) | deref()\n",
    "\n",
    "    rs = iter(range(8)) | applySerial(rows()[::2] | deref())\n",
    "    # returns [0, 2, 4, 6]\n",
    "    rs | rows(1) | item()\n",
    "    # returns [0, 4]\n",
    "    rs | item() # or `next(rs)`\n",
    "    # returns [0]\n",
    "    rs | item() # or `next(rs)`\n",
    "\n",
    ":param f: function to apply repeatedly\"\"\"\n",
    "        fs = [f]; super().__init__(fs=fs); self.f = fs[0]\n",
    "        self.unpack = False; self.args = args; self.kwargs = kwargs\n",
    "    def __ror__(self, it):\n",
    "        f = fastF(self.f)\n",
    "        if self.unpack:\n",
    "            it = init.dfGuard(it)\n",
    "            while True: yield it; it = f(*it, *self.args, **self.kwargs)\n",
    "        else:\n",
    "            while True: yield it; it = f(it, *self.args, **self.kwargs)\n",
    "    def __invert__(self):\n",
    "        ans = applySerial(self.f, *self.args, **self.kwargs)\n",
    "        ans.unpack = True; return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f239ddd0-f039-4ff2-bfb3-4514109c7f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert 2 | applySerial(cli.op()*2) | cli.head(5) | cli.deref() == [2, 4, 8, 16, 32]\n",
    "rs = iter(range(8)) | applySerial(cli.rows()[::2])\n",
    "assert rs | cli.rows(1) | cli.item() | cli.deref() == [0, 2, 4, 6]\n",
    "assert rs | cli.item() | cli.deref() == []\n",
    "rs = iter(range(8)) | applySerial(cli.rows()[::2] | cli.deref())\n",
    "assert rs | cli.rows(1) | cli.item() == [0, 2, 4, 6]\n",
    "assert next(rs) == [0, 4]; assert next(rs) == [0]\n",
    "assert [2, 8] | ~applySerial(lambda a, b: (a + b, a - b)) | cli.head(4) | cli.deref() == [[2, 8], (10, -6), (4, 16), (20, -12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "238ff6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def argsort(it, key=None, reverse=False):\n",
    "    if isinstance(it, settings.arrayTypes): return np.argsort(it) # this mode ignores key and reverse!\n",
    "    if key: return sorted(range(len(it)), key=lambda i: key(it[i]), reverse=reverse)\n",
    "    else: return sorted(range(len(it)), key=it.__getitem__, reverse=reverse)\n",
    "class sort(BaseCli):\n",
    "    blurb=\"Sorts list/table based on an optional column\"\n",
    "    def __init__(self, column:int=0, numeric=True, reverse=False, unsort=False):\n",
    "        \"\"\"Sorts list/table based on a specific `column`.\n",
    "Example::\n",
    "\n",
    "    # returns [[5, 'a'], [1, 'b']]\n",
    "    [[1, \"b\"], [5, \"a\"]] | ~sort(0) | deref()\n",
    "    # returns [[2, 3]]\n",
    "    [[1, \"b\"], [5, \"a\"], [2, 3]] | ~sort(1) | deref()\n",
    "    # errors out, as you can't really compare str with int\n",
    "    [[1, \"b\"], [2, 3], [5, \"a\"]] | sort(1, False) | deref()\n",
    "    # returns [-1, 2, 3, 5, 8]\n",
    "    [2, 5, 3, -1, 8] | sort(None) | deref()\n",
    "\n",
    ".. admonition:: unsort\n",
    "\n",
    "    This is how it works::\n",
    "\n",
    "        a = np.array([1, 5, 9, 2, 6, 3, 7, 4, 8])\n",
    "        a | sort(None, unsort=True)                                    # returns np.array([1, 5, 9, 2, 6, 3, 7, 4, 8])\n",
    "        a | sort(None)                                                 # returns np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]), normal sort\n",
    "        a | (sort(None, unsort=True) | aS(lambda x: x - x[-1]/2))      # returns np.array([-3.5,  0.5,  4.5, -2.5,  1.5, -1.5,  2.5, -0.5,  3.5]), sorts, do transformation, then unsort\n",
    "        a | (sort(None, unsort=True) | aS(lambda x: (x - x[-1]/2)**2)) # returns np.array([12.25,  0.25, 20.25,  6.25,  2.25,  2.25,  6.25,  0.25, 12.25])\n",
    "\n",
    "    How this works is that it will sort everything as usual, then it'll execute the captured\n",
    "    transformation and then it will unsort everything. This is for scenarios when an operation\n",
    "    needs to operate on sorted data, but you still want to keep the original ordering for some\n",
    "    reason.\n",
    "\n",
    ":param column: if None, sort rows based on themselves and not an element\n",
    ":param numeric: whether to convert column to float\n",
    ":param reverse: False for smaller to bigger, True for bigger to smaller. Use\n",
    "    :meth:`__invert__` to quickly reverse the order instead of using this param\n",
    ":param unsort: whether to sort and then unsort the input or not\"\"\"\n",
    "        super().__init__(capture=True)\n",
    "        self.column = column; self.reverse = reverse; self.numeric = numeric; self.unsort = unsort\n",
    "        self.filterF = (lambda x: float(x)) if numeric else (lambda x: str(x))\n",
    "    def _all_array_opt(self, it, level):\n",
    "        if self.unsort: return NotImplemented # too complex to think about right now\n",
    "        c = self.column; reverse = self.reverse; p = [slice(None, None, None)]*level; p1 = (*p, slice(None, None, None)); ser = self.capturedSerial\n",
    "        if c is None and len(it.shape)-level != 1: raise Exception(f\"Expected sort(None) to take in 1-d array, but the array has shape {it.shape[level:]}\")\n",
    "        if c is not None and len(it.shape)-level != 2: raise Exception(f\"Expected sort(None) to take in 2-d array, but the array has shape {it.shape[level:]}\")\n",
    "        bm = np if isinstance(it, np.ndarray) else (torch if (hasTorch and isinstance(it, torch.Tensor)) else None)\n",
    "        if bm is not None:\n",
    "            if c is None: b = bm.argsort(it);   b = bm.flip(b, (level,)) if reverse else b; return bm.gather(it, level, b) | ser.all(level)\n",
    "            else:\n",
    "                try: b = bm.argsort(it[(*p1, c)]); b = bm.flip(b, (level,)) if reverse else b; return bm.gather(it, level, b[(*p1, None)].expand(it.shape)) | ser.all(level)\n",
    "                except ValueError: # this is to catch a super strange numpy bug. This bug happens when executing\n",
    "                    # the above line like: sort(1)._all_array_opt((np.random.randn(183, 32, 2)), 1). This throws \"ValueError: Need at least 0 and at most 32 array objects.\".\n",
    "                    # That error is equivalent to np.choose([0]*32, [np.arange(0,32)]*32). But if you change all 32 into 31, then everything works fine.\n",
    "                    # Pytorch version of gather() doesn't have this limitation, so if this errors out, let's create a pytorch tensor, run through this optimization\n",
    "                    # again, and convert back to the correct dtype np array\n",
    "                    if isinstance(it, np.ndarray) and hasTorch: return self._all_array_opt(torch.from_numpy(it), level).numpy().astype(it.dtype)\n",
    "                    return NotImplemented\n",
    "        return NotImplemented\n",
    "    def __ror__(self, it:Iterator[str]):\n",
    "        c = self.column; reverse = self.reverse; unsort = self.unsort; bm = None; ser = self.capturedSerial\n",
    "        if hasPandas:\n",
    "            if isinstance(it, pd.DataFrame):\n",
    "                if c is None: it = init.dfGuard(it)\n",
    "                else:\n",
    "                    s = it[list(it)[c]]; arg = s.argsort(); arg = (arg[::-1] if reverse else arg).to_numpy(); res = it.iloc[arg] | ser\n",
    "                    if unsort: return res.iloc[argsort(arg)] if isinstance(res, pd.DataFrame) else res | cli.rows(*argsort(arg))\n",
    "                    else: return res\n",
    "            elif isinstance(it, pd.core.arraylike.OpsMixin): it = it.to_numpy()\n",
    "        if isinstance(it, settings.arrayTypes):\n",
    "            if c is None and len(it.shape) != 1: raise Exception(f\"Expected sort(None) to take in a 1-d array, but the array has shape {it.shape}\")\n",
    "            if c is not None and len(it.shape) != 2: raise Exception(f\"Expected sort(col) to take in a 2-d array, but the array has shape {it.shape}\")\n",
    "            bm = np if isinstance(it, np.ndarray) else (torch if (hasTorch and isinstance(it, torch.Tensor)) else None)\n",
    "            if bm:\n",
    "                arg = bm.argsort(it) if c is None else bm.argsort(it[:,c]); arg = bm.flip(arg, (0,)) if reverse else arg\n",
    "                return it[arg] | ser | (cli.rows(*argsort(arg)) if self.unsort else cli.iden())\n",
    "        f = self.filterF\n",
    "        rows = list(it) if c is None else list((it | cli.isNumeric(c) if self.numeric else it) | cli.apply(list))\n",
    "        def sortF(row):\n",
    "            if len(row) > c: return f(row[c])\n",
    "            return float(\"inf\")\n",
    "        if self.unsort:\n",
    "            arg = argsort(rows, f if c is None else sortF, self.reverse)\n",
    "            return rows | cli.rows(*arg) | ser | cli.rows(*argsort(arg))\n",
    "        return sorted(rows, key=f if c is None else sortF, reverse=self.reverse) | ser\n",
    "    def __invert__(self):\n",
    "        \"\"\"Creates a clone that has the opposite sort order\"\"\"\n",
    "        return sort(self.column, self.numeric, not self.reverse, self.unsort)\n",
    "    def _jsF(self, meta):\n",
    "        fIdx = init._jsFAuto(); dataIdx = init._jsDAuto(); argIdx = init._jsDAuto()\n",
    "        if self.unsort: raise Exception(\"sort._jsF() unsort mode is unavailable, as the cli capture mechanism isn't possible in JS\")\n",
    "        return f\"{fIdx} = ({dataIdx}) => {dataIdx}.ksort({cli.kjs.v(self.column)}, {cli.kjs.v(self.numeric)}, {cli.kjs.v(self.reverse)})\", fIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1627b32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert [[1, \"b\"], [5, \"a\"]] | ~sort(0) | cli.deref() == [[5, 'a'], [1, 'b']]\n",
    "assert [[1, \"b\"], [5, \"a\"], [2, 3]] | ~sort(1) | cli.deref() == [[2, 3]]\n",
    "try: [[1, \"b\"], [2, 3], [5, \"a\"]] | sort(1, False) | cli.deref()\n",
    "except TypeError: pass\n",
    "assert [2, 5, 3, -1, 8] | sort(None) | cli.deref() == [-1, 2, 3, 5, 8]\n",
    "a = np.array(range(2*3*4)).reshape(2, 3, 4); ta = torch.tensor(a)\n",
    "assert (a  | (~sort()).all() == np.array    ([[[ 8,  9, 10, 11], [ 4,  5,  6,  7], [ 0,  1,  2,  3]], [[20, 21, 22, 23], [16, 17, 18, 19], [12, 13, 14, 15]]])).all()\n",
    "assert (ta | (~sort()).all() == torch.tensor([[[ 8,  9, 10, 11], [ 4,  5,  6,  7], [ 0,  1,  2,  3]], [[20, 21, 22, 23], [16, 17, 18, 19], [12, 13, 14, 15]]])).all()\n",
    "assert (a  | (~sort(None)).all(2) == np.array    ([[[ 3,  2,  1,  0], [ 7,  6,  5,  4], [11, 10,  9,  8]], [[15, 14, 13, 12], [19, 18, 17, 16], [23, 22, 21, 20]]])).all()\n",
    "assert (ta | (~sort(None)).all(2) == torch.tensor([[[ 3,  2,  1,  0], [ 7,  6,  5,  4], [11, 10,  9,  8]], [[15, 14, 13, 12], [19, 18, 17, 16], [23, 22, 21, 20]]])).all()\n",
    "assert (a[0]  | ~sort() == np.array    ([[ 8,  9, 10, 11], [ 4,  5,  6,  7], [ 0,  1,  2,  3]])).all()\n",
    "assert (ta[0] | ~sort() == torch.tensor([[ 8,  9, 10, 11], [ 4,  5,  6,  7], [ 0,  1,  2,  3]])).all()\n",
    "assert (a[0][0]  | ~sort(None) == np.array    ([3, 2, 1, 0])).all()\n",
    "assert (ta[0][0] | ~sort(None) == torch.tensor([3, 2, 1, 0])).all()\n",
    "assert (a[0]  | (~sort(unsort=True) | cli.op()**2) == np.array([[[  0,   1,   4,   9], [ 16,  25,  36,  49], [ 64,  81, 100, 121]]])).all()\n",
    "a = [1, 9, 4, 7, 0, 3, 8, 6, 5, 2]\n",
    "assert a | sort(None, unsort=True) | cli.deref() == a\n",
    "a = np.array([1, 5, 9, 2, 6, 3, 7, 4, 8]); assert (a | sort(None, unsort=True) | cli.deref() == a).all()\n",
    "assert (a | sort(None) == np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])).all()\n",
    "assert (a | (sort(None, unsort=True) | aS(lambda x: x - x[-1]/2)) == np.array([-3.5,  0.5,  4.5, -2.5,  1.5, -1.5,  2.5, -0.5,  3.5])).all()\n",
    "assert (a | (sort(None, unsort=True) | aS(lambda x: (x - x[-1]/2)**2)) == np.array([12.25,  0.25, 20.25,  6.25,  2.25,  2.25,  6.25,  0.25, 12.25])).all()\n",
    "assert isinstance(df1 | ~sort(0), pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "167a0ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class sortF(BaseCli):\n",
    "    \"Sorts list/table using a function\"\n",
    "    def __init__(self, f:Callable[[Any], float], column:int=None, reverse=False):\n",
    "        \"\"\"Sorts list/table using a function.\n",
    "Example::\n",
    "\n",
    "    # returns ['a', 'aa', 'aaa', 'aaaa', 'aaaaa']\n",
    "    [\"a\", \"aaa\", \"aaaaa\", \"aa\", \"aaaa\"] | sortF(lambda r: len(r)) | deref()\n",
    "    # returns ['aaaaa', 'aaaa', 'aaa', 'aa', 'a']\n",
    "    [\"a\", \"aaa\", \"aaaaa\", \"aa\", \"aaaa\"] | ~sortF(lambda r: len(r)) | deref()\"\"\"\n",
    "        fs = [f]; super().__init__(fs=fs); self.f = fs[0]; self._fC = fastF(self.f)\n",
    "        self.column = column; self.reverse = reverse\n",
    "    def __ror__(self, it:Iterator[Any]) -> Iterator[Any]:\n",
    "        c = self.column; f = self._fC\n",
    "        if c is None:\n",
    "            it = init.dfGuard(it)\n",
    "            try: it[:]; len(it)\n",
    "            except: it = list(it)\n",
    "            return sorted(it, key=f, reverse=self.reverse)\n",
    "        if hasPandas and isinstance(it, pd.DataFrame):\n",
    "            col = it[list(it)[c]]\n",
    "            arg = [x[0] for x in sorted([[i, f(x)] for i,x in enumerate(col)], key=lambda x:x[1], reverse=self.reverse)]\n",
    "            return it.iloc[arg]\n",
    "        def sortF(row):\n",
    "            if len(row) > c: return f(row[c])\n",
    "            return float(\"inf\")\n",
    "        return sorted(list(it), key=sortF, reverse=self.reverse)\n",
    "    def __invert__(self) -> \"sortF\":\n",
    "        return sortF(self.f, self.column, not self.reverse)\n",
    "    def _jsF(self, meta):\n",
    "        fIdx = init._jsFAuto(); dataIdx = init._jsDAuto(); argIdx = init._jsDAuto()\n",
    "        header, _fIdx, _async = k1lib.kast.asyncGuard(k1lib.kast.prepareFunc3(self.f, (\"sortF\", meta)))\n",
    "        return f\"{header}\\n{fIdx} = {'async ' if _async else ''}({dataIdx}) => {{ return {'await ' if _async else ''}{dataIdx}.sortF{'_async' if _async else ''}(({argIdx}) => {_fIdx}({argIdx}), {cli.kjs.v(self.column)}, {cli.kjs.v(self.reverse)}); }}\", fIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87e43803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert [\"a\", \"aaa\", \"aaaaa\", \"aa\", \"aaaa\"] | sortF(lambda r: len(r)) | cli.deref() == ['a', 'aa', 'aaa', 'aaaa', 'aaaaa']\n",
    "assert [\"a\", \"aaa\", \"aaaaa\", \"aa\", \"aaaa\"] | ~sortF(lambda r: len(r)) | cli.deref() == ['aaaaa', 'aaaa', 'aaa', 'aa', 'a']\n",
    "assert isinstance(df1 | sortF(\"2-x\", 0), pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4032f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class consume(BaseCli):\n",
    "    blurb=\"Consumes the iterator in a side stream and returns the iterator\"\n",
    "    def __init__(self, f:Union[BaseCli, Callable[[Any], None]]):\n",
    "        r\"\"\"Consumes the iterator in a side stream and returns the iterator.\n",
    "Kinda like the bash command ``tee``. Example::\n",
    "\n",
    "    # prints \"0\\n1\\n2\" and returns [0, 1, 2]\n",
    "    range(3) | consume(headOut()) | toList()\n",
    "    # prints \"range(0, 3)\" and returns [0, 1, 2]\n",
    "    range(3) | consume(lambda it: print(it)) | toList()\n",
    "\n",
    "This is useful whenever you want to mutate something, but don't want to\n",
    "include the function result into the main stream.\n",
    "\n",
    "See also: :class:`~k1lib.cli.output.tee`\"\"\"\n",
    "        fs = [f]; super().__init__(fs=fs); self.f = fs[0]\n",
    "    def __ror__(self, it):\n",
    "        self.f(it); return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09844c5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with k1lib.captureStdout() as out:\n",
    "    assert range(3) | consume(lambda it: print(it)) | cli.toList() == [0, 1, 2]\n",
    "assert out()[0] == \"range(0, 3)\"\n",
    "with k1lib.captureStdout() as out:\n",
    "    assert range(3) | consume(cli.headOut()) | cli.toList() == [0, 1, 2]\n",
    "assert \"\\n\".join(out()) == \"0\\n1\\n2\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4af7c8e-3f65-46f0-aeac-1cdd637f6a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "def batched_randperm(n, l, gen=None): # courtesy of https://discuss.pytorch.org/t/batch-version-of-torch-randperm/111121/3\n",
    "    if gen is None: return np.argsort(np.random.rand(l, n))\n",
    "    else: return torch.argsort(torch.rand(l, n, generator=gen), dim=-1)\n",
    "def randperm(n, gen=None): return batched_randperm(n, 1, gen)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b082f34a-f689-468c-981e-31a4310d4068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert batched_randperm(5, 10) | cli.shape() == (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4d7d52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "default = 100\n",
    "class randomize(BaseCli):\n",
    "    blurb=\"Randomizes the input elements' order\"\n",
    "    def __init__(self, bs=default, seed=None):\n",
    "        \"\"\"Randomize input stream. In order to be efficient, this does not\n",
    "convert the input iterator to a giant list and yield random values from that.\n",
    "Instead, this fetches ``bs`` items at a time, randomizes them, returns and\n",
    "fetch another ``bs`` items. If you want to do the giant list, then just pass\n",
    "in ``float(\"inf\")``, or ``None``. Example::\n",
    "\n",
    "    # returns [0, 1, 2, 3, 4], effectively no randomize at all\n",
    "    range(5) | randomize(1) | deref()\n",
    "    # returns something like this: [1, 0, 2, 3, 5, 4, 6, 8, 7, 9]. You can clearly see the batches\n",
    "    range(10) | randomize(3) | deref()\n",
    "    # returns something like this: [7, 0, 5, 2, 4, 9, 6, 3, 1, 8]\n",
    "    range(10) | randomize(float(\"inf\")) | deref()\n",
    "    # same as above\n",
    "    range(10) | randomize(None) | deref()\n",
    "    # returns True, as the seed is the same\n",
    "    range(10) | randomize(seed=4) | deref() == range(10) | randomize(seed=4) | deref()\n",
    "\n",
    "Note that if ``seed=True``, then it will randomize all input\n",
    "iterators the same way and independently of each other. Meaning::\n",
    "\n",
    "    r = randomize(seed=42)\n",
    "    range(10) | r | deref() #      returns [6, 9, 1, 2, 0, 8, 3, 5, 4, 7]\n",
    "    range(10) | r | deref() # also returns [6, 9, 1, 2, 0, 8, 3, 5, 4, 7]\n",
    "\n",
    "This may or may not be desireable, but I think it's desirable.\n",
    "\n",
    ":param bs: batch size\n",
    ":param seed: if specified, will always randomize the input iterator in the same way\"\"\"\n",
    "        self.bs = bs if bs != None else float(\"inf\")\n",
    "        self.seed = seed\n",
    "        if seed is not None and not hasTorch: raise Exception(\"Seeded randomize() depends on PyTorch. Please install it first\")\n",
    "    def _newTorchGen(self): return torch.Generator().manual_seed(random.Random(self.seed).getrandbits(63))\n",
    "    def _newGenn(self):\n",
    "        if self.seed is None: return randperm\n",
    "        gen = self._newTorchGen(); return lambda n: randperm(n, gen)\n",
    "    def _newGenn2(self):\n",
    "        if self.seed is None: return batched_randperm\n",
    "        gen = self._newTorchGen(); return lambda n, l: batched_randperm(n, l, gen)\n",
    "    def _all_array_opt(self, it, level):\n",
    "        perms = self._newGenn2()(it.shape[level], np.prod(it.shape[:level]))\n",
    "        b = it | cli.joinSt(level-1); return b[np.arange(len(b))[:, None], perms].reshape(it.shape)\n",
    "    def __ror__(self, it:Iterator[Any]) -> Iterator[Any]:\n",
    "        bs = self.bs\n",
    "        if isinstance(it, settings.arrayTypes):\n",
    "            if bs is default or bs is None or len(it) <= bs: return it if len(it) == 1 else it[self._newGenn()(len(it))]\n",
    "        if hasPandas and isinstance(it, pd.DataFrame):\n",
    "            if bs is default or bs is None or len(it) <= bs: return it.iloc[self._newGenn()(len(it))]\n",
    "        if bs is default: bs = 100\n",
    "        def gen():\n",
    "            genn = self._newGenn()\n",
    "            for batch in it | cli.batched(bs, True):\n",
    "                batch = list(batch); perms = genn(len(batch))\n",
    "                for idx in perms: yield batch[idx]\n",
    "        return gen()\n",
    "    def _jsF(self, meta):\n",
    "        fIdx = init._jsFAuto(); dataIdx = init._jsDAuto()\n",
    "        return f\"{fIdx} = ({dataIdx}) => {dataIdx}.randomize({cli.kjs.v(self.seed)})\", fIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57699ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert range(10) | randomize(1) | cli.deref() == list(range(10))\n",
    "a = range(10) | randomize(3) | cli.deref()\n",
    "assert a[-1] == 9; assert len(a) == 10\n",
    "assert len(range(10) | randomize(float(\"inf\")) | cli.deref()) == 10\n",
    "assert len(range(10) | randomize(None) | cli.deref()) == 10\n",
    "assert range(10) | randomize(seed=4) | cli.deref() == range(10) | randomize(seed=4) | cli.deref()\n",
    "assert np.random.randn(9, 3) | randomize() | cli.aS(type) == np.ndarray\n",
    "assert np.random.randn(1, 3) | randomize() | cli.shape() == (1, 3)\n",
    "r = randomize(seed=42)\n",
    "assert range(10) | r | cli.deref() == [6, 0, 8, 5, 7, 9, 1, 4, 2, 3]\n",
    "assert range(10) | r | cli.deref() == [6, 0, 8, 5, 7, 9, 1, 4, 2, 3]\n",
    "assert range(10) | r | cli.deref() == [6, 0, 8, 5, 7, 9, 1, 4, 2, 3]\n",
    "# _all_array_opt\n",
    "it = np.arange(60).reshape(3, 20)\n",
    "assert (it | randomize().all() | cli.toSum().all() | cli.deref() == np.array([190, 590, 990])).all()\n",
    "assert not (it | randomize(seed=50) | cli.toSum().all() | cli.deref() == np.array([190, 590, 990])).all()\n",
    "it = np.arange(60).reshape(1, 3, 20)\n",
    "assert (it | randomize().all(2) | cli.toSum().all(2) == it | randomize().all(2) | cli.toSum().all(2)).all()\n",
    "assert not (it | randomize().all(2) | cli.toSum().all(2) == it | randomize().all() | cli.toSum().all(2)).all()\n",
    "assert isinstance(df1 | randomize(), pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e937a059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class StaggeredStream:\n",
    "    def __init__(self, stream:Iterator[Any], every:int):\n",
    "        \"\"\"Not intended to be instantiated by the end user. Use :class:`stagger`\n",
    "instead.\"\"\"\n",
    "        self.stream = stream; self.every = every\n",
    "    def __iter__(self):\n",
    "        for i, v in zip(range(self.every), self.stream): yield v\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of window (length of result if you were to deref it).\"\"\"\n",
    "        return self.every\n",
    "class stagger(BaseCli):\n",
    "    blurb='Staggers input stream into multiple stream \"windows\" placed serially'\n",
    "    def __init__(self, every:int):\n",
    "        \"\"\"Staggers input stream into multiple stream \"windows\" placed serially. Best\n",
    "explained with an example::\n",
    "\n",
    "    o = range(10) | stagger(3)\n",
    "    o | deref() # returns [0, 1, 2], 1st \"window\"\n",
    "    o | deref() # returns [3, 4, 5], 2nd \"window\"\n",
    "    o | deref() # returns [6, 7, 8]\n",
    "    o | deref() # returns [9]\n",
    "    o | deref() # returns []\n",
    "\n",
    "This might be useful when you're constructing a data loader::\n",
    "\n",
    "    dataset = [range(20), range(30, 50)] | transpose()\n",
    "    dl = dataset | batched(3) | (transpose() | toTensor()).all() | stagger(4)\n",
    "    for epoch in range(3):\n",
    "        for xb, yb in dl: # looping over a window\n",
    "            print(epoch)\n",
    "            # then something like: model(xb)\n",
    "\n",
    "The above code will print 6 lines. 4 of them is \"0\" (because we stagger every 4\n",
    "batches), and xb's shape' will be (3,) (because we batched every 3 samples).\n",
    "\n",
    "You should also keep in mind that this doesn't really change the property of the\n",
    "stream itself. Essentially, treat these pairs of statement as being the same thing::\n",
    "\n",
    "    o = range(11, 100)\n",
    "    \n",
    "    # both returns 11\n",
    "    o | stagger(20) | item()\n",
    "    o | item()\n",
    "\n",
    "    # both returns [11, 12, ..., 20]\n",
    "    o | head(10) | deref()\n",
    "    o | stagger(20) | head(10) | deref()\n",
    "\n",
    "Lastly, multiple iterators might be getting values from the same stream window,\n",
    "meaning::\n",
    "\n",
    "    o = range(11, 100) | stagger(10)\n",
    "    it1 = iter(o); it2 = iter(o)\n",
    "    next(it1) # returns 11\n",
    "    next(it2) # returns 12\n",
    "\n",
    "This may or may not be desirable. Also this should be obvious, but I want to\n",
    "mention this in case it's not clear to you.\"\"\"\n",
    "        self.every = int(every)\n",
    "    def __ror__(self, it:Iterator[Any]) -> StaggeredStream:\n",
    "        return StaggeredStream(iter(init.dfGuard(it)), self.every)\n",
    "    @staticmethod\n",
    "    def tv(every:int, ratio:float=0.8):\n",
    "        \"\"\"Convenience method to quickly stagger train and valid datasets.\n",
    "Example::\n",
    "\n",
    "    # returns [[16], [4]]\n",
    "    [range(100)]*2 | stagger.tv(20) | shape().all() | deref()\"\"\"\n",
    "        return stagger(round(every*ratio)) + stagger(round(every*(1-ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e7102c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 12)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert [range(100)]*2 | stagger.tv(20) | cli.shape().all() | cli.deref() == [(16,), (4,)]\n",
    "o = range(11,100) | stagger(10)\n",
    "it1 = iter(o); it2 = iter(o)\n",
    "next(it1), next(it2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9143b796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = range(10) | stagger(3); assert a | cli.deref() == [0, 1, 2]\n",
    "assert a | cli.deref() == [3, 4, 5]; assert a | cli.deref() == [6, 7, 8]\n",
    "assert a | cli.deref() == [9]; assert a | cli.deref() == []\n",
    "with k1lib.captureStdout() as outer:\n",
    "    dataset = [range(20), range(30, 50)] | cli.transpose()\n",
    "    dl = dataset | cli.batched(3) | (cli.transpose() | cli.toTensor()).all() | stagger(4)\n",
    "    for epoch in range(3):\n",
    "        for xb, yb in dl: print(epoch)\n",
    "assert outer() == [\"0\"] * 4 + [\"1\"] * 2 + [\"\"]\n",
    "o = range(11, 100)\n",
    "assert o | stagger(20) | cli.item() == o | cli.item()\n",
    "assert o | cli.head(10) | cli.deref() == o | stagger(20) | cli.head(10) | cli.deref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae428316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "compareOps = {\"__lt__\", \"__le__\", \"__eq__\", \"__ne__\", \"__gt__\", \"__ge__\"}\n",
    "class op(k1lib.Absorber, BaseCli):\n",
    "    blurb=\"Shorthand for lambda functions\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Absorbs operations done on it and applies it on the stream. Based\n",
    "on :class:`~k1lib.Absorber`. Example::\n",
    "\n",
    "    # returns 16\n",
    "    4 | op()**2\n",
    "    # returns 16, equivalent to the above\n",
    "    4 | aS(lambda x: x**2)\n",
    "    # returns [0, 1, 4, 9, 16]\n",
    "    range(5) | apply(op()**2) | deref()\n",
    "    # returns [0, 1, 4, 9, 16], equivalent to the above\n",
    "    range(5) | apply(lambda x: x**2) | deref()\n",
    "\n",
    "Main advantage is that you don't have to waste keystrokes when you just want\n",
    "to do a simple operation. How it works underneath is a little magical, so just\n",
    "treat it as a blackbox. A more complex example::\n",
    "\n",
    "    t = torch.tensor([[1, 2, 3], [4, 5, 6.0]])\n",
    "    # returns [torch.tensor([[4., 5., 6., 7., 8., 9.]])]\n",
    "    [t] | (op() + 3).view(1, -1).all() | deref()\n",
    "\n",
    "Basically, you can treat ``op()`` as the input tensor. Tbh, you\n",
    "can do the same thing with this::\n",
    "\n",
    "    [t] | applyS(lambda t: (t+3).view(-1, 1)).all() | deref()\n",
    "\n",
    "But that's kinda long and may not be obvious. This can be surprisingly resilient, as\n",
    "you can still combine with other cli tools as usual, for example::\n",
    "\n",
    "    # returns [2, 3], demonstrating \"&\" operator\n",
    "    torch.randn(2, 3) | (op().shape & iden()) | deref() | item()\n",
    "\n",
    "    a = torch.tensor([[1, 2, 3], [7, 8, 9]])\n",
    "    # returns torch.tensor([4, 5, 6]), demonstrating \"+\" operator for clis and not clis\n",
    "    (a | op() + 3 + iden() | item() == torch.tensor([4, 5, 6])).all()\n",
    "\n",
    "    # returns [[3], [3]], demonstrating .all() and \"|\" serial chaining\n",
    "    torch.randn(2, 3) | (op().shape.all() | deref())\n",
    "    \n",
    "    # returns [[8, 18], [9, 19]], demonstrating you can treat `op()` as a regular function\n",
    "    [range(10), range(10, 20)] | transpose() | filt(op() > 7, 0) | deref()\n",
    "    \n",
    "    # returns [3, 4, 5, 6, 7, 8, 9], demonstrating bounds comparison\n",
    "    range(100) | filt(3 <= op() < 10) | deref()\n",
    "\n",
    "This can only deal with simple operations only. For complex operations, resort\n",
    "to the longer version ``aS(lambda x: ...)`` instead!\n",
    "\n",
    "There are also operations that are difficult to achieve, like\n",
    "``len(op())``, as Python is expecting an integer output, so\n",
    "``op()`` can't exactly take over. Instead, you have to use :class:`aS`,\n",
    "or do ``op().ab_len()``. Get a list of all of these special operations\n",
    "in the source of :class:`~k1lib.Absorber`.\n",
    "\n",
    "Performance-wise, in most cases, there are no degradation, so don't worry\n",
    "about it. Everything is pretty much on par with native lambdas::\n",
    "\n",
    "    n = 10_000_000\n",
    "    # takes 1.48s\n",
    "    for i in range(n): i**2\n",
    "    # takes 1.89s, 1.28x worse than for loop\n",
    "    range(n) | apply(lambda x: x**2) | ignore()\n",
    "    # takes 1.86s, 1.26x worse than for loop\n",
    "    range(n) | apply(op()**2) | ignore()\n",
    "    # takes 1.86s\n",
    "    range(n) | (op()**2).all() | ignore()\n",
    "\n",
    "More complex operations still retains the same speeds, as there's a JIT compiler embedded in::\n",
    "\n",
    "    # takes 2.15s\n",
    "    for i in range(n): (i**2-3)*0.1\n",
    "    # takes 2.53s, 1.18x worse than for loop\n",
    "    range(n) | apply(lambda x: (x**2-3)*0.1) | ignore()\n",
    "    # takes 2.46s, 1.14x worse than for loop\n",
    "    range(n) | apply((op()**2-3)*0.1) | ignore()\n",
    "\n",
    "Reserved operations that are not absorbed are:\n",
    "\n",
    "- all\n",
    "- __ror__ (__or__ still works!)\n",
    "- ab_solidify\n",
    "- op_hint\"\"\"\n",
    "        super().__init__({\"_hint\": None})\n",
    "    @staticmethod\n",
    "    def solidify(f):\n",
    "        \"\"\"Static equivalent of ``a.ab_solidify()``.\n",
    "Example::\n",
    "        \n",
    "    f = op()**2\n",
    "    f = op.solidify(f)\n",
    "\n",
    "If ``f`` is not an ``op``, then just return it without doing anything to it\"\"\"\n",
    "        if f.__class__.__name__.split(\".\")[-1] == \"op\": f.ab_solidify()\n",
    "        return f\n",
    "    def __ror__(self, it):\n",
    "        return self.ab_operate(it)\n",
    "    def __or__(self, o):\n",
    "        if isinstance(o, BaseCli): return super(k1lib.Absorber, self).__or__(o)\n",
    "        return super().__add__(o)\n",
    "    def __add__(self, o):\n",
    "        if isinstance(o, BaseCli): return super(k1lib.Absorber, self).__add__(o)\n",
    "        return super().__add__(o)\n",
    "    def __and__(self, o):\n",
    "        if isinstance(o, BaseCli): return super(k1lib.Absorber, self).__and__(o)\n",
    "        return super().__and__(o)\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if self._ab_solidified: return self.ab_operate(*args, **kwargs)\n",
    "        return super().__call__(*args, **kwargs)\n",
    "    def _typehint(self, inp):\n",
    "        return self._hint if self._hint is not None else tAny()\n",
    "    def op_hint(self, _hint):\n",
    "        \"\"\"Specify output type hint\"\"\"\n",
    "        self._ab_sentinel = True; self._hint = _hint\n",
    "        self._ab_sentinel = False; return self\n",
    "    def _jsF(self, meta): return cli.aS(self)._jsF(meta)\n",
    "cli.op = op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44eab5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([[1, 2, 3], [4, 5, 6.0]])\n",
    "assert torch.all([t] | (op() + 3).view(1, -1).all() | cli.deref() | cli.item() == torch.tensor([[4., 5., 6., 7., 8., 9.]]))\n",
    "assert torch.randn(2, 3) | (op().shape & cli.iden()) | cli.deref() | cli.item() == (2, 3)\n",
    "a = torch.tensor([[1, 2, 3], [7, 8, 9]])\n",
    "assert (a | op() + 3 + cli.iden() | cli.item() == torch.tensor([4, 5, 6])).all()\n",
    "assert torch.randn(2, 3) | op().shape.all() | cli.deref() == [(3,), (3,)]\n",
    "assert torch.randn(2, 3) | (op().shape.all() | cli.deref()) == [(3,), (3,)]\n",
    "o = op()(56); o.ab_solidify(); assert o(lambda x: x+20) == 76\n",
    "assert [range(10), range(10, 20)] | cli.transpose() | cli.filt(op() > 7, 0) | cli.deref() == [[8, 18], [9, 19]]\n",
    "f = (op()**2).ab_solidify(); assert f(3) == 9\n",
    "assert range(100) | cli.filt(3 <= op() < 10) | cli.deref() == [3, 4, 5, 6, 7, 8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ce6d17b-93e1-4701-963a-3f095648b213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class integrate(BaseCli):\n",
    "    blurb=\"Calculates the cumulative sum of the input\"\n",
    "    def __init__(self, col=None, dt=1):\n",
    "        \"\"\"Integrates the input.\n",
    "Example::\n",
    "\n",
    "    # returns [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\n",
    "    range(10) | integrate() | deref()\n",
    "    # returns [0, 2, 6, 12, 20, 30, 42, 56, 72, 90]\n",
    "    range(10) | integrate(dt=2) | deref()\n",
    "\n",
    "\n",
    "    # returns [['a', 0], ['b', 1], ['c', 4], ['d', 10]]\n",
    "    [[\"a\", 0], [\"b\", 1], [\"c\", 3], [\"d\", 6]] | integrate(1) | deref()\n",
    "    # returns [['a', 0], ['b', 2], ['c', 8], ['d', 20]]\n",
    "    [[\"a\", 0], [\"b\", 1], [\"c\", 3], [\"d\", 6]] | integrate(1, dt=2) | deref()\n",
    "\n",
    ":param col: column to integrate over\n",
    ":param dt: optional step size, or delta time\"\"\"\n",
    "        self.col = col; self.dt = dt\n",
    "    def __ror__(self, it):\n",
    "        it = init.dfGuard(it)\n",
    "        if self.col is None:\n",
    "            if self.dt == 1:\n",
    "                s = 0\n",
    "                for e in it: s += e; yield s\n",
    "            else:\n",
    "                dt = self.dt; s = 0\n",
    "                for e in it: s += e*dt; yield s\n",
    "        else:\n",
    "            col = self.col\n",
    "            if self.dt == 1:\n",
    "                s = 0\n",
    "                for e in it: s += e[col]; e[col] = s; yield e\n",
    "            else:\n",
    "                dt = self.dt; s = 0\n",
    "                for e in it: s += e[col]*dt; e[col] = s; yield e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeb13bec-fa54-48e5-b5e8-3d26b1b604c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert range(10) | integrate() | cli.deref() == [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\n",
    "assert range(10) | integrate(dt=2) | cli.deref() == [0, 2, 6, 12, 20, 30, 42, 56, 72, 90]\n",
    "assert [[\"a\", 0], [\"b\", 1], [\"c\", 3], [\"d\", 6]] | integrate(1) | cli.deref() == [['a', 0], ['b', 1], ['c', 4], ['d', 10]]\n",
    "assert [[\"a\", 0], [\"b\", 1], [\"c\", 3], [\"d\", 6]] | integrate(1, dt=2) | cli.deref() == [['a', 0], ['b', 2], ['c', 8], ['d', 20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84f17853-3dc4-4dba-bec3-3e889614a5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class roll(BaseCli):\n",
    "    blurb=\"Rolls the input by some amount of shift\"\n",
    "    def __init__(self, shift:int):\n",
    "        \"\"\"Rolls the input some amount of shift.\n",
    "Example::\n",
    "\n",
    "    # returns [7, 8, 9, 0, 1, 2, 3, 4, 5, 6]\n",
    "    range(10) | roll(3)\n",
    "\n",
    ":param shift: shift amount\"\"\"\n",
    "        self.shift = shift\n",
    "    def _all_array_opt(self, it, level):\n",
    "        if isinstance(it, np.ndarray): return np.roll(it, self.shift, level)\n",
    "        if hasTorch and isinstance(it, torch.Tensor): return torch.roll(it, self.shift, level)\n",
    "    def _all_opt2(self): return NotImplemented # should not do this optimization, this is not dimension-agnostic!\n",
    "    def __ror__(self, it):\n",
    "        shift = self.shift\n",
    "        if isinstance(it, np.ndarray): return np.roll(it, shift, 0)\n",
    "        if hasTorch and isinstance(it, torch.Tensor): return torch.roll(it, shift, 0)\n",
    "        if hasPandas:\n",
    "            arg = np.roll(np.arange(len(it)), shift, 0)\n",
    "            if isinstance(it, pd.DataFrame): return it.iloc[arg]\n",
    "            if isinstance(it, pd.core.arraylike.OpsMixin): return it[arg]\n",
    "        it = init.dfGuard(it)\n",
    "        try: it[0]; len(it)\n",
    "        except: it = list(it)\n",
    "        return [*it[-shift:], *it[:-shift]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3ada625-0f42-4493-b50f-65300f39c61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert range(10) | roll(3) == [7, 8, 9, 0, 1, 2, 3, 4, 5, 6]\n",
    "assert (torch.roll(torch.arange(12).view(3, 4), 1, 0) == torch.tensor([[ 8,  9, 10, 11], [ 0,  1,  2,  3], [ 4,  5,  6,  7]])).all()\n",
    "assert isinstance(df1 | roll(1), pd.DataFrame)\n",
    "assert df1 | roll(1).all() | cli.cut(0) | cli.op().tolist() == [\"foo\"]*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e592b88b-60d2-4942-82e5-2314970fa448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class clamp(BaseCli):\n",
    "    blurb=\"Clamps input list/array between 2 values\"\n",
    "    def __init__(self, col=None, min=0, max=1, std=None):\n",
    "        \"\"\"Clamps input list/array between 2 values.\n",
    "Example::\n",
    "\n",
    "    # returns [3, 3, 3, 3, 4, 5, 6, 7, 7, 7]\n",
    "    range(10) | clamp(None, 3, 7) | deref()\n",
    "    # clamps the 1st column (0-index!) between 0 and 2\n",
    "    np.random.randn(10, 3) | clamp(1, 0, 2)\n",
    "\n",
    "This cli has 2 modes. Absolute mode and std mode. Absolute mode will\n",
    "clamp between .min and .max, and is activated when .std is left alone\n",
    "(aka None).\n",
    "\n",
    "Std mode is activated when you specify .std value. Then, it will calculate\n",
    "the mean and std of the incoming data and will auto calculate the min and\n",
    "max values.\n",
    "\n",
    ":param col: column to clamp\"\"\"\n",
    "        self.col = col; self.min = min; self.max = max; self.std = std\n",
    "    def _all_array_opt(self, it, level): # this was pretty painful, I have to admit!\n",
    "        n = len(it.shape); col = self.col; std = self.std\n",
    "        if col is None:\n",
    "            if self.std is not None:\n",
    "                it = np.copy(it) if isinstance(it, np.ndarray) else torch.clone(it)\n",
    "                b = it | cli.joinSt(n-level-1).all(level)\n",
    "                min_ = (b.mean(level) - std*b.std(level)) | cli.repeat(b.shape[-1]).all(level)\n",
    "                max_ = (b.mean(level) + std*b.std(level)) | cli.repeat(b.shape[-1]).all(level)\n",
    "                b[b < min_] = min_[b < min_]; b[b > max_] = max_[b > max_]; return b.reshape(it.shape)\n",
    "            else:\n",
    "                if isinstance(it, np.ndarray): return np.clip(it, self.min, self.max)\n",
    "                if hasTorch and isinstance(it, torch.Tensor): return torch.clamp(it, self.min, self.max)\n",
    "        else:\n",
    "            if self.std is not None:\n",
    "                it = np.copy(it) if isinstance(it, np.ndarray) else torch.clone(it)\n",
    "                b = it[(*[slice(None,None,None)]*level,col)] | cli.joinSt(n-level-2).all(level)\n",
    "                min_ = (b.mean(level) - std*b.std(level)) | cli.repeat(b.shape[-1]).all(level)\n",
    "                max_ = (b.mean(level) + std*b.std(level)) | cli.repeat(b.shape[-1]).all(level)\n",
    "                b[b < min_] = min_[b < min_]; b[b > max_] = max_[b > max_]; return it\n",
    "            else:\n",
    "                it = np.copy(it) if isinstance(it, np.ndarray) else torch.clone(it)\n",
    "                it[(*[slice(None,None,None)]*level,col)] = (np.clip if isinstance(it, np.ndarray) else torch.clamp)(it[(*[slice(None,None,None)]*level,col)], self.min, self.max); return it\n",
    "        return NotImplemented\n",
    "    def __ror__(self, it):\n",
    "        col = self.col; min_ = self.min; max_ = self.max; std = self.std\n",
    "        if isinstance(it, np.ndarray):\n",
    "            if col is None:\n",
    "                if std is None: return np.clip(it, min_, max_)\n",
    "                m = it.mean(); s = it.std(); return np.clip(it, m-s*std, m+s*std)\n",
    "            else:\n",
    "                a = np.copy(it); c = a[:,col]\n",
    "                if std is None: a[:,col] = np.clip(c, min_, max_)\n",
    "                else: m = c.mean(); s = c.std(); a[:,col] = np.clip(c, m-s*std, m+s*std)\n",
    "                return a\n",
    "        if hasTorch and isinstance(it, torch.Tensor):\n",
    "            if col is None:\n",
    "                if std is None: return torch.clamp(it, min_, max_)\n",
    "                m = it.mean(); s = it.std(); return torch.clamp(it, m-s*std, m+s*std)\n",
    "            else:\n",
    "                a = torch.clone(it); c = a[:,col]\n",
    "                if std is None: a[:,col] = torch.clamp(c, min_, max_)\n",
    "                else: m = c.mean(); s = c.std(); a[:,col] = torch.clamp(c, m-s*std, m+s*std)\n",
    "                return a\n",
    "        if hasPandas and isinstance(it, pd.DataFrame):\n",
    "            if col is None: it = init.dfGuard(it)\n",
    "            else:\n",
    "                c = it[list(it)[col]]\n",
    "                if std is None: c = np.clip(c, min_, max_)\n",
    "                else: m = c.mean(); s = c.std(); c = np.clip(c, m-s*std, m+s*std)\n",
    "                return it.replaceCol(list(it)[col], c)\n",
    "        return it | cli.apply(lambda x: max(min(x, max_), min_), self.col) # TODO: needs fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d2c16a23-22dd-4d82-aa05-0dbbe201c4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(3,4,5)\n",
    "assert range(10) | clamp(None, 3, 7) | cli.deref() == [3, 3, 3, 3, 4, 5, 6, 7, 7, 7]\n",
    "assert np.random.randn(10, 3) | clamp(1) | cli.shape() == (10, 3)\n",
    "assert np.random.randn(10, 3) | clamp() | cli.shape() == (10, 3)\n",
    "assert a | clamp().all() | aS(type) == np.ndarray\n",
    "assert np.allclose(np.arange(10) | clamp(std=1), np.array([1.62771868, 1.62771868, 2., 3., 4., 5., 6., 7., 7.37228132, 7.37228132]))\n",
    "# ----- array opts\n",
    "a = np.random.randn(3,4,5,6,7); ogA = np.copy(a); a.shape\n",
    "assert (a | clamp(std=0.01) == a) | cli.joinSt(4) | cli.count() | ~cli.sort() | cli.item(2) > 2400\n",
    "assert a | clamp(std=10) | cli.shape() == a | cli.shape()\n",
    "assert a | clamp(std=10).all(3) | cli.shape() == a | cli.shape()\n",
    "assert 1500 < ((a | clamp(std=1).all(3) == a) | cli.joinSt(4) | cli.count() | ~cli.sort() | cli.item(2)) < 1800\n",
    "assert 1800 < ((a | clamp(std=0.0001, col=1).all(1) == a) | cli.joinSt(4) | cli.count() | ~cli.sort() | cli.item(2)) < 2000\n",
    "assert 2000 < ((a | clamp(std=0.0001, col=1).all(2) == a) | cli.joinSt(4) | cli.count() | ~cli.sort() | cli.item(2)) < 2100\n",
    "assert 1800 < ((a | clamp(col=1, min=1, max=1).all(1) == a) | cli.joinSt(4) | cli.count() | ~cli.sort() | cli.item(2)) < 2000\n",
    "assert 2000 < ((a | clamp(col=1, min=1, max=1).all(2) == a) | cli.joinSt(4) | cli.count() | ~cli.sort() | cli.item(2)) < 2100\n",
    "assert np.allclose(df1 | clamp(0, std=1) | cli.cut(0) | cli.toNdArray(), np.array([1.20900555, 2.        , 3.        , 3.79099445]))\n",
    "assert isinstance(df1 | clamp(0, std=1), pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05559693-c29f-4e8c-87fe-032ddb897760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20478478, 1.        , 0.33406023],\n",
       "       [0.        , 0.        , 0.        ],\n",
       "       [0.05877187, 0.29446164, 0.99815907],\n",
       "       [0.        , 0.74063816, 0.20165022],\n",
       "       [0.        , 0.25604022, 0.        ],\n",
       "       [0.59492556, 0.        , 0.23151868],\n",
       "       [0.        , 0.22214514, 0.        ],\n",
       "       [1.        , 0.        , 1.        ],\n",
       "       [0.62448199, 0.        , 0.15706065],\n",
       "       [0.70723769, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10, 3) | clamp().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7119be2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./export started up - /home/quang/miniconda3/envs/torch/bin/python3\n",
      "----- exportAll\n",
      "16139   0   61%   \n",
      "10454   1   39%   \n",
      "Found existing installation: k1lib 1.7\n",
      "Uninstalling k1lib-1.7:\n",
      "  Successfully uninstalled k1lib-1.7\n",
      "Looking in indexes: https://pypi.org/simple, http://10.104.0.3:3141/\n",
      "Processing /home/quang/k1lib\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from k1lib==1.7) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=2.0 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from k1lib==1.7) (3.8.3)\n",
      "Requirement already satisfied: dill in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from k1lib==1.7) (0.3.8)\n",
      "Requirement already satisfied: forbiddenfruit in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from k1lib==1.7) (0.1.4)\n",
      "Requirement already satisfied: wurlitzer in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from k1lib==1.7) (3.0.3)\n",
      "Requirement already satisfied: validators in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from k1lib==1.7) (0.22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from matplotlib>=2.0->k1lib==1.7) (6.1.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.0->k1lib==1.7) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/quang/miniconda3/envs/torch/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.0->k1lib==1.7) (1.16.0)\n",
      "Building wheels for collected packages: k1lib\n",
      "  Building wheel for k1lib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for k1lib: filename=k1lib-1.7-py3-none-any.whl size=5106786 sha256=7fdabfb89461042ba2282b932c9919988738468b58f57a6c579dce4e1e039068\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-grgsee3c/wheels/11/94/07/711323eb4091c7ef1b180ccc3793fc75a96521821bdd2932ac\n",
      "Successfully built k1lib\n",
      "Installing collected packages: k1lib\n",
      "Successfully installed k1lib-1.7\n"
     ]
    }
   ],
   "source": [
    "!../../export.py cli/modifier --upload=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "adccf27f-ef72-4fd2-9579-e337257b40ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-08 05:22:21,019\tINFO worker.py:1458 -- Connecting to existing Ray cluster at address: 192.168.1.17:6379...\n",
      "2024-03-08 05:22:21,028\tINFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "./export started up - /home/kelvin/anaconda3/envs/ray2/bin/python3\n",
      "----- exportAll\n",
      "15667   0   61%   \n",
      "10027   1   39%   \n",
      "rm: cannot remove '__pycache__': No such file or directory\n",
      "Found existing installation: k1lib 1.6\n",
      "Uninstalling k1lib-1.6:\n",
      "  Successfully uninstalled k1lib-1.6\n",
      "running install\n",
      "/home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "/home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating k1lib.egg-info\n",
      "writing k1lib.egg-info/PKG-INFO\n",
      "writing dependency_links to k1lib.egg-info/dependency_links.txt\n",
      "writing requirements to k1lib.egg-info/requires.txt\n",
      "writing top-level names to k1lib.egg-info/top_level.txt\n",
      "writing manifest file 'k1lib.egg-info/SOURCES.txt'\n",
      "reading manifest file 'k1lib.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'k1lib.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/k1lib\n",
      "copying k1lib/_learner.py -> build/lib/k1lib\n",
      "copying k1lib/fmt.py -> build/lib/k1lib\n",
      "copying k1lib/selen.py -> build/lib/k1lib\n",
      "copying k1lib/kast.py -> build/lib/k1lib\n",
      "copying k1lib/_k1a.py -> build/lib/k1lib\n",
      "copying k1lib/_context.py -> build/lib/k1lib\n",
      "copying k1lib/selector.py -> build/lib/k1lib\n",
      "copying k1lib/imports.py -> build/lib/k1lib\n",
      "copying k1lib/_baseClasses.py -> build/lib/k1lib\n",
      "copying k1lib/_basics.py -> build/lib/k1lib\n",
      "copying k1lib/serpent.py -> build/lib/k1lib\n",
      "copying k1lib/viz.py -> build/lib/k1lib\n",
      "copying k1lib/zircon.py -> build/lib/k1lib\n",
      "copying k1lib/_higher.py -> build/lib/k1lib\n",
      "copying k1lib/__init__.py -> build/lib/k1lib\n",
      "copying k1lib/_monkey.py -> build/lib/k1lib\n",
      "copying k1lib/knn.py -> build/lib/k1lib\n",
      "copying k1lib/p5.py -> build/lib/k1lib\n",
      "copying k1lib/kcom.py -> build/lib/k1lib\n",
      "copying k1lib/graphEqn.py -> build/lib/k1lib\n",
      "copying k1lib/_advanced.py -> build/lib/k1lib\n",
      "copying k1lib/schedule.py -> build/lib/k1lib\n",
      "copying k1lib/_perlin.py -> build/lib/k1lib\n",
      "copying k1lib/kws.py -> build/lib/k1lib\n",
      "copying k1lib/trans.py -> build/lib/k1lib\n",
      "copying k1lib/eqn.py -> build/lib/k1lib\n",
      "copying k1lib/kop.py -> build/lib/k1lib\n",
      "creating build/lib/k1lib/_hidden\n",
      "copying k1lib/_hidden/hiddenFile.py -> build/lib/k1lib/_hidden\n",
      "copying k1lib/_hidden/__init__.py -> build/lib/k1lib/_hidden\n",
      "creating build/lib/k1lib/cli\n",
      "copying k1lib/cli/bio.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/cif.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/structural.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/modifier.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/gb.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/output.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kxml.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/ktree.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/nb.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/inp.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/mol.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/mgi.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kcv.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/_applyCl.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/grep.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kapi.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/models.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/sam.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/trace.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kjs.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/__init__.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/typehint.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kgv.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/filt.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/utils.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/init.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/conv.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/optimizations.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/lsext.py -> build/lib/k1lib/cli\n",
      "creating build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/loss_accuracy.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/progress.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/limits.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/hookParam.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/profiler.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/callbacks.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/paramFinder.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/core.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/__init__.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/landscape.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/confusionMatrix.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/recorder.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/shorts.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/hookModule.py -> build/lib/k1lib/callbacks\n",
      "creating build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/time.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/memory.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/__init__.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/io.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/computation.py -> build/lib/k1lib/callbacks/profilers\n",
      "creating build/lib/k1lib/callbacks/lossFunctions\n",
      "copying k1lib/callbacks/lossFunctions/accuracy.py -> build/lib/k1lib/callbacks/lossFunctions\n",
      "copying k1lib/callbacks/lossFunctions/__init__.py -> build/lib/k1lib/callbacks/lossFunctions\n",
      "copying k1lib/callbacks/lossFunctions/shorts.py -> build/lib/k1lib/callbacks/lossFunctions\n",
      "creating build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/atom.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/parseM.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/substance.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/system.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/__init__.py -> build/lib/k1lib/_mo\n",
      "creating build/lib/k1lib/serve\n",
      "copying k1lib/serve/suffix.py -> build/lib/k1lib/serve\n",
      "copying k1lib/serve/suffix-dash.py -> build/lib/k1lib/serve\n",
      "copying k1lib/serve/__init__.py -> build/lib/k1lib/serve\n",
      "copying k1lib/serve/main.py -> build/lib/k1lib/serve\n",
      "creating build/lib/k1lib/k1ui\n",
      "copying k1lib/k1ui/__init__.py -> build/lib/k1lib/k1ui\n",
      "copying k1lib/k1ui/main.py -> build/lib/k1lib/k1ui\n",
      "copying k1lib/serve/main.html -> build/lib/k1lib/serve\n",
      "copying k1lib/k1ui/256.model.state_dict.pth -> build/lib/k1lib/k1ui\n",
      "copying k1lib/k1ui/mouseKey.pth -> build/lib/k1lib/k1ui\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_learner.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/main.html -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/suffix.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/suffix-dash.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/main.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/fmt.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/selen.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kast.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_k1a.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_context.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/selector.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/imports.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/mouseKey.pth -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/main.py -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/256.model.state_dict.pth -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/_baseClasses.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_basics.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/serpent.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/bio.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/cif.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/structural.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/modifier.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/gb.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/output.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kxml.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/ktree.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/nb.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/inp.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/mol.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/mgi.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kcv.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/_applyCl.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/grep.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kapi.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/models.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/sam.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/trace.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kjs.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/typehint.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kgv.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/filt.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/utils.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/init.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/conv.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/optimizations.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/lsext.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/viz.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/zircon.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_higher.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/__init__.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_monkey.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/atom.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/parseM.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/substance.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/system.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/knn.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/p5.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kcom.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/graphEqn.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_advanced.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/schedule.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/loss_accuracy.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/progress.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/limits.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/hookParam.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/profiler.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/callbacks.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/paramFinder.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/core.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/time.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/memory.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/io.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/computation.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/landscape.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/confusionMatrix.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/recorder.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/shorts.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/hookModule.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/callbacks/lossFunctions/accuracy.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/callbacks/lossFunctions/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/callbacks/lossFunctions/shorts.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/_perlin.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kws.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/trans.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/_hidden\n",
      "copying build/lib/k1lib/_hidden/hiddenFile.py -> build/bdist.linux-x86_64/egg/k1lib/_hidden\n",
      "copying build/lib/k1lib/_hidden/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/_hidden\n",
      "copying build/lib/k1lib/eqn.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kop.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_learner.py to _learner.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/suffix.py to suffix.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/suffix-dash.py to suffix-dash.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/main.py to main.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/fmt.py to fmt.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/selen.py to selen.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kast.py to kast.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_k1a.py to _k1a.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_context.py to _context.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/selector.py to selector.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/imports.py to imports.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/k1ui/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/k1ui/main.py to main.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_baseClasses.py to _baseClasses.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_basics.py to _basics.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serpent.py to serpent.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/bio.py to bio.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/cif.py to cif.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/structural.py to structural.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/modifier.py to modifier.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/gb.py to gb.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/output.py to output.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kxml.py to kxml.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/ktree.py to ktree.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/nb.py to nb.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/inp.py to inp.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/mol.py to mol.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/mgi.py to mgi.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kcv.py to kcv.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/_applyCl.py to _applyCl.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/grep.py to grep.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kapi.py to kapi.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/models.py to models.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/sam.py to sam.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/trace.py to trace.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kjs.py to kjs.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/typehint.py to typehint.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kgv.py to kgv.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/filt.py to filt.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/utils.py to utils.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/init.py to init.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/conv.py to conv.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/optimizations.py to optimizations.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/lsext.py to lsext.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/viz.py to viz.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/zircon.py to zircon.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_higher.py to _higher.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_monkey.py to _monkey.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/atom.py to atom.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/parseM.py to parseM.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/substance.py to substance.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/system.py to system.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/knn.py to knn.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/p5.py to p5.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kcom.py to kcom.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/graphEqn.py to graphEqn.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_advanced.py to _advanced.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/schedule.py to schedule.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/loss_accuracy.py to loss_accuracy.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/progress.py to progress.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/limits.py to limits.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/hookParam.py to hookParam.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profiler.py to profiler.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/callbacks.py to callbacks.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/paramFinder.py to paramFinder.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/core.py to core.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/time.py to time.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/memory.py to memory.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/io.py to io.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/computation.py to computation.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/landscape.py to landscape.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/confusionMatrix.py to confusionMatrix.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/recorder.py to recorder.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/shorts.py to shorts.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/hookModule.py to hookModule.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions/accuracy.py to accuracy.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions/shorts.py to shorts.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_perlin.py to _perlin.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kws.py to kws.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/trans.py to trans.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_hidden/hiddenFile.py to hiddenFile.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_hidden/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/eqn.py to eqn.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kop.py to kop.cpython-39.pyc\n",
      "installing package data to build/bdist.linux-x86_64/egg\n",
      "running install_data\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "k1lib.cli.__pycache__.init.cpython-39: module MAY be using inspect.trace\n",
      "k1lib.cli.__pycache__.modifier.cpython-39: module references __file__\n",
      "k1lib.k1ui.__pycache__.main.cpython-39: module MAY be using inspect.getabsfile\n",
      "k1lib.k1ui.__pycache__.main.cpython-39: module MAY be using inspect.stack\n",
      "k1lib.serve.__pycache__.main.cpython-39: module MAY be using inspect.getsource\n",
      "k1lib.serve.__pycache__.main.cpython-39: module MAY be using inspect.getabsfile\n",
      "creating dist\n",
      "creating 'dist/k1lib-1.6-py3.9.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing k1lib-1.6-py3.9.egg\n",
      "creating /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/k1lib-1.6-py3.9.egg\n",
      "Extracting k1lib-1.6-py3.9.egg to /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Adding k1lib 1.6 to easy-install.pth file\n",
      "\n",
      "Installed /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/k1lib-1.6-py3.9.egg\n",
      "Processing dependencies for k1lib==1.6\n",
      "Searching for validators==0.20.0\n",
      "Best match: validators 0.20.0\n",
      "Adding validators 0.20.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for wurlitzer==3.0.3\n",
      "Best match: wurlitzer 3.0.3\n",
      "Adding wurlitzer 3.0.3 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for forbiddenfruit==0.1.4\n",
      "Best match: forbiddenfruit 0.1.4\n",
      "Adding forbiddenfruit 0.1.4 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for dill==0.3.7\n",
      "Best match: dill 0.3.7\n",
      "Adding dill 0.3.7 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for matplotlib==3.7.1\n",
      "Best match: matplotlib 3.7.1\n",
      "Adding matplotlib 3.7.1 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for numpy==1.25.0\n",
      "Best match: numpy 1.25.0\n",
      "Adding numpy 1.25.0 to easy-install.pth file\n",
      "Installing f2py script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing f2py3 script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing f2py3.9 script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for decorator==5.1.1\n",
      "Best match: decorator 5.1.1\n",
      "Adding decorator 5.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for importlib-resources==5.12.0\n",
      "Best match: importlib-resources 5.12.0\n",
      "Adding importlib-resources 5.12.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for python-dateutil==2.8.2\n",
      "Best match: python-dateutil 2.8.2\n",
      "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for pyparsing==3.1.0\n",
      "Best match: pyparsing 3.1.0\n",
      "Adding pyparsing 3.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for Pillow==9.5.0\n",
      "Best match: Pillow 9.5.0\n",
      "Adding Pillow 9.5.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for packaging==23.1\n",
      "Best match: packaging 23.1\n",
      "Adding packaging 23.1 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for kiwisolver==1.4.4\n",
      "Best match: kiwisolver 1.4.4\n",
      "Adding kiwisolver 1.4.4 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for fonttools==4.40.0\n",
      "Best match: fonttools 4.40.0\n",
      "Adding fonttools 4.40.0 to easy-install.pth file\n",
      "Installing fonttools script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing pyftmerge script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing pyftsubset script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing ttx script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for cycler==0.11.0\n",
      "Best match: cycler 0.11.0\n",
      "Adding cycler 0.11.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for contourpy==1.1.0\n",
      "Best match: contourpy 1.1.0\n",
      "Adding contourpy 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for zipp==3.15.0\n",
      "Best match: zipp 3.15.0\n",
      "Adding zipp 3.15.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for six==1.16.0\n",
      "Best match: six 1.16.0\n",
      "Adding six 1.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Finished processing dependencies for k1lib==1.6\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!../../export.py cli/modifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b9d4c-8aba-43fb-a619-503b0177e45f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kelvin/repos/labs/k1lib/k1lib/_monkey.py:757: UserWarning: Tried to patch __or__ operator of type `pd.core.arraylike.OpsMixin` but can't because: module 'pandas' has no attribute 'core'\n",
      "  except Exception as e: warnings.warn(f\"Tried to patch __or__ operator of type `pd.core.arraylike.OpsMixin` but can't because: {e}\") # patchPandas\n",
      "2024-03-08 03:53:16,406\tINFO worker.py:1458 -- Connecting to existing Ray cluster at address: 192.168.1.17:6379...\n",
      "2024-03-08 03:53:16,415\tINFO worker.py:1633 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "./export started up - /home/kelvin/anaconda3/envs/ray2/bin/python3\n",
      "----- bootstrapping\n",
      "Current dir: /home/kelvin/repos/labs/k1lib, /home/kelvin/repos/labs/k1lib/k1lib/cli/../../export.py\n",
      "rm: cannot remove '__pycache__': No such file or directory\n",
      "Found existing installation: k1lib 1.6\n",
      "Uninstalling k1lib-1.6:\n",
      "  Successfully uninstalled k1lib-1.6\n",
      "running install\n",
      "/home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "/home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating k1lib.egg-info\n",
      "writing k1lib.egg-info/PKG-INFO\n",
      "writing dependency_links to k1lib.egg-info/dependency_links.txt\n",
      "writing requirements to k1lib.egg-info/requires.txt\n",
      "writing top-level names to k1lib.egg-info/top_level.txt\n",
      "writing manifest file 'k1lib.egg-info/SOURCES.txt'\n",
      "reading manifest file 'k1lib.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'k1lib.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/k1lib\n",
      "copying k1lib/_learner.py -> build/lib/k1lib\n",
      "copying k1lib/fmt.py -> build/lib/k1lib\n",
      "copying k1lib/selen.py -> build/lib/k1lib\n",
      "copying k1lib/kast.py -> build/lib/k1lib\n",
      "copying k1lib/_k1a.py -> build/lib/k1lib\n",
      "copying k1lib/_context.py -> build/lib/k1lib\n",
      "copying k1lib/selector.py -> build/lib/k1lib\n",
      "copying k1lib/imports.py -> build/lib/k1lib\n",
      "copying k1lib/_baseClasses.py -> build/lib/k1lib\n",
      "copying k1lib/_basics.py -> build/lib/k1lib\n",
      "copying k1lib/serpent.py -> build/lib/k1lib\n",
      "copying k1lib/viz.py -> build/lib/k1lib\n",
      "copying k1lib/zircon.py -> build/lib/k1lib\n",
      "copying k1lib/_higher.py -> build/lib/k1lib\n",
      "copying k1lib/__init__.py -> build/lib/k1lib\n",
      "copying k1lib/_monkey.py -> build/lib/k1lib\n",
      "copying k1lib/knn.py -> build/lib/k1lib\n",
      "copying k1lib/p5.py -> build/lib/k1lib\n",
      "copying k1lib/kcom.py -> build/lib/k1lib\n",
      "copying k1lib/graphEqn.py -> build/lib/k1lib\n",
      "copying k1lib/_advanced.py -> build/lib/k1lib\n",
      "copying k1lib/schedule.py -> build/lib/k1lib\n",
      "copying k1lib/_perlin.py -> build/lib/k1lib\n",
      "copying k1lib/kws.py -> build/lib/k1lib\n",
      "copying k1lib/trans.py -> build/lib/k1lib\n",
      "copying k1lib/eqn.py -> build/lib/k1lib\n",
      "copying k1lib/kop.py -> build/lib/k1lib\n",
      "creating build/lib/k1lib/_hidden\n",
      "copying k1lib/_hidden/hiddenFile.py -> build/lib/k1lib/_hidden\n",
      "copying k1lib/_hidden/__init__.py -> build/lib/k1lib/_hidden\n",
      "creating build/lib/k1lib/cli\n",
      "copying k1lib/cli/bio.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/cif.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/structural.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/modifier.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/gb.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/output.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kxml.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/ktree.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/nb.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/inp.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/mol.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/mgi.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kcv.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/_applyCl.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/grep.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kapi.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/models.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/sam.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/trace.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kjs.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/__init__.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/typehint.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/kgv.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/filt.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/utils.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/init.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/conv.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/optimizations.py -> build/lib/k1lib/cli\n",
      "copying k1lib/cli/lsext.py -> build/lib/k1lib/cli\n",
      "creating build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/loss_accuracy.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/progress.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/limits.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/hookParam.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/profiler.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/callbacks.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/paramFinder.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/core.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/__init__.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/landscape.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/confusionMatrix.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/recorder.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/shorts.py -> build/lib/k1lib/callbacks\n",
      "copying k1lib/callbacks/hookModule.py -> build/lib/k1lib/callbacks\n",
      "creating build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/time.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/memory.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/__init__.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/io.py -> build/lib/k1lib/callbacks/profilers\n",
      "copying k1lib/callbacks/profilers/computation.py -> build/lib/k1lib/callbacks/profilers\n",
      "creating build/lib/k1lib/callbacks/lossFunctions\n",
      "copying k1lib/callbacks/lossFunctions/accuracy.py -> build/lib/k1lib/callbacks/lossFunctions\n",
      "copying k1lib/callbacks/lossFunctions/__init__.py -> build/lib/k1lib/callbacks/lossFunctions\n",
      "copying k1lib/callbacks/lossFunctions/shorts.py -> build/lib/k1lib/callbacks/lossFunctions\n",
      "creating build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/atom.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/parseM.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/substance.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/system.py -> build/lib/k1lib/_mo\n",
      "copying k1lib/_mo/__init__.py -> build/lib/k1lib/_mo\n",
      "creating build/lib/k1lib/serve\n",
      "copying k1lib/serve/suffix.py -> build/lib/k1lib/serve\n",
      "copying k1lib/serve/suffix-dash.py -> build/lib/k1lib/serve\n",
      "copying k1lib/serve/__init__.py -> build/lib/k1lib/serve\n",
      "copying k1lib/serve/main.py -> build/lib/k1lib/serve\n",
      "creating build/lib/k1lib/k1ui\n",
      "copying k1lib/k1ui/__init__.py -> build/lib/k1lib/k1ui\n",
      "copying k1lib/k1ui/main.py -> build/lib/k1lib/k1ui\n",
      "copying k1lib/serve/main.html -> build/lib/k1lib/serve\n",
      "copying k1lib/k1ui/256.model.state_dict.pth -> build/lib/k1lib/k1ui\n",
      "copying k1lib/k1ui/mouseKey.pth -> build/lib/k1lib/k1ui\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_learner.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/main.html -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/suffix.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/suffix-dash.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/serve/main.py -> build/bdist.linux-x86_64/egg/k1lib/serve\n",
      "copying build/lib/k1lib/fmt.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/selen.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kast.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_k1a.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_context.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/selector.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/imports.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/mouseKey.pth -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/main.py -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/k1ui/256.model.state_dict.pth -> build/bdist.linux-x86_64/egg/k1lib/k1ui\n",
      "copying build/lib/k1lib/_baseClasses.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_basics.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/serpent.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/bio.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/cif.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/structural.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/modifier.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/gb.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/output.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kxml.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/ktree.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/nb.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/inp.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/mol.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/mgi.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kcv.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/_applyCl.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/grep.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kapi.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/models.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/sam.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/trace.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kjs.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/typehint.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/kgv.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/filt.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/utils.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/init.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/conv.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/optimizations.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/cli/lsext.py -> build/bdist.linux-x86_64/egg/k1lib/cli\n",
      "copying build/lib/k1lib/viz.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/zircon.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_higher.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/__init__.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_monkey.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/atom.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/parseM.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/substance.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/system.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/_mo/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/_mo\n",
      "copying build/lib/k1lib/knn.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/p5.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kcom.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/graphEqn.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/_advanced.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/schedule.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/loss_accuracy.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/progress.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/limits.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/hookParam.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/profiler.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/callbacks.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/paramFinder.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/core.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/time.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/memory.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/io.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/profilers/computation.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers\n",
      "copying build/lib/k1lib/callbacks/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/landscape.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/confusionMatrix.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/recorder.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/shorts.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "copying build/lib/k1lib/callbacks/hookModule.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/callbacks/lossFunctions/accuracy.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/callbacks/lossFunctions/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/callbacks/lossFunctions/shorts.py -> build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions\n",
      "copying build/lib/k1lib/_perlin.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kws.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/trans.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "creating build/bdist.linux-x86_64/egg/k1lib/_hidden\n",
      "copying build/lib/k1lib/_hidden/hiddenFile.py -> build/bdist.linux-x86_64/egg/k1lib/_hidden\n",
      "copying build/lib/k1lib/_hidden/__init__.py -> build/bdist.linux-x86_64/egg/k1lib/_hidden\n",
      "copying build/lib/k1lib/eqn.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "copying build/lib/k1lib/kop.py -> build/bdist.linux-x86_64/egg/k1lib\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_learner.py to _learner.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/suffix.py to suffix.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/suffix-dash.py to suffix-dash.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serve/main.py to main.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/fmt.py to fmt.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/selen.py to selen.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kast.py to kast.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_k1a.py to _k1a.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_context.py to _context.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/selector.py to selector.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/imports.py to imports.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/k1ui/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/k1ui/main.py to main.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_baseClasses.py to _baseClasses.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_basics.py to _basics.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/serpent.py to serpent.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/bio.py to bio.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/cif.py to cif.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/structural.py to structural.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/modifier.py to modifier.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/gb.py to gb.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/output.py to output.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kxml.py to kxml.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/ktree.py to ktree.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/nb.py to nb.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/inp.py to inp.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/mol.py to mol.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/mgi.py to mgi.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kcv.py to kcv.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/_applyCl.py to _applyCl.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/grep.py to grep.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kapi.py to kapi.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/models.py to models.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/sam.py to sam.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/trace.py to trace.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kjs.py to kjs.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/typehint.py to typehint.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/kgv.py to kgv.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/filt.py to filt.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/utils.py to utils.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/init.py to init.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/conv.py to conv.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/optimizations.py to optimizations.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/cli/lsext.py to lsext.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/viz.py to viz.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/zircon.py to zircon.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_higher.py to _higher.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_monkey.py to _monkey.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/atom.py to atom.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/parseM.py to parseM.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/substance.py to substance.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/system.py to system.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_mo/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/knn.py to knn.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/p5.py to p5.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kcom.py to kcom.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/graphEqn.py to graphEqn.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_advanced.py to _advanced.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/schedule.py to schedule.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/loss_accuracy.py to loss_accuracy.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/progress.py to progress.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/limits.py to limits.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/hookParam.py to hookParam.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profiler.py to profiler.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/callbacks.py to callbacks.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/paramFinder.py to paramFinder.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/core.py to core.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/time.py to time.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/memory.py to memory.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/io.py to io.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/profilers/computation.py to computation.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/landscape.py to landscape.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/confusionMatrix.py to confusionMatrix.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/recorder.py to recorder.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/shorts.py to shorts.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/hookModule.py to hookModule.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions/accuracy.py to accuracy.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/callbacks/lossFunctions/shorts.py to shorts.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_perlin.py to _perlin.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kws.py to kws.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/trans.py to trans.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_hidden/hiddenFile.py to hiddenFile.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/_hidden/__init__.py to __init__.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/eqn.py to eqn.cpython-39.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/k1lib/kop.py to kop.cpython-39.pyc\n",
      "installing package data to build/bdist.linux-x86_64/egg\n",
      "running install_data\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying k1lib.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "k1lib.cli.__pycache__.init.cpython-39: module MAY be using inspect.trace\n",
      "k1lib.cli.__pycache__.modifier.cpython-39: module references __file__\n",
      "k1lib.k1ui.__pycache__.main.cpython-39: module MAY be using inspect.getabsfile\n",
      "k1lib.k1ui.__pycache__.main.cpython-39: module MAY be using inspect.stack\n",
      "k1lib.serve.__pycache__.main.cpython-39: module MAY be using inspect.getsource\n",
      "k1lib.serve.__pycache__.main.cpython-39: module MAY be using inspect.getabsfile\n",
      "creating dist\n",
      "creating 'dist/k1lib-1.6-py3.9.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing k1lib-1.6-py3.9.egg\n",
      "creating /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/k1lib-1.6-py3.9.egg\n",
      "Extracting k1lib-1.6-py3.9.egg to /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Adding k1lib 1.6 to easy-install.pth file\n",
      "\n",
      "Installed /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages/k1lib-1.6-py3.9.egg\n",
      "Processing dependencies for k1lib==1.6\n",
      "Searching for validators==0.20.0\n",
      "Best match: validators 0.20.0\n",
      "Adding validators 0.20.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for wurlitzer==3.0.3\n",
      "Best match: wurlitzer 3.0.3\n",
      "Adding wurlitzer 3.0.3 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for forbiddenfruit==0.1.4\n",
      "Best match: forbiddenfruit 0.1.4\n",
      "Adding forbiddenfruit 0.1.4 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for dill==0.3.7\n",
      "Best match: dill 0.3.7\n",
      "Adding dill 0.3.7 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for matplotlib==3.7.1\n",
      "Best match: matplotlib 3.7.1\n",
      "Adding matplotlib 3.7.1 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for numpy==1.25.0\n",
      "Best match: numpy 1.25.0\n",
      "Adding numpy 1.25.0 to easy-install.pth file\n",
      "Installing f2py script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing f2py3 script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing f2py3.9 script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for decorator==5.1.1\n",
      "Best match: decorator 5.1.1\n",
      "Adding decorator 5.1.1 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for importlib-resources==5.12.0\n",
      "Best match: importlib-resources 5.12.0\n",
      "Adding importlib-resources 5.12.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for python-dateutil==2.8.2\n",
      "Best match: python-dateutil 2.8.2\n",
      "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for pyparsing==3.1.0\n",
      "Best match: pyparsing 3.1.0\n",
      "Adding pyparsing 3.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for Pillow==9.5.0\n",
      "Best match: Pillow 9.5.0\n",
      "Adding Pillow 9.5.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for packaging==23.1\n",
      "Best match: packaging 23.1\n",
      "Adding packaging 23.1 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for kiwisolver==1.4.4\n",
      "Best match: kiwisolver 1.4.4\n",
      "Adding kiwisolver 1.4.4 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for fonttools==4.40.0\n",
      "Best match: fonttools 4.40.0\n",
      "Adding fonttools 4.40.0 to easy-install.pth file\n",
      "Installing fonttools script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing pyftmerge script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing pyftsubset script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "Installing ttx script to /home/kelvin/anaconda3/envs/ray2/bin\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for cycler==0.11.0\n",
      "Best match: cycler 0.11.0\n",
      "Adding cycler 0.11.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for contourpy==1.1.0\n",
      "Best match: contourpy 1.1.0\n",
      "Adding contourpy 1.1.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for zipp==3.15.0\n",
      "Best match: zipp 3.15.0\n",
      "Adding zipp 3.15.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Searching for six==1.16.0\n",
      "Best match: six 1.16.0\n",
      "Adding six 1.16.0 to easy-install.pth file\n",
      "\n",
      "Using /home/kelvin/anaconda3/envs/ray2/lib/python3.9/site-packages\n",
      "Finished processing dependencies for k1lib==1.6\n"
     ]
    }
   ],
   "source": [
    "!../../export.py cli/modifier --bootstrap=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434a012a-f78e-465d-97f8-e06b14bbeb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
